{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10_ResNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "adf9f995ccde40a0859cdbf3d7aaf697": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b6690f33fa97436d87a5b4a78a6c1f1b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_903160d45eee40758e25e9e8f1cf09d6",
              "IPY_MODEL_035c8dea93414696a17f49359ed767de"
            ]
          }
        },
        "b6690f33fa97436d87a5b4a78a6c1f1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "903160d45eee40758e25e9e8f1cf09d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fa9c792cc89a4d7388a8c11c7e1b3a0d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9fd4a8dc00824edf99e8cc1ffe177d87"
          }
        },
        "035c8dea93414696a17f49359ed767de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9be061d2af4c4afd94ee98dc8830f976",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:07&lt;00:00, 21850318.58it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5ddbb0064ac64214be4f65cd3778cf45"
          }
        },
        "fa9c792cc89a4d7388a8c11c7e1b3a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9fd4a8dc00824edf99e8cc1ffe177d87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9be061d2af4c4afd94ee98dc8830f976": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5ddbb0064ac64214be4f65cd3778cf45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fe37K5CtO0g"
      },
      "source": [
        "#Reproducing CIFAR10 Experiment in the ResNet paper\n",
        "\n",
        "In this notebook we \"replicate\" Table 6 in \n",
        "[original ResNet paper](https://arxiv.org/abs/1512.03385), \n",
        "i.e. the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
        "experiment in the original ResNet paper \n",
        "published in CVPR 2016 conference andreceived \n",
        "more than 38k citations so far. This ```Pytorch```\n",
        "implementation started from the code in \n",
        "[torchvision tutorial]( https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py) and the implementation by \n",
        "[Yerlan Idelbayev](https://github.com/akamaster/pytorch_resnet_cifar10). \n",
        "We developed the code in Jupyter notebook and it \n",
        "is compatible with Google Colab platform to be used with GPU. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## How/Why ResNet models are working?\n",
        "There have been rigorous attempts to make deeper \n",
        "convolutional neural networks (CNN) since their \n",
        "advent in [2012](https://dl.acm.org/doi/10.1145/3065386) as \n",
        "the performance is believed to be tightly related to the  [complexity of the network](https://arxiv.org/abs/1409.1556). \n",
        "A major obstacle in training deeper neural networks is the well-known  [vanishing gradient](http://proceedings.mlr.press/v9/glorot10a.html) problem. \n",
        "As the layers are added to the network the \n",
        "multiplying gradients make the overall gradient \n",
        "infinitesimal which in turn causes very slow \n",
        "convergence if at all. \n",
        "\n",
        "The same training difficulties with *exploding gradients* \n",
        "can also hinder the learning process once layers are stacked in the \n",
        "neural networks. In an attempt to tackle the vanishing/exploding \n",
        "gradients in deeper networks, in 2015, a novel CNN architecture \n",
        "is introduced, which won the ImageNet classification \n",
        "competition in [ILSVRC 2015](https://arxiv.org/abs/1502.01852) by \n",
        "a good margin (2.84 %) from its competitors. \n",
        "\n",
        "The intuition behind the ResNet architecture is \n",
        "rather simple: Assuming that a neural network unit \n",
        "can learn any function, asymptotically, then it can learn the identity function as well. Why is it \n",
        "important to learn the identity function? The \n",
        "answer lies in the ResNet architecture. An example \n",
        "of a residual unit is shown in the following figure, \n",
        "taken from the paper. The input to the residual \n",
        "block is $X$ and the output is $\\mathcal{F}(X)+X$, \n",
        "therefore, by learning $\\mathcal{F}(X)=0$ this basic \n",
        "block is bypassed during the training process,which is equivalent to identity mapping. A cascade of these residual blocks is used \n",
        "to create very deep CNN models with more than 100 \n",
        "layers as presented in the original ResNet paper. \n",
        "\n",
        "<img align=\"center\" src=\"https://drive.google.com/uc?id=1c4QvJN4H_GdGWNM-vW46j_JIG64CD_mD\" />\n",
        "\n",
        "Note that a plain CNN model (without residual connections)posses the same solution space as the counterpart network \n",
        "with the residual connections, however, it is argued in \n",
        "[original ResNet paper](https://arxiv.org/abs/1512.03385) that \"If the optimal function is closer to an identity\n",
        "mapping than to a zero mapping, it should be easier for the\n",
        "solver to find the perturbations with reference to an identity\n",
        "mapping , than to learn the function like a new one.\n",
        "\" This hypothesis is backed up in the paper with \n",
        "several experiments in different datasets.\n",
        "In essence, the ResNet model gives a chance to the \n",
        "network to learn \"flexible\" depth for the CNN model \n",
        "and avoid the vanishing/exploding gradients if that \n",
        "hinders the optimization process. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Experiment Setup \n",
        "The authors train and test six different ResNet architectures for \n",
        "[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) and \n",
        "compare the results in Table 6 in the original paper.CIFAR10 image classification dataset consists of 50k training \n",
        "images and 10k testing images in 10 classes. The network inputs \n",
        "are $32\u0002\\times 32$ images, with the per-pixel mean subtracted. \n",
        "The first layer is $3\u0002\\times 3$ convolutions.\n",
        "There are totally $6n+2$ residual blocks stacked, \n",
        "where replacing $n$ with $3,5,7,9,18,200$ produces \n",
        "networks of depth $20,32,44,56,110,1202$ respectively. \n",
        "The architecture is summarized in the following table \n",
        "taken from the paper, where three columns represent \n",
        "three different feature-map sizes. \n",
        "\n",
        "<img align=\"center\" src=\"https://drive.google.com/uc?id=1W_k5HZ8lS9_h9BUOx9R0AvZPTqRVDSlT\" />\n",
        "\n",
        "Note that training in the original paper uses \n",
        "validation set to select the best performing model, \n",
        "however, this implementation does not use \n",
        "validation set to select the model, rather we \n",
        "use the train and validation set for training \n",
        "and testing the performance of a single model \n",
        "(reported in the paper). Also to train ResNet1202, \n",
        "you need 16GB memory on GPU, therefore, you can \n",
        "not run it on the Google CoLab platform.\n",
        "For the rest,we use the same setting as \n",
        "described in the original paper and we \n",
        "reproduce the following results in terms \n",
        "of performance error: \n",
        "\n",
        "| MODEL | PAPER   | OURS\n",
        "|------|------||\n",
        "|   ResNet-20  | 8.75|7.92|\n",
        "|   ResNet-32  |7.51 |7.27|\n",
        "|   ResNet-44  |7.17|6.88|\n",
        "|   ResNet-56  |6.97 |7.58|\n",
        "|   ResNet-110  | 6.43|6.54|\n",
        "|   ResNet-1202  | 7.93|?|\n",
        "\n",
        "Except for the ResNet1202, we reproduced the \n",
        "experiments with comparable or better results. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAEdiF4Ymizi"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Running the Experiment on Google Colab\n",
        "This notebook is running remotely on the Google Colab platform, therefore to save and access the trained model and checkpoints in your local computer you may need to mount the Google drive (gdrive). I used the following code snippet to set up a local drive on my computer. You need to do the same by specifying the path to your project directory.  Therefore, you need to create a directory in your gdrive for this project and change the paths in the code to your directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H418uCYl6OlQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48f9236d-3069-49e7-825f-56e332cc7c36"
      },
      "source": [
        "# First we need to mount the Google drive \n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# Here specify the path to your directory\n",
        "!ls \"/content/gdrive/My Drive/CIFAR10_ResNet\" \n",
        "root_path = 'gdrive/My Drive/CIFAR10_ResNet' \n",
        "path ='/content/gdrive/My Drive/CIFAR10_ResNet'\n",
        "os.chdir(path)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "resnet.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYUYwQltB9ip",
        "outputId": "dee92376-0948-4253-d645-6cf90dec9e22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile resnet.py\n",
        "\n",
        "'''\n",
        "Properly implemented ResNet-s for CIFAR10 as described in paper [1].\n",
        "The implementation and structure of this file is hugely influenced by [2]\n",
        "which is implemented for ImageNet and doesn't have option A for identity.\n",
        "Moreover, most of the implementations on the web is copy-paste from\n",
        "torchvision's resnet and has wrong number of params.\n",
        "Proper ResNet-s for CIFAR10 (for fair comparision and etc.) has following\n",
        "number of layers and parameters:\n",
        "name      | layers | params\n",
        "ResNet20  |    20  | 0.27M\n",
        "ResNet32  |    32  | 0.46M\n",
        "ResNet44  |    44  | 0.66M\n",
        "ResNet56  |    56  | 0.85M\n",
        "ResNet110 |   110  |  1.7M\n",
        "ResNet1202|  1202  | 19.4m\n",
        "which this implementation indeed has.\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "[2] https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
        "If you use this implementation in you work, please don't forget to mention the\n",
        "author, Yerlan Idelbayev.\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
        "\n",
        "def _weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    #print(classname)\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            if option == 'A':\n",
        "                \"\"\"\n",
        "                For CIFAR10 ResNet paper uses option A.\n",
        "                \"\"\"\n",
        "                self.shortcut = LambdaLayer(lambda x:\n",
        "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "            elif option == 'B':\n",
        "                self.shortcut = nn.Sequential(\n",
        "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                     nn.BatchNorm2d(self.expansion * planes)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def resnet20():\n",
        "    return ResNet(BasicBlock, [3, 3, 3])\n",
        "\n",
        "\n",
        "def resnet32():\n",
        "    return ResNet(BasicBlock, [5, 5, 5])\n",
        "\n",
        "\n",
        "def resnet44():\n",
        "    return ResNet(BasicBlock, [7, 7, 7])\n",
        "\n",
        "\n",
        "def resnet56():\n",
        "    return ResNet(BasicBlock, [9, 9, 9])\n",
        "\n",
        "\n",
        "def resnet110():\n",
        "    return ResNet(BasicBlock, [18, 18, 18])\n",
        "\n",
        "\n",
        "def resnet1202():\n",
        "    return ResNet(BasicBlock, [200, 200, 200])\n",
        "\n",
        "\n",
        "def test(net):\n",
        "    import numpy as np\n",
        "    total_params = 0\n",
        "\n",
        "    for x in filter(lambda p: p.requires_grad, net.parameters()):\n",
        "        total_params += np.prod(x.data.numpy().shape)\n",
        "    print(\"Total number of params\", total_params)\n",
        "    print(\"Total layers\", len(list(filter(lambda p: p.requires_grad and len(p.data.size())>1, net.parameters()))))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for net_name in __all__:\n",
        "        if net_name.startswith('resnet'):\n",
        "            print(net_name)\n",
        "            test(globals()[net_name]())\n",
        "            print()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting resnet.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjvbl18HsjnT"
      },
      "source": [
        "---\n",
        "## ResNet Architecture\n",
        "\n",
        "In the following code cell, the ResNet model is defined with all the related functions for initialization of the weights, the residual block and skip connections are implemented as presented in the original paper. By executing the cell you can see all possible ResNet architecture, with the number of their learning parameters and layers, that we use in this experiment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9Y2hYRwB-qg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ef87e70-08a0-413c-a62b-51f76e67dcce"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import resnet\n",
        "# We define all the classes and function regarding the ResNet architecture in this code cell\n",
        "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
        " \n",
        "def _weights_init(m):\n",
        "    \"\"\"\n",
        "        Initialization of CNN weights\n",
        "    \"\"\"\n",
        "    classname = m.__class__.__name__\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    \"\"\"\n",
        "      Identity mapping between ResNet blocks with diffrenet size feature map\n",
        "    \"\"\"\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "# A basic block as shown in Fig.3 (right) in the paper consists of two convolutional blocks, each followed by a Bach-Norm layer. \n",
        "# Every basic block is shortcuted in ResNet architecture to construct f(x)+x module. \n",
        "# Expansion for option 'A' in the paper is equal to identity with extra zero entries padded\n",
        "# for increasing dimensions between layers with different feature map size. This option introduces no extra parameter. \n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            if option == 'A':\n",
        "                \"\"\"\n",
        "                For CIFAR10 experiment, ResNet paper uses option A.\n",
        "                \"\"\"\n",
        "                self.shortcut = LambdaLayer(lambda x:\n",
        "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "            elif option == 'B':\n",
        "                self.shortcut = nn.Sequential(\n",
        "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                     nn.BatchNorm2d(self.expansion * planes)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "# Stack of 3 times 2*n (n is the number of basic blocks) layers are used for making the ResNet model, \n",
        "# where each 2n layers have feature maps of size {16,32,64}, respectively. \n",
        "# The subsampling is performed by convolutions with a stride of 2.\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        print(out.size())\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def resnet20():\n",
        "    return ResNet(BasicBlock, [3, 3, 3])\n",
        "\n",
        "\n",
        "def resnet32():\n",
        "    return ResNet(BasicBlock, [5, 5, 5])\n",
        "\n",
        "\n",
        "def resnet44():\n",
        "    return ResNet(BasicBlock, [7, 7, 7])\n",
        "\n",
        "\n",
        "def resnet56():\n",
        "    return ResNet(BasicBlock, [9, 9, 9])\n",
        "\n",
        "\n",
        "def resnet110():\n",
        "    return ResNet(BasicBlock, [18, 18, 18])\n",
        "\n",
        "\n",
        "def resnet1202():\n",
        "    return ResNet(BasicBlock, [200, 200, 200])\n",
        "\n",
        "\n",
        "def test(net):\n",
        "    total_params = 0\n",
        "\n",
        "    for x in filter(lambda p: p.requires_grad, net.parameters()):\n",
        "        total_params += np.prod(x.data.numpy().shape)\n",
        "    print(\"Total number of params\", total_params)\n",
        "    print(\"Total layers\", len(list(filter(lambda p: p.requires_grad and len(p.data.size())>1, net.parameters()))))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for net_name in __all__:\n",
        "        if net_name.startswith('resnet'):\n",
        "            test(globals()[net_name]())\n",
        "            print()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of params 269722\n",
            "Total layers 20\n",
            "\n",
            "Total number of params 464154\n",
            "Total layers 32\n",
            "\n",
            "Total number of params 658586\n",
            "Total layers 44\n",
            "\n",
            "Total number of params 853018\n",
            "Total layers 56\n",
            "\n",
            "Total number of params 1727962\n",
            "Total layers 110\n",
            "\n",
            "Total number of params 19421274\n",
            "Total layers 1202\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANRMm1zq2J7a"
      },
      "source": [
        "\n",
        "---\n",
        "# Hyperparameter Setting\n",
        "\n",
        "We define a class referred to as ```MyResNetArgs```in the following to assign the hyperparameters such as a number of training epochs, learning rate, momentum, batch size, etc. to the training function. The objects of this class are initialized inherently once created with a void argument. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIXmCsEZ6dFV"
      },
      "source": [
        " class MyResNetArgs:\n",
        "   \"\"\"\n",
        "    Passing the hyperparameters to the model\n",
        "   \"\"\"\n",
        "   def __init__(self, arch='resnet20' ,epochs=200, start_epoch=0, batch_size=128, lr=0.1, momentum=0.9, weight_decay=1e-4, print_freq=55,\n",
        "                 evaluate=0, pretrained=0, half=0, save_dir='save_temp', save_every=10):\n",
        "        self.save_every = save_every #Saves checkpoints at every specified number of epochs\n",
        "        self.save_dir = save_dir #The directory used to save the trained models\n",
        "        self.half = half #use half-precision(16-bit)\n",
        "        self.evaluate = evaluate #evaluate model on the validation set\n",
        "        self.pretrained = pretrained #evaluate the pretrained model on the validation set\n",
        "        self.print_freq = print_freq #print frequency \n",
        "        self.weight_decay = weight_decay\n",
        "        self.momentum = momentum \n",
        "        self.lr = lr #Learning rate\n",
        "        self.batch_size = batch_size \n",
        "        self.start_epoch = start_epoch\n",
        "        self.epochs = epochs\n",
        "        self.arch = arch #ResNet model\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpNzFe-13pWj"
      },
      "source": [
        "Now we can create an instance of ResNet model and inspect the architecture by printing the model summary. \n",
        "One can easily check the difference between different ResNet models to understand the construction units. There are totally $6n+2$ stacked weighted layers, e.g., for ResNet 20, there are 19 convolutional layers plus one fully connected layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djW0RO80_prY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4af7ab8-832d-42ec-cb5c-f22235927992"
      },
      "source": [
        "from torchsummary import summary\n",
        "args=MyResNetArgs('resnet20',pretrained=0)\n",
        "#model = resnet.__dict__[args.arch]()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "model = resnet.__dict__[args.arch]().to(device)\n",
        "summary(model, (3,32,32))\n",
        "best_prec1 = 0"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 32]             432\n",
            "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
            "            Conv2d-3           [-1, 16, 32, 32]           2,304\n",
            "       BatchNorm2d-4           [-1, 16, 32, 32]              32\n",
            "            Conv2d-5           [-1, 16, 32, 32]           2,304\n",
            "       BatchNorm2d-6           [-1, 16, 32, 32]              32\n",
            "        BasicBlock-7           [-1, 16, 32, 32]               0\n",
            "            Conv2d-8           [-1, 16, 32, 32]           2,304\n",
            "       BatchNorm2d-9           [-1, 16, 32, 32]              32\n",
            "           Conv2d-10           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-11           [-1, 16, 32, 32]              32\n",
            "       BasicBlock-12           [-1, 16, 32, 32]               0\n",
            "           Conv2d-13           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-14           [-1, 16, 32, 32]              32\n",
            "           Conv2d-15           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-16           [-1, 16, 32, 32]              32\n",
            "       BasicBlock-17           [-1, 16, 32, 32]               0\n",
            "           Conv2d-18           [-1, 32, 16, 16]           4,608\n",
            "      BatchNorm2d-19           [-1, 32, 16, 16]              64\n",
            "           Conv2d-20           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-21           [-1, 32, 16, 16]              64\n",
            "      LambdaLayer-22           [-1, 32, 16, 16]               0\n",
            "       BasicBlock-23           [-1, 32, 16, 16]               0\n",
            "           Conv2d-24           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-25           [-1, 32, 16, 16]              64\n",
            "           Conv2d-26           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-27           [-1, 32, 16, 16]              64\n",
            "       BasicBlock-28           [-1, 32, 16, 16]               0\n",
            "           Conv2d-29           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-30           [-1, 32, 16, 16]              64\n",
            "           Conv2d-31           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-32           [-1, 32, 16, 16]              64\n",
            "       BasicBlock-33           [-1, 32, 16, 16]               0\n",
            "           Conv2d-34             [-1, 64, 8, 8]          18,432\n",
            "      BatchNorm2d-35             [-1, 64, 8, 8]             128\n",
            "           Conv2d-36             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-37             [-1, 64, 8, 8]             128\n",
            "      LambdaLayer-38             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-39             [-1, 64, 8, 8]               0\n",
            "           Conv2d-40             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-41             [-1, 64, 8, 8]             128\n",
            "           Conv2d-42             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-43             [-1, 64, 8, 8]             128\n",
            "       BasicBlock-44             [-1, 64, 8, 8]               0\n",
            "           Conv2d-45             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-46             [-1, 64, 8, 8]             128\n",
            "           Conv2d-47             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-48             [-1, 64, 8, 8]             128\n",
            "       BasicBlock-49             [-1, 64, 8, 8]               0\n",
            "           Linear-50                   [-1, 10]             650\n",
            "================================================================\n",
            "Total params: 269,722\n",
            "Trainable params: 269,722\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 3.63\n",
            "Params size (MB): 1.03\n",
            "Estimated Total Size (MB): 4.67\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAb3r6OQ43ve"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Training and Validation\n",
        "The next two code blocks (```train``` and ```validate```) are for training and testing the model on the validation set. There is a print module at the end of the train function which prints the top-k classification accuracy and error at specified epochs. The checkpoints are also saved for the training model at ```save_every``` epoch steps, which is initialized to every $10$ epoch by default.  The average accuracy among mini-batches is also recorded for inspection purposes, which is calculated using the ```AverageMeter``` class. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKt6Q9Zlt8MU"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTjp-tkWtmmI"
      },
      "source": [
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    \"\"\"\n",
        "        Run one train epoch\n",
        "    \"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        target = target.cuda()\n",
        "        input_var = input.cuda()\n",
        "        target_var = target\n",
        "        if args.half:\n",
        "            input_var = input_var.half()\n",
        "\n",
        "        # compute output\n",
        "        output = model(input_var)\n",
        "        loss = criterion(output, target_var)\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        output = output.float()\n",
        "        loss = loss.float()\n",
        "        # measure accuracy and record loss\n",
        "        prec1 = accuracy(output.data, target)[0]\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "        top1.update(prec1.item(), input.size(0))\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % args.print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
        "                      data_time=data_time, loss=losses, top1=top1))\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B57y5hgMtzDe"
      },
      "source": [
        "def validate(val_loader, model, criterion):\n",
        "    \"\"\"\n",
        "    Run evaluation\n",
        "    \"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    with torch.no_grad():\n",
        "        for i, (input, target) in enumerate(val_loader):\n",
        "            target = target.cuda()\n",
        "            input_var = input.cuda()\n",
        "            target_var = target.cuda()\n",
        "\n",
        "            if args.half:\n",
        "                input_var = input_var.half()\n",
        "\n",
        "            # compute output\n",
        "            output = model(input_var)\n",
        "            loss = criterion(output, target_var)\n",
        "\n",
        "            output = output.float()\n",
        "            loss = loss.float()\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            prec1 = accuracy(output.data, target)[0]\n",
        "            losses.update(loss.item(), input.size(0))\n",
        "            top1.update(prec1.item(), input.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "\n",
        "    print('Test\\t  Prec@1: {top1.avg:.3f} (Err: {error:.3f} )\\n'\n",
        "          .format(top1=top1,error=100-top1.avg))\n",
        "\n",
        "    return top1.avg\n",
        "\n",
        "def save_checkpoint(state, filename='checkpoint.th'):\n",
        "    \"\"\"\n",
        "    Save the training model\n",
        "    \"\"\"\n",
        "    torch.save(state, filename)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwM5UxsVXdRd"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Dataset \n",
        "\n",
        "Now that we defined our ResNet model, we need to download and prepare [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset to start the experiment. This is impressively easy with ```\n",
        "torchvision```. We use ```Dataloader``` to download the train and the validation set. We show random samples in a batch of training images for a better understanding of data. Note that CIFAR-10 images are quite small in size $32 \\times 32$. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5R-ck_wWQDD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374,
          "referenced_widgets": [
            "adf9f995ccde40a0859cdbf3d7aaf697",
            "b6690f33fa97436d87a5b4a78a6c1f1b",
            "903160d45eee40758e25e9e8f1cf09d6",
            "035c8dea93414696a17f49359ed767de",
            "fa9c792cc89a4d7388a8c11c7e1b3a0d",
            "9fd4a8dc00824edf99e8cc1ffe177d87",
            "9be061d2af4c4afd94ee98dc8830f976",
            "5ddbb0064ac64214be4f65cd3778cf45"
          ]
        },
        "outputId": "41c4f200-4b6a-4382-ae70-f2270a3bf00c"
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(32, 4),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ]), download=True),\n",
        "        batch_size=args.batch_size, shuffle=True,\n",
        "        num_workers=4, pin_memory=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])),\n",
        "        batch_size=128, shuffle=False,\n",
        "        num_workers=4, pin_memory=True)\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# functions to show an image\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "plt.figure(figsize=(20,10)) \n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images[0:8,:,:]))\n",
        "# print labels\n",
        "print(' '.join('%15s' % classes[labels[j]] for j in range(8)))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "adf9f995ccde40a0859cdbf3d7aaf697",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "            cat            frog            frog           plane           plane             dog            deer            ship\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 1150,
              "height": 179
            },
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OKFeRNm7mJA"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Running the Experiment\n",
        "Here comes the main script using all the functions explained above to execute the train and the test procedure for a specified ResNet model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OONRZVaZtP0f"
      },
      "source": [
        "import time\n",
        "\n",
        "def main():\n",
        "    global args, best_prec1\n",
        "    \n",
        "    # Check the save_dir exists or not\n",
        "    if not os.path.exists(args.save_dir):\n",
        "        os.makedirs(args.save_dir)\n",
        "\n",
        "    model = resnet.__dict__[args.arch]()\n",
        "    model.cuda()\n",
        "\n",
        "\n",
        "    # define loss function (criterion) and pptimizer\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "    if args.half:\n",
        "        print('half persicion is used.')\n",
        "        model.half()\n",
        "        criterion.half()\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
        "                                momentum=args.momentum,\n",
        "                                weight_decay=args.weight_decay)\n",
        "\n",
        "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                                        milestones=[100, 150], last_epoch=args.start_epoch - 1)\n",
        "\n",
        "    if args.arch in ['resnet1202']:\n",
        "        # for resnet1202 original paper uses lr=0.01 for first 400 minibatches for warm-up\n",
        "        # then switch back. In this setup it will correspond for first epoch.\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = args.lr*0.1\n",
        "\n",
        "\n",
        "    if args.evaluate:\n",
        "        print('evalution mode')\n",
        "        model.load_state_dict(torch.load(os.path.join(args.save_dir, 'model.th')))\n",
        "        best_prec1 = validate(val_loader, model, criterion)\n",
        "        return best_prec1\n",
        "\n",
        "    if args.pretrained:\n",
        "        print('evalution of pretrained model')\n",
        "        args.save_dir='pretrained_models'\n",
        "        pretrained_model= args.arch +'.th'\n",
        "        model.load_state_dict(torch.load(os.path.join(args.save_dir, pretrained_model)))\n",
        "        best_prec1 = validate(val_loader, model, criterion)\n",
        "        return best_prec1\n",
        "\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "\n",
        "        # train for one epoch\n",
        "        print('Training {} model'.format(args.arch))\n",
        "        print('current lr {:.5e}'.format(optimizer.param_groups[0]['lr']))\n",
        "        train(train_loader, model, criterion, optimizer, epoch)\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # evaluate on validation set\n",
        "        prec1 = validate(val_loader, model, criterion)\n",
        "\n",
        "        # remember best prec@1 and save checkpoint\n",
        "        is_best = prec1 > best_prec1\n",
        "        best_prec1 = max(prec1, best_prec1)\n",
        "\n",
        "        if epoch > 0 and epoch % args.save_every == 0:\n",
        "            save_checkpoint(model.state_dict(), filename=os.path.join(args.save_dir, 'checkpoint.th'))\n",
        "        if is_best:\n",
        "            save_checkpoint(model.state_dict(), filename=os.path.join(args.save_dir, 'model.th'))\n",
        "\n",
        "    return best_prec1"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su50THmTmXMh"
      },
      "source": [
        "Since Google Colab imposes time restriction, we can not loop over all ResNet models to reproduce the results in one go, instead, we run each ResNet model separately by setting the name of the model (e.g. resnet20)  manually in the hyperparameters and record the results once training is finished for the specified network. We saved the trained models in a separate directory to do the inference later. One can try to run the main script several times for each network and report the mean and the variance performance for more reliable results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkntiMU2uKk6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96f260f4-a9c6-4361-fc40-19891bbf9de1"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "   best_prec1 = main()\n",
        "   print('The lowest error from {} model after {} epochs is {error:.3f}'.format(args.arch,args.epochs,error=100-best_prec1)) "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [0][0/391]\tLoss 3.3945 (3.3945)\tPrec@1 10.938 (10.938)\n",
            "Epoch: [0][55/391]\tLoss 1.7172 (2.1816)\tPrec@1 28.125 (23.661)\n",
            "Epoch: [0][110/391]\tLoss 1.5869 (1.9402)\tPrec@1 39.844 (29.512)\n",
            "Epoch: [0][165/391]\tLoss 1.5357 (1.8364)\tPrec@1 39.062 (32.605)\n",
            "Epoch: [0][220/391]\tLoss 1.4699 (1.7511)\tPrec@1 49.219 (35.641)\n",
            "Epoch: [0][275/391]\tLoss 1.4936 (1.6765)\tPrec@1 42.969 (38.335)\n",
            "Epoch: [0][330/391]\tLoss 1.2453 (1.6135)\tPrec@1 54.688 (40.790)\n",
            "Epoch: [0][385/391]\tLoss 1.2503 (1.5591)\tPrec@1 58.594 (42.876)\n",
            "Test\t  Prec@1: 50.110 (Err: 49.890 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [1][0/391]\tLoss 1.2353 (1.2353)\tPrec@1 57.812 (57.812)\n",
            "Epoch: [1][55/391]\tLoss 1.0528 (1.1670)\tPrec@1 58.594 (57.799)\n",
            "Epoch: [1][110/391]\tLoss 1.0640 (1.1370)\tPrec@1 63.281 (59.072)\n",
            "Epoch: [1][165/391]\tLoss 1.0580 (1.1140)\tPrec@1 61.719 (59.982)\n",
            "Epoch: [1][220/391]\tLoss 1.1345 (1.0999)\tPrec@1 53.125 (60.496)\n",
            "Epoch: [1][275/391]\tLoss 0.9850 (1.0788)\tPrec@1 65.625 (61.192)\n",
            "Epoch: [1][330/391]\tLoss 1.1833 (1.0573)\tPrec@1 53.906 (61.985)\n",
            "Epoch: [1][385/391]\tLoss 0.9113 (1.0390)\tPrec@1 67.188 (62.672)\n",
            "Test\t  Prec@1: 59.520 (Err: 40.480 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [2][0/391]\tLoss 0.9132 (0.9132)\tPrec@1 67.969 (67.969)\n",
            "Epoch: [2][55/391]\tLoss 0.9154 (0.9133)\tPrec@1 67.188 (67.425)\n",
            "Epoch: [2][110/391]\tLoss 0.8649 (0.8993)\tPrec@1 67.969 (67.751)\n",
            "Epoch: [2][165/391]\tLoss 0.6690 (0.8824)\tPrec@1 77.344 (68.505)\n",
            "Epoch: [2][220/391]\tLoss 0.8338 (0.8685)\tPrec@1 69.531 (69.157)\n",
            "Epoch: [2][275/391]\tLoss 0.9830 (0.8605)\tPrec@1 62.500 (69.486)\n",
            "Epoch: [2][330/391]\tLoss 0.8946 (0.8474)\tPrec@1 70.312 (70.041)\n",
            "Epoch: [2][385/391]\tLoss 0.7165 (0.8380)\tPrec@1 74.219 (70.485)\n",
            "Test\t  Prec@1: 71.740 (Err: 28.260 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [3][0/391]\tLoss 0.5669 (0.5669)\tPrec@1 81.250 (81.250)\n",
            "Epoch: [3][55/391]\tLoss 0.7001 (0.7164)\tPrec@1 74.219 (75.112)\n",
            "Epoch: [3][110/391]\tLoss 0.9284 (0.7252)\tPrec@1 68.750 (74.493)\n",
            "Epoch: [3][165/391]\tLoss 0.6573 (0.7265)\tPrec@1 76.562 (74.398)\n",
            "Epoch: [3][220/391]\tLoss 0.6897 (0.7193)\tPrec@1 76.562 (74.593)\n",
            "Epoch: [3][275/391]\tLoss 0.8980 (0.7169)\tPrec@1 73.438 (74.708)\n",
            "Epoch: [3][330/391]\tLoss 0.5265 (0.7131)\tPrec@1 82.812 (74.837)\n",
            "Epoch: [3][385/391]\tLoss 0.6307 (0.7073)\tPrec@1 82.031 (75.142)\n",
            "Test\t  Prec@1: 74.890 (Err: 25.110 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [4][0/391]\tLoss 0.5617 (0.5617)\tPrec@1 81.250 (81.250)\n",
            "Epoch: [4][55/391]\tLoss 0.7317 (0.6599)\tPrec@1 75.781 (76.855)\n",
            "Epoch: [4][110/391]\tLoss 0.4933 (0.6537)\tPrec@1 83.594 (77.428)\n",
            "Epoch: [4][165/391]\tLoss 0.5638 (0.6525)\tPrec@1 81.250 (77.334)\n",
            "Epoch: [4][220/391]\tLoss 0.8030 (0.6504)\tPrec@1 68.750 (77.326)\n",
            "Epoch: [4][275/391]\tLoss 0.7648 (0.6443)\tPrec@1 72.656 (77.494)\n",
            "Epoch: [4][330/391]\tLoss 0.6011 (0.6457)\tPrec@1 78.906 (77.433)\n",
            "Epoch: [4][385/391]\tLoss 0.6437 (0.6442)\tPrec@1 79.688 (77.514)\n",
            "Test\t  Prec@1: 74.030 (Err: 25.970 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [5][0/391]\tLoss 0.6558 (0.6558)\tPrec@1 81.250 (81.250)\n",
            "Epoch: [5][55/391]\tLoss 0.5183 (0.5914)\tPrec@1 84.375 (79.688)\n",
            "Epoch: [5][110/391]\tLoss 0.6500 (0.6045)\tPrec@1 77.344 (78.991)\n",
            "Epoch: [5][165/391]\tLoss 0.5745 (0.5954)\tPrec@1 77.344 (79.363)\n",
            "Epoch: [5][220/391]\tLoss 0.5725 (0.5931)\tPrec@1 81.250 (79.461)\n",
            "Epoch: [5][275/391]\tLoss 0.4760 (0.5942)\tPrec@1 83.594 (79.427)\n",
            "Epoch: [5][330/391]\tLoss 0.5588 (0.5900)\tPrec@1 81.250 (79.520)\n",
            "Epoch: [5][385/391]\tLoss 0.6410 (0.5911)\tPrec@1 75.781 (79.522)\n",
            "Test\t  Prec@1: 75.750 (Err: 24.250 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [6][0/391]\tLoss 0.5994 (0.5994)\tPrec@1 78.906 (78.906)\n",
            "Epoch: [6][55/391]\tLoss 0.6731 (0.5501)\tPrec@1 78.125 (80.790)\n",
            "Epoch: [6][110/391]\tLoss 0.6113 (0.5529)\tPrec@1 79.688 (80.842)\n",
            "Epoch: [6][165/391]\tLoss 0.6664 (0.5592)\tPrec@1 70.312 (80.676)\n",
            "Epoch: [6][220/391]\tLoss 0.6390 (0.5643)\tPrec@1 78.125 (80.412)\n",
            "Epoch: [6][275/391]\tLoss 0.4215 (0.5602)\tPrec@1 81.250 (80.667)\n",
            "Epoch: [6][330/391]\tLoss 0.5775 (0.5581)\tPrec@1 76.562 (80.769)\n",
            "Epoch: [6][385/391]\tLoss 0.5745 (0.5575)\tPrec@1 80.469 (80.827)\n",
            "Test\t  Prec@1: 76.750 (Err: 23.250 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [7][0/391]\tLoss 0.4628 (0.4628)\tPrec@1 85.938 (85.938)\n",
            "Epoch: [7][55/391]\tLoss 0.4865 (0.5280)\tPrec@1 85.156 (82.073)\n",
            "Epoch: [7][110/391]\tLoss 0.5491 (0.5270)\tPrec@1 82.031 (82.200)\n",
            "Epoch: [7][165/391]\tLoss 0.5272 (0.5290)\tPrec@1 82.031 (81.979)\n",
            "Epoch: [7][220/391]\tLoss 0.6920 (0.5271)\tPrec@1 79.688 (81.922)\n",
            "Epoch: [7][275/391]\tLoss 0.3482 (0.5314)\tPrec@1 89.844 (81.745)\n",
            "Epoch: [7][330/391]\tLoss 0.6353 (0.5279)\tPrec@1 78.125 (81.857)\n",
            "Epoch: [7][385/391]\tLoss 0.4349 (0.5265)\tPrec@1 84.375 (81.884)\n",
            "Test\t  Prec@1: 79.030 (Err: 20.970 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [8][0/391]\tLoss 0.5825 (0.5825)\tPrec@1 78.125 (78.125)\n",
            "Epoch: [8][55/391]\tLoss 0.3826 (0.4853)\tPrec@1 87.500 (83.440)\n",
            "Epoch: [8][110/391]\tLoss 0.5114 (0.4931)\tPrec@1 82.031 (83.052)\n",
            "Epoch: [8][165/391]\tLoss 0.6611 (0.4935)\tPrec@1 80.469 (83.015)\n",
            "Epoch: [8][220/391]\tLoss 0.5412 (0.4944)\tPrec@1 82.031 (82.957)\n",
            "Epoch: [8][275/391]\tLoss 0.5598 (0.4968)\tPrec@1 81.250 (82.861)\n",
            "Epoch: [8][330/391]\tLoss 0.5713 (0.4992)\tPrec@1 83.594 (82.808)\n",
            "Epoch: [8][385/391]\tLoss 0.4664 (0.4979)\tPrec@1 83.594 (82.891)\n",
            "Test\t  Prec@1: 76.520 (Err: 23.480 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [9][0/391]\tLoss 0.5641 (0.5641)\tPrec@1 81.250 (81.250)\n",
            "Epoch: [9][55/391]\tLoss 0.5040 (0.4729)\tPrec@1 83.594 (84.012)\n",
            "Epoch: [9][110/391]\tLoss 0.4943 (0.4651)\tPrec@1 84.375 (83.868)\n",
            "Epoch: [9][165/391]\tLoss 0.4538 (0.4641)\tPrec@1 84.375 (83.857)\n",
            "Epoch: [9][220/391]\tLoss 0.5207 (0.4723)\tPrec@1 80.469 (83.399)\n",
            "Epoch: [9][275/391]\tLoss 0.4544 (0.4743)\tPrec@1 84.375 (83.333)\n",
            "Epoch: [9][330/391]\tLoss 0.4330 (0.4741)\tPrec@1 82.812 (83.327)\n",
            "Epoch: [9][385/391]\tLoss 0.5649 (0.4768)\tPrec@1 79.688 (83.288)\n",
            "Test\t  Prec@1: 79.850 (Err: 20.150 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [10][0/391]\tLoss 0.4357 (0.4357)\tPrec@1 85.156 (85.156)\n",
            "Epoch: [10][55/391]\tLoss 0.4922 (0.4615)\tPrec@1 85.156 (84.124)\n",
            "Epoch: [10][110/391]\tLoss 0.4610 (0.4572)\tPrec@1 84.375 (84.481)\n",
            "Epoch: [10][165/391]\tLoss 0.5956 (0.4566)\tPrec@1 78.125 (84.422)\n",
            "Epoch: [10][220/391]\tLoss 0.4206 (0.4498)\tPrec@1 88.281 (84.566)\n",
            "Epoch: [10][275/391]\tLoss 0.4576 (0.4565)\tPrec@1 82.812 (84.330)\n",
            "Epoch: [10][330/391]\tLoss 0.5212 (0.4593)\tPrec@1 82.031 (84.248)\n",
            "Epoch: [10][385/391]\tLoss 0.3619 (0.4578)\tPrec@1 86.719 (84.324)\n",
            "Test\t  Prec@1: 77.110 (Err: 22.890 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [11][0/391]\tLoss 0.2493 (0.2493)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [11][55/391]\tLoss 0.3501 (0.4298)\tPrec@1 86.719 (85.198)\n",
            "Epoch: [11][110/391]\tLoss 0.3872 (0.4276)\tPrec@1 85.938 (85.241)\n",
            "Epoch: [11][165/391]\tLoss 0.3763 (0.4367)\tPrec@1 85.156 (84.794)\n",
            "Epoch: [11][220/391]\tLoss 0.4264 (0.4371)\tPrec@1 85.156 (84.838)\n",
            "Epoch: [11][275/391]\tLoss 0.5625 (0.4391)\tPrec@1 82.812 (84.788)\n",
            "Epoch: [11][330/391]\tLoss 0.6588 (0.4421)\tPrec@1 76.562 (84.672)\n",
            "Epoch: [11][385/391]\tLoss 0.6729 (0.4455)\tPrec@1 75.000 (84.541)\n",
            "Test\t  Prec@1: 80.580 (Err: 19.420 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [12][0/391]\tLoss 0.4550 (0.4550)\tPrec@1 81.250 (81.250)\n",
            "Epoch: [12][55/391]\tLoss 0.6068 (0.4043)\tPrec@1 81.250 (85.896)\n",
            "Epoch: [12][110/391]\tLoss 0.4004 (0.4164)\tPrec@1 84.375 (85.656)\n",
            "Epoch: [12][165/391]\tLoss 0.5642 (0.4289)\tPrec@1 78.906 (85.180)\n",
            "Epoch: [12][220/391]\tLoss 0.3775 (0.4291)\tPrec@1 85.156 (85.057)\n",
            "Epoch: [12][275/391]\tLoss 0.4074 (0.4309)\tPrec@1 85.938 (85.029)\n",
            "Epoch: [12][330/391]\tLoss 0.4435 (0.4295)\tPrec@1 84.375 (85.118)\n",
            "Epoch: [12][385/391]\tLoss 0.4348 (0.4289)\tPrec@1 87.500 (85.183)\n",
            "Test\t  Prec@1: 81.830 (Err: 18.170 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [13][0/391]\tLoss 0.4342 (0.4342)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [13][55/391]\tLoss 0.3430 (0.3993)\tPrec@1 88.281 (86.733)\n",
            "Epoch: [13][110/391]\tLoss 0.4935 (0.4127)\tPrec@1 82.812 (86.156)\n",
            "Epoch: [13][165/391]\tLoss 0.4420 (0.4135)\tPrec@1 83.594 (85.947)\n",
            "Epoch: [13][220/391]\tLoss 0.3815 (0.4143)\tPrec@1 85.938 (85.892)\n",
            "Epoch: [13][275/391]\tLoss 0.4538 (0.4169)\tPrec@1 85.156 (85.691)\n",
            "Epoch: [13][330/391]\tLoss 0.3413 (0.4156)\tPrec@1 88.281 (85.697)\n",
            "Epoch: [13][385/391]\tLoss 0.5254 (0.4135)\tPrec@1 79.688 (85.749)\n",
            "Test\t  Prec@1: 81.860 (Err: 18.140 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [14][0/391]\tLoss 0.3971 (0.3971)\tPrec@1 87.500 (87.500)\n",
            "Epoch: [14][55/391]\tLoss 0.4004 (0.3847)\tPrec@1 85.938 (86.398)\n",
            "Epoch: [14][110/391]\tLoss 0.4564 (0.4006)\tPrec@1 85.156 (86.135)\n",
            "Epoch: [14][165/391]\tLoss 0.5249 (0.3983)\tPrec@1 84.375 (86.173)\n",
            "Epoch: [14][220/391]\tLoss 0.5289 (0.3998)\tPrec@1 80.469 (86.111)\n",
            "Epoch: [14][275/391]\tLoss 0.4162 (0.4011)\tPrec@1 85.156 (86.130)\n",
            "Epoch: [14][330/391]\tLoss 0.5008 (0.4019)\tPrec@1 82.031 (86.060)\n",
            "Epoch: [14][385/391]\tLoss 0.5372 (0.4018)\tPrec@1 83.594 (86.089)\n",
            "Test\t  Prec@1: 79.000 (Err: 21.000 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [15][0/391]\tLoss 0.2615 (0.2615)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [15][55/391]\tLoss 0.3821 (0.3838)\tPrec@1 87.500 (86.579)\n",
            "Epoch: [15][110/391]\tLoss 0.3923 (0.3857)\tPrec@1 83.594 (86.641)\n",
            "Epoch: [15][165/391]\tLoss 0.3822 (0.3850)\tPrec@1 86.719 (86.639)\n",
            "Epoch: [15][220/391]\tLoss 0.3406 (0.3947)\tPrec@1 88.281 (86.111)\n",
            "Epoch: [15][275/391]\tLoss 0.3208 (0.3909)\tPrec@1 87.500 (86.322)\n",
            "Epoch: [15][330/391]\tLoss 0.3561 (0.3934)\tPrec@1 88.281 (86.313)\n",
            "Epoch: [15][385/391]\tLoss 0.4526 (0.3942)\tPrec@1 85.938 (86.365)\n",
            "Test\t  Prec@1: 82.580 (Err: 17.420 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [16][0/391]\tLoss 0.3354 (0.3354)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [16][55/391]\tLoss 0.3497 (0.3642)\tPrec@1 88.281 (87.681)\n",
            "Epoch: [16][110/391]\tLoss 0.3374 (0.3761)\tPrec@1 89.062 (87.134)\n",
            "Epoch: [16][165/391]\tLoss 0.3747 (0.3765)\tPrec@1 87.500 (86.959)\n",
            "Epoch: [16][220/391]\tLoss 0.2984 (0.3725)\tPrec@1 92.969 (86.945)\n",
            "Epoch: [16][275/391]\tLoss 0.3891 (0.3742)\tPrec@1 85.156 (86.900)\n",
            "Epoch: [16][330/391]\tLoss 0.2853 (0.3785)\tPrec@1 90.625 (86.806)\n",
            "Epoch: [16][385/391]\tLoss 0.3823 (0.3795)\tPrec@1 85.938 (86.709)\n",
            "Test\t  Prec@1: 83.260 (Err: 16.740 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [17][0/391]\tLoss 0.3066 (0.3066)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [17][55/391]\tLoss 0.4033 (0.3555)\tPrec@1 85.938 (87.863)\n",
            "Epoch: [17][110/391]\tLoss 0.3171 (0.3756)\tPrec@1 91.406 (87.064)\n",
            "Epoch: [17][165/391]\tLoss 0.3916 (0.3770)\tPrec@1 86.719 (87.039)\n",
            "Epoch: [17][220/391]\tLoss 0.4602 (0.3777)\tPrec@1 82.031 (86.991)\n",
            "Epoch: [17][275/391]\tLoss 0.3017 (0.3823)\tPrec@1 89.844 (86.767)\n",
            "Epoch: [17][330/391]\tLoss 0.2893 (0.3845)\tPrec@1 89.844 (86.676)\n",
            "Epoch: [17][385/391]\tLoss 0.3899 (0.3825)\tPrec@1 88.281 (86.784)\n",
            "Test\t  Prec@1: 85.000 (Err: 15.000 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [18][0/391]\tLoss 0.2876 (0.2876)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [18][55/391]\tLoss 0.3879 (0.3638)\tPrec@1 87.500 (87.500)\n",
            "Epoch: [18][110/391]\tLoss 0.3297 (0.3578)\tPrec@1 89.062 (87.521)\n",
            "Epoch: [18][165/391]\tLoss 0.4335 (0.3616)\tPrec@1 85.938 (87.382)\n",
            "Epoch: [18][220/391]\tLoss 0.6004 (0.3602)\tPrec@1 78.125 (87.359)\n",
            "Epoch: [18][275/391]\tLoss 0.4169 (0.3635)\tPrec@1 83.594 (87.299)\n",
            "Epoch: [18][330/391]\tLoss 0.3417 (0.3659)\tPrec@1 86.719 (87.174)\n",
            "Epoch: [18][385/391]\tLoss 0.4861 (0.3677)\tPrec@1 83.594 (87.170)\n",
            "Test\t  Prec@1: 83.540 (Err: 16.460 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [19][0/391]\tLoss 0.3570 (0.3570)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [19][55/391]\tLoss 0.3445 (0.3436)\tPrec@1 87.500 (87.751)\n",
            "Epoch: [19][110/391]\tLoss 0.3287 (0.3494)\tPrec@1 85.938 (87.620)\n",
            "Epoch: [19][165/391]\tLoss 0.3347 (0.3557)\tPrec@1 90.625 (87.444)\n",
            "Epoch: [19][220/391]\tLoss 0.3425 (0.3574)\tPrec@1 89.844 (87.514)\n",
            "Epoch: [19][275/391]\tLoss 0.3577 (0.3575)\tPrec@1 89.844 (87.449)\n",
            "Epoch: [19][330/391]\tLoss 0.3391 (0.3585)\tPrec@1 86.719 (87.441)\n",
            "Epoch: [19][385/391]\tLoss 0.3670 (0.3619)\tPrec@1 85.156 (87.358)\n",
            "Test\t  Prec@1: 80.890 (Err: 19.110 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [20][0/391]\tLoss 0.4469 (0.4469)\tPrec@1 85.156 (85.156)\n",
            "Epoch: [20][55/391]\tLoss 0.2491 (0.3491)\tPrec@1 93.750 (88.002)\n",
            "Epoch: [20][110/391]\tLoss 0.4378 (0.3403)\tPrec@1 83.594 (88.253)\n",
            "Epoch: [20][165/391]\tLoss 0.2165 (0.3414)\tPrec@1 92.188 (88.272)\n",
            "Epoch: [20][220/391]\tLoss 0.2817 (0.3451)\tPrec@1 90.625 (87.981)\n",
            "Epoch: [20][275/391]\tLoss 0.3632 (0.3481)\tPrec@1 85.938 (87.893)\n",
            "Epoch: [20][330/391]\tLoss 0.4183 (0.3496)\tPrec@1 87.500 (87.748)\n",
            "Epoch: [20][385/391]\tLoss 0.2737 (0.3513)\tPrec@1 91.406 (87.708)\n",
            "Test\t  Prec@1: 83.600 (Err: 16.400 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [21][0/391]\tLoss 0.4603 (0.4603)\tPrec@1 85.156 (85.156)\n",
            "Epoch: [21][55/391]\tLoss 0.3369 (0.3523)\tPrec@1 89.844 (87.891)\n",
            "Epoch: [21][110/391]\tLoss 0.3033 (0.3514)\tPrec@1 89.062 (88.042)\n",
            "Epoch: [21][165/391]\tLoss 0.3268 (0.3495)\tPrec@1 89.844 (87.966)\n",
            "Epoch: [21][220/391]\tLoss 0.3046 (0.3474)\tPrec@1 89.844 (88.009)\n",
            "Epoch: [21][275/391]\tLoss 0.4275 (0.3482)\tPrec@1 85.938 (88.043)\n",
            "Epoch: [21][330/391]\tLoss 0.3544 (0.3454)\tPrec@1 89.062 (88.125)\n",
            "Epoch: [21][385/391]\tLoss 0.3480 (0.3457)\tPrec@1 89.062 (88.057)\n",
            "Test\t  Prec@1: 82.110 (Err: 17.890 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [22][0/391]\tLoss 0.3350 (0.3350)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [22][55/391]\tLoss 0.2922 (0.3117)\tPrec@1 87.500 (89.118)\n",
            "Epoch: [22][110/391]\tLoss 0.2925 (0.3173)\tPrec@1 92.188 (89.020)\n",
            "Epoch: [22][165/391]\tLoss 0.2944 (0.3246)\tPrec@1 89.062 (88.822)\n",
            "Epoch: [22][220/391]\tLoss 0.2815 (0.3319)\tPrec@1 92.188 (88.536)\n",
            "Epoch: [22][275/391]\tLoss 0.3673 (0.3359)\tPrec@1 86.719 (88.366)\n",
            "Epoch: [22][330/391]\tLoss 0.3511 (0.3377)\tPrec@1 86.719 (88.284)\n",
            "Epoch: [22][385/391]\tLoss 0.6155 (0.3435)\tPrec@1 81.250 (88.075)\n",
            "Test\t  Prec@1: 82.080 (Err: 17.920 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [23][0/391]\tLoss 0.2581 (0.2581)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [23][55/391]\tLoss 0.4348 (0.3204)\tPrec@1 82.812 (88.644)\n",
            "Epoch: [23][110/391]\tLoss 0.2853 (0.3331)\tPrec@1 91.406 (88.190)\n",
            "Epoch: [23][165/391]\tLoss 0.3773 (0.3371)\tPrec@1 88.281 (88.079)\n",
            "Epoch: [23][220/391]\tLoss 0.2926 (0.3347)\tPrec@1 88.281 (88.221)\n",
            "Epoch: [23][275/391]\tLoss 0.3919 (0.3367)\tPrec@1 87.500 (88.171)\n",
            "Epoch: [23][330/391]\tLoss 0.2991 (0.3394)\tPrec@1 90.625 (88.163)\n",
            "Epoch: [23][385/391]\tLoss 0.3784 (0.3413)\tPrec@1 85.938 (88.144)\n",
            "Test\t  Prec@1: 82.730 (Err: 17.270 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [24][0/391]\tLoss 0.3426 (0.3426)\tPrec@1 83.594 (83.594)\n",
            "Epoch: [24][55/391]\tLoss 0.3335 (0.3182)\tPrec@1 89.844 (88.770)\n",
            "Epoch: [24][110/391]\tLoss 0.3973 (0.3139)\tPrec@1 87.500 (89.027)\n",
            "Epoch: [24][165/391]\tLoss 0.3663 (0.3215)\tPrec@1 86.719 (88.775)\n",
            "Epoch: [24][220/391]\tLoss 0.3701 (0.3269)\tPrec@1 87.500 (88.734)\n",
            "Epoch: [24][275/391]\tLoss 0.2173 (0.3286)\tPrec@1 91.406 (88.714)\n",
            "Epoch: [24][330/391]\tLoss 0.2419 (0.3339)\tPrec@1 89.844 (88.510)\n",
            "Epoch: [24][385/391]\tLoss 0.2670 (0.3345)\tPrec@1 89.844 (88.486)\n",
            "Test\t  Prec@1: 83.350 (Err: 16.650 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [25][0/391]\tLoss 0.3197 (0.3197)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [25][55/391]\tLoss 0.2156 (0.3035)\tPrec@1 90.625 (89.328)\n",
            "Epoch: [25][110/391]\tLoss 0.3413 (0.3144)\tPrec@1 87.500 (89.175)\n",
            "Epoch: [25][165/391]\tLoss 0.3370 (0.3138)\tPrec@1 89.062 (89.091)\n",
            "Epoch: [25][220/391]\tLoss 0.4541 (0.3154)\tPrec@1 80.469 (88.946)\n",
            "Epoch: [25][275/391]\tLoss 0.3869 (0.3239)\tPrec@1 88.281 (88.672)\n",
            "Epoch: [25][330/391]\tLoss 0.2975 (0.3253)\tPrec@1 90.625 (88.614)\n",
            "Epoch: [25][385/391]\tLoss 0.3823 (0.3255)\tPrec@1 88.281 (88.583)\n",
            "Test\t  Prec@1: 82.350 (Err: 17.650 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [26][0/391]\tLoss 0.3306 (0.3306)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [26][55/391]\tLoss 0.2512 (0.3182)\tPrec@1 93.750 (88.937)\n",
            "Epoch: [26][110/391]\tLoss 0.2342 (0.3188)\tPrec@1 91.406 (88.795)\n",
            "Epoch: [26][165/391]\tLoss 0.2740 (0.3146)\tPrec@1 90.625 (89.039)\n",
            "Epoch: [26][220/391]\tLoss 0.2821 (0.3144)\tPrec@1 91.406 (89.062)\n",
            "Epoch: [26][275/391]\tLoss 0.2613 (0.3158)\tPrec@1 91.406 (89.046)\n",
            "Epoch: [26][330/391]\tLoss 0.2628 (0.3176)\tPrec@1 92.188 (88.994)\n",
            "Epoch: [26][385/391]\tLoss 0.3345 (0.3195)\tPrec@1 86.719 (88.913)\n",
            "Test\t  Prec@1: 86.050 (Err: 13.950 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [27][0/391]\tLoss 0.3537 (0.3537)\tPrec@1 87.500 (87.500)\n",
            "Epoch: [27][55/391]\tLoss 0.2990 (0.3046)\tPrec@1 89.062 (89.174)\n",
            "Epoch: [27][110/391]\tLoss 0.2893 (0.3037)\tPrec@1 90.625 (89.309)\n",
            "Epoch: [27][165/391]\tLoss 0.2849 (0.3136)\tPrec@1 88.281 (89.001)\n",
            "Epoch: [27][220/391]\tLoss 0.3170 (0.3143)\tPrec@1 88.281 (88.949)\n",
            "Epoch: [27][275/391]\tLoss 0.2502 (0.3186)\tPrec@1 90.625 (88.830)\n",
            "Epoch: [27][330/391]\tLoss 0.2996 (0.3205)\tPrec@1 89.844 (88.817)\n",
            "Epoch: [27][385/391]\tLoss 0.2264 (0.3217)\tPrec@1 91.406 (88.769)\n",
            "Test\t  Prec@1: 83.780 (Err: 16.220 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [28][0/391]\tLoss 0.2962 (0.2962)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [28][55/391]\tLoss 0.2869 (0.3062)\tPrec@1 91.406 (89.439)\n",
            "Epoch: [28][110/391]\tLoss 0.2923 (0.3028)\tPrec@1 92.969 (89.520)\n",
            "Epoch: [28][165/391]\tLoss 0.2499 (0.3037)\tPrec@1 92.969 (89.406)\n",
            "Epoch: [28][220/391]\tLoss 0.2780 (0.3062)\tPrec@1 88.281 (89.345)\n",
            "Epoch: [28][275/391]\tLoss 0.2887 (0.3071)\tPrec@1 89.844 (89.289)\n",
            "Epoch: [28][330/391]\tLoss 0.3385 (0.3081)\tPrec@1 87.500 (89.270)\n",
            "Epoch: [28][385/391]\tLoss 0.4914 (0.3098)\tPrec@1 82.812 (89.145)\n",
            "Test\t  Prec@1: 80.960 (Err: 19.040 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [29][0/391]\tLoss 0.3754 (0.3754)\tPrec@1 85.938 (85.938)\n",
            "Epoch: [29][55/391]\tLoss 0.2912 (0.3076)\tPrec@1 93.750 (89.425)\n",
            "Epoch: [29][110/391]\tLoss 0.2800 (0.2988)\tPrec@1 89.062 (89.604)\n",
            "Epoch: [29][165/391]\tLoss 0.3111 (0.2975)\tPrec@1 87.500 (89.580)\n",
            "Epoch: [29][220/391]\tLoss 0.3189 (0.3021)\tPrec@1 90.625 (89.409)\n",
            "Epoch: [29][275/391]\tLoss 0.2883 (0.3070)\tPrec@1 88.281 (89.275)\n",
            "Epoch: [29][330/391]\tLoss 0.2874 (0.3073)\tPrec@1 92.969 (89.273)\n",
            "Epoch: [29][385/391]\tLoss 0.1985 (0.3091)\tPrec@1 94.531 (89.202)\n",
            "Test\t  Prec@1: 78.810 (Err: 21.190 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [30][0/391]\tLoss 0.2344 (0.2344)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [30][55/391]\tLoss 0.2988 (0.2778)\tPrec@1 89.844 (90.262)\n",
            "Epoch: [30][110/391]\tLoss 0.2872 (0.2850)\tPrec@1 89.844 (90.153)\n",
            "Epoch: [30][165/391]\tLoss 0.2080 (0.2920)\tPrec@1 95.312 (89.881)\n",
            "Epoch: [30][220/391]\tLoss 0.3537 (0.2958)\tPrec@1 86.719 (89.731)\n",
            "Epoch: [30][275/391]\tLoss 0.3245 (0.2987)\tPrec@1 88.281 (89.716)\n",
            "Epoch: [30][330/391]\tLoss 0.3831 (0.3053)\tPrec@1 89.062 (89.551)\n",
            "Epoch: [30][385/391]\tLoss 0.3156 (0.3063)\tPrec@1 89.844 (89.504)\n",
            "Test\t  Prec@1: 85.540 (Err: 14.460 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [31][0/391]\tLoss 0.3004 (0.3004)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [31][55/391]\tLoss 0.2216 (0.2812)\tPrec@1 90.625 (90.053)\n",
            "Epoch: [31][110/391]\tLoss 0.2215 (0.2850)\tPrec@1 90.625 (90.027)\n",
            "Epoch: [31][165/391]\tLoss 0.2620 (0.2922)\tPrec@1 90.625 (89.717)\n",
            "Epoch: [31][220/391]\tLoss 0.4372 (0.2945)\tPrec@1 85.156 (89.720)\n",
            "Epoch: [31][275/391]\tLoss 0.2361 (0.2937)\tPrec@1 92.188 (89.736)\n",
            "Epoch: [31][330/391]\tLoss 0.1783 (0.2961)\tPrec@1 96.094 (89.664)\n",
            "Epoch: [31][385/391]\tLoss 0.2979 (0.2993)\tPrec@1 92.188 (89.556)\n",
            "Test\t  Prec@1: 84.990 (Err: 15.010 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [32][0/391]\tLoss 0.2841 (0.2841)\tPrec@1 87.500 (87.500)\n",
            "Epoch: [32][55/391]\tLoss 0.2514 (0.2927)\tPrec@1 89.844 (89.900)\n",
            "Epoch: [32][110/391]\tLoss 0.2232 (0.2940)\tPrec@1 90.625 (89.865)\n",
            "Epoch: [32][165/391]\tLoss 0.2940 (0.2916)\tPrec@1 86.719 (89.816)\n",
            "Epoch: [32][220/391]\tLoss 0.1886 (0.2933)\tPrec@1 96.875 (89.819)\n",
            "Epoch: [32][275/391]\tLoss 0.3497 (0.2975)\tPrec@1 89.844 (89.745)\n",
            "Epoch: [32][330/391]\tLoss 0.4220 (0.3026)\tPrec@1 84.375 (89.480)\n",
            "Epoch: [32][385/391]\tLoss 0.3482 (0.3024)\tPrec@1 85.938 (89.429)\n",
            "Test\t  Prec@1: 84.970 (Err: 15.030 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [33][0/391]\tLoss 0.3601 (0.3601)\tPrec@1 85.938 (85.938)\n",
            "Epoch: [33][55/391]\tLoss 0.2251 (0.2762)\tPrec@1 90.625 (90.234)\n",
            "Epoch: [33][110/391]\tLoss 0.2355 (0.2773)\tPrec@1 92.188 (90.301)\n",
            "Epoch: [33][165/391]\tLoss 0.2599 (0.2834)\tPrec@1 91.406 (90.056)\n",
            "Epoch: [33][220/391]\tLoss 0.2811 (0.2925)\tPrec@1 89.844 (89.734)\n",
            "Epoch: [33][275/391]\tLoss 0.2522 (0.2909)\tPrec@1 91.406 (89.841)\n",
            "Epoch: [33][330/391]\tLoss 0.3331 (0.2914)\tPrec@1 89.062 (89.806)\n",
            "Epoch: [33][385/391]\tLoss 0.3721 (0.2912)\tPrec@1 89.062 (89.832)\n",
            "Test\t  Prec@1: 85.790 (Err: 14.210 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [34][0/391]\tLoss 0.2880 (0.2880)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [34][55/391]\tLoss 0.2738 (0.2820)\tPrec@1 89.844 (89.927)\n",
            "Epoch: [34][110/391]\tLoss 0.2854 (0.2867)\tPrec@1 89.844 (89.773)\n",
            "Epoch: [34][165/391]\tLoss 0.4810 (0.2899)\tPrec@1 85.156 (89.811)\n",
            "Epoch: [34][220/391]\tLoss 0.2973 (0.2931)\tPrec@1 91.406 (89.653)\n",
            "Epoch: [34][275/391]\tLoss 0.2463 (0.2963)\tPrec@1 90.625 (89.535)\n",
            "Epoch: [34][330/391]\tLoss 0.4449 (0.2941)\tPrec@1 85.156 (89.681)\n",
            "Epoch: [34][385/391]\tLoss 0.2605 (0.2956)\tPrec@1 89.062 (89.579)\n",
            "Test\t  Prec@1: 81.470 (Err: 18.530 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [35][0/391]\tLoss 0.3215 (0.3215)\tPrec@1 85.156 (85.156)\n",
            "Epoch: [35][55/391]\tLoss 0.3379 (0.2849)\tPrec@1 92.188 (90.179)\n",
            "Epoch: [35][110/391]\tLoss 0.2741 (0.2874)\tPrec@1 90.625 (90.034)\n",
            "Epoch: [35][165/391]\tLoss 0.1909 (0.2854)\tPrec@1 91.406 (90.107)\n",
            "Epoch: [35][220/391]\tLoss 0.2695 (0.2841)\tPrec@1 89.062 (90.201)\n",
            "Epoch: [35][275/391]\tLoss 0.2443 (0.2875)\tPrec@1 89.844 (90.042)\n",
            "Epoch: [35][330/391]\tLoss 0.2790 (0.2909)\tPrec@1 88.281 (89.893)\n",
            "Epoch: [35][385/391]\tLoss 0.2378 (0.2940)\tPrec@1 92.188 (89.736)\n",
            "Test\t  Prec@1: 84.280 (Err: 15.720 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [36][0/391]\tLoss 0.2260 (0.2260)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [36][55/391]\tLoss 0.2766 (0.2791)\tPrec@1 89.062 (90.137)\n",
            "Epoch: [36][110/391]\tLoss 0.3441 (0.2863)\tPrec@1 90.625 (89.865)\n",
            "Epoch: [36][165/391]\tLoss 0.3478 (0.2838)\tPrec@1 90.625 (89.976)\n",
            "Epoch: [36][220/391]\tLoss 0.2502 (0.2905)\tPrec@1 89.844 (89.713)\n",
            "Epoch: [36][275/391]\tLoss 0.3755 (0.2916)\tPrec@1 87.500 (89.714)\n",
            "Epoch: [36][330/391]\tLoss 0.3258 (0.2879)\tPrec@1 87.500 (89.874)\n",
            "Epoch: [36][385/391]\tLoss 0.2849 (0.2885)\tPrec@1 92.188 (89.761)\n",
            "Test\t  Prec@1: 84.680 (Err: 15.320 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [37][0/391]\tLoss 0.3153 (0.3153)\tPrec@1 87.500 (87.500)\n",
            "Epoch: [37][55/391]\tLoss 0.2768 (0.2884)\tPrec@1 89.844 (90.109)\n",
            "Epoch: [37][110/391]\tLoss 0.3399 (0.2880)\tPrec@1 90.625 (90.287)\n",
            "Epoch: [37][165/391]\tLoss 0.2625 (0.2820)\tPrec@1 90.625 (90.441)\n",
            "Epoch: [37][220/391]\tLoss 0.2670 (0.2806)\tPrec@1 89.844 (90.427)\n",
            "Epoch: [37][275/391]\tLoss 0.3424 (0.2828)\tPrec@1 89.062 (90.297)\n",
            "Epoch: [37][330/391]\tLoss 0.3068 (0.2845)\tPrec@1 89.062 (90.179)\n",
            "Epoch: [37][385/391]\tLoss 0.3625 (0.2883)\tPrec@1 87.500 (90.050)\n",
            "Test\t  Prec@1: 85.860 (Err: 14.140 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [38][0/391]\tLoss 0.2484 (0.2484)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [38][55/391]\tLoss 0.3330 (0.2689)\tPrec@1 85.938 (90.374)\n",
            "Epoch: [38][110/391]\tLoss 0.1809 (0.2624)\tPrec@1 93.750 (90.667)\n",
            "Epoch: [38][165/391]\tLoss 0.2792 (0.2721)\tPrec@1 90.625 (90.432)\n",
            "Epoch: [38][220/391]\tLoss 0.2516 (0.2774)\tPrec@1 92.188 (90.222)\n",
            "Epoch: [38][275/391]\tLoss 0.2872 (0.2769)\tPrec@1 87.500 (90.226)\n",
            "Epoch: [38][330/391]\tLoss 0.3658 (0.2802)\tPrec@1 89.062 (90.118)\n",
            "Epoch: [38][385/391]\tLoss 0.2212 (0.2814)\tPrec@1 92.969 (90.093)\n",
            "Test\t  Prec@1: 85.420 (Err: 14.580 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [39][0/391]\tLoss 0.2394 (0.2394)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [39][55/391]\tLoss 0.2386 (0.2539)\tPrec@1 90.625 (90.918)\n",
            "Epoch: [39][110/391]\tLoss 0.2445 (0.2733)\tPrec@1 90.625 (90.526)\n",
            "Epoch: [39][165/391]\tLoss 0.3038 (0.2738)\tPrec@1 91.406 (90.404)\n",
            "Epoch: [39][220/391]\tLoss 0.2427 (0.2750)\tPrec@1 89.844 (90.409)\n",
            "Epoch: [39][275/391]\tLoss 0.2276 (0.2777)\tPrec@1 90.625 (90.268)\n",
            "Epoch: [39][330/391]\tLoss 0.2489 (0.2796)\tPrec@1 88.281 (90.186)\n",
            "Epoch: [39][385/391]\tLoss 0.3171 (0.2813)\tPrec@1 89.062 (90.149)\n",
            "Test\t  Prec@1: 84.100 (Err: 15.900 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [40][0/391]\tLoss 0.1862 (0.1862)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [40][55/391]\tLoss 0.3375 (0.2691)\tPrec@1 85.156 (90.499)\n",
            "Epoch: [40][110/391]\tLoss 0.2330 (0.2726)\tPrec@1 91.406 (90.512)\n",
            "Epoch: [40][165/391]\tLoss 0.3575 (0.2717)\tPrec@1 87.500 (90.489)\n",
            "Epoch: [40][220/391]\tLoss 0.2751 (0.2747)\tPrec@1 92.188 (90.296)\n",
            "Epoch: [40][275/391]\tLoss 0.5265 (0.2755)\tPrec@1 87.500 (90.348)\n",
            "Epoch: [40][330/391]\tLoss 0.1954 (0.2744)\tPrec@1 94.531 (90.394)\n",
            "Epoch: [40][385/391]\tLoss 0.3555 (0.2792)\tPrec@1 85.938 (90.244)\n",
            "Test\t  Prec@1: 83.490 (Err: 16.510 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [41][0/391]\tLoss 0.3218 (0.3218)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [41][55/391]\tLoss 0.4279 (0.2566)\tPrec@1 85.938 (90.834)\n",
            "Epoch: [41][110/391]\tLoss 0.3507 (0.2598)\tPrec@1 86.719 (90.970)\n",
            "Epoch: [41][165/391]\tLoss 0.3811 (0.2591)\tPrec@1 85.938 (91.044)\n",
            "Epoch: [41][220/391]\tLoss 0.3964 (0.2639)\tPrec@1 85.938 (90.869)\n",
            "Epoch: [41][275/391]\tLoss 0.3052 (0.2693)\tPrec@1 88.281 (90.676)\n",
            "Epoch: [41][330/391]\tLoss 0.2719 (0.2710)\tPrec@1 91.406 (90.599)\n",
            "Epoch: [41][385/391]\tLoss 0.4225 (0.2737)\tPrec@1 85.156 (90.449)\n",
            "Test\t  Prec@1: 81.630 (Err: 18.370 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [42][0/391]\tLoss 0.2398 (0.2398)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [42][55/391]\tLoss 0.3092 (0.2539)\tPrec@1 90.625 (91.183)\n",
            "Epoch: [42][110/391]\tLoss 0.3403 (0.2648)\tPrec@1 87.500 (90.716)\n",
            "Epoch: [42][165/391]\tLoss 0.2462 (0.2707)\tPrec@1 87.500 (90.493)\n",
            "Epoch: [42][220/391]\tLoss 0.2666 (0.2767)\tPrec@1 89.844 (90.300)\n",
            "Epoch: [42][275/391]\tLoss 0.3009 (0.2786)\tPrec@1 92.969 (90.268)\n",
            "Epoch: [42][330/391]\tLoss 0.2786 (0.2785)\tPrec@1 90.625 (90.262)\n",
            "Epoch: [42][385/391]\tLoss 0.2479 (0.2778)\tPrec@1 90.625 (90.263)\n",
            "Test\t  Prec@1: 86.600 (Err: 13.400 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [43][0/391]\tLoss 0.2548 (0.2548)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [43][55/391]\tLoss 0.2509 (0.2692)\tPrec@1 92.188 (90.611)\n",
            "Epoch: [43][110/391]\tLoss 0.2872 (0.2689)\tPrec@1 88.281 (90.442)\n",
            "Epoch: [43][165/391]\tLoss 0.1827 (0.2655)\tPrec@1 93.750 (90.639)\n",
            "Epoch: [43][220/391]\tLoss 0.1706 (0.2655)\tPrec@1 95.312 (90.643)\n",
            "Epoch: [43][275/391]\tLoss 0.2926 (0.2636)\tPrec@1 92.188 (90.730)\n",
            "Epoch: [43][330/391]\tLoss 0.3689 (0.2668)\tPrec@1 89.844 (90.642)\n",
            "Epoch: [43][385/391]\tLoss 0.1492 (0.2676)\tPrec@1 95.312 (90.635)\n",
            "Test\t  Prec@1: 80.410 (Err: 19.590 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [44][0/391]\tLoss 0.3228 (0.3228)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [44][55/391]\tLoss 0.2144 (0.2460)\tPrec@1 92.188 (91.588)\n",
            "Epoch: [44][110/391]\tLoss 0.2696 (0.2430)\tPrec@1 90.625 (91.688)\n",
            "Epoch: [44][165/391]\tLoss 0.2337 (0.2508)\tPrec@1 92.188 (91.307)\n",
            "Epoch: [44][220/391]\tLoss 0.1988 (0.2616)\tPrec@1 92.188 (90.819)\n",
            "Epoch: [44][275/391]\tLoss 0.2853 (0.2648)\tPrec@1 89.062 (90.730)\n",
            "Epoch: [44][330/391]\tLoss 0.3080 (0.2657)\tPrec@1 89.844 (90.649)\n",
            "Epoch: [44][385/391]\tLoss 0.2781 (0.2695)\tPrec@1 89.844 (90.562)\n",
            "Test\t  Prec@1: 87.060 (Err: 12.940 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [45][0/391]\tLoss 0.3710 (0.3710)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [45][55/391]\tLoss 0.3208 (0.2662)\tPrec@1 91.406 (90.792)\n",
            "Epoch: [45][110/391]\tLoss 0.3707 (0.2672)\tPrec@1 81.250 (90.618)\n",
            "Epoch: [45][165/391]\tLoss 0.3663 (0.2693)\tPrec@1 89.062 (90.573)\n",
            "Epoch: [45][220/391]\tLoss 0.1829 (0.2660)\tPrec@1 93.750 (90.696)\n",
            "Epoch: [45][275/391]\tLoss 0.2742 (0.2685)\tPrec@1 89.062 (90.585)\n",
            "Epoch: [45][330/391]\tLoss 0.3358 (0.2712)\tPrec@1 86.719 (90.500)\n",
            "Epoch: [45][385/391]\tLoss 0.3893 (0.2709)\tPrec@1 87.500 (90.538)\n",
            "Test\t  Prec@1: 85.750 (Err: 14.250 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [46][0/391]\tLoss 0.2690 (0.2690)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [46][55/391]\tLoss 0.2293 (0.2505)\tPrec@1 91.406 (91.044)\n",
            "Epoch: [46][110/391]\tLoss 0.1682 (0.2507)\tPrec@1 95.312 (91.104)\n",
            "Epoch: [46][165/391]\tLoss 0.3977 (0.2512)\tPrec@1 85.156 (91.176)\n",
            "Epoch: [46][220/391]\tLoss 0.2518 (0.2588)\tPrec@1 92.188 (90.862)\n",
            "Epoch: [46][275/391]\tLoss 0.3397 (0.2642)\tPrec@1 85.156 (90.713)\n",
            "Epoch: [46][330/391]\tLoss 0.3905 (0.2645)\tPrec@1 85.938 (90.630)\n",
            "Epoch: [46][385/391]\tLoss 0.2072 (0.2657)\tPrec@1 93.750 (90.593)\n",
            "Test\t  Prec@1: 82.880 (Err: 17.120 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [47][0/391]\tLoss 0.2055 (0.2055)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [47][55/391]\tLoss 0.2736 (0.2576)\tPrec@1 90.625 (90.890)\n",
            "Epoch: [47][110/391]\tLoss 0.3797 (0.2582)\tPrec@1 88.281 (90.907)\n",
            "Epoch: [47][165/391]\tLoss 0.2264 (0.2593)\tPrec@1 89.844 (90.969)\n",
            "Epoch: [47][220/391]\tLoss 0.2152 (0.2614)\tPrec@1 93.750 (90.971)\n",
            "Epoch: [47][275/391]\tLoss 0.3247 (0.2635)\tPrec@1 88.281 (90.834)\n",
            "Epoch: [47][330/391]\tLoss 0.4117 (0.2639)\tPrec@1 86.719 (90.795)\n",
            "Epoch: [47][385/391]\tLoss 0.1857 (0.2650)\tPrec@1 95.312 (90.769)\n",
            "Test\t  Prec@1: 83.620 (Err: 16.380 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [48][0/391]\tLoss 0.2781 (0.2781)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [48][55/391]\tLoss 0.2005 (0.2435)\tPrec@1 92.969 (91.378)\n",
            "Epoch: [48][110/391]\tLoss 0.2634 (0.2580)\tPrec@1 92.188 (91.104)\n",
            "Epoch: [48][165/391]\tLoss 0.2419 (0.2606)\tPrec@1 92.188 (91.053)\n",
            "Epoch: [48][220/391]\tLoss 0.2367 (0.2625)\tPrec@1 90.625 (90.848)\n",
            "Epoch: [48][275/391]\tLoss 0.2231 (0.2630)\tPrec@1 92.188 (90.826)\n",
            "Epoch: [48][330/391]\tLoss 0.3683 (0.2632)\tPrec@1 89.062 (90.741)\n",
            "Epoch: [48][385/391]\tLoss 0.3680 (0.2647)\tPrec@1 86.719 (90.663)\n",
            "Test\t  Prec@1: 83.970 (Err: 16.030 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [49][0/391]\tLoss 0.2416 (0.2416)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [49][55/391]\tLoss 0.2849 (0.2451)\tPrec@1 90.625 (91.044)\n",
            "Epoch: [49][110/391]\tLoss 0.2563 (0.2457)\tPrec@1 90.625 (91.322)\n",
            "Epoch: [49][165/391]\tLoss 0.2830 (0.2499)\tPrec@1 89.844 (91.152)\n",
            "Epoch: [49][220/391]\tLoss 0.2660 (0.2581)\tPrec@1 90.625 (90.858)\n",
            "Epoch: [49][275/391]\tLoss 0.2359 (0.2581)\tPrec@1 92.969 (90.843)\n",
            "Epoch: [49][330/391]\tLoss 0.2464 (0.2616)\tPrec@1 92.188 (90.757)\n",
            "Epoch: [49][385/391]\tLoss 0.2995 (0.2638)\tPrec@1 88.281 (90.680)\n",
            "Test\t  Prec@1: 79.450 (Err: 20.550 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [50][0/391]\tLoss 0.2644 (0.2644)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [50][55/391]\tLoss 0.1432 (0.2656)\tPrec@1 93.750 (90.695)\n",
            "Epoch: [50][110/391]\tLoss 0.2486 (0.2487)\tPrec@1 91.406 (91.244)\n",
            "Epoch: [50][165/391]\tLoss 0.1332 (0.2494)\tPrec@1 96.094 (91.213)\n",
            "Epoch: [50][220/391]\tLoss 0.2177 (0.2496)\tPrec@1 92.188 (91.254)\n",
            "Epoch: [50][275/391]\tLoss 0.2806 (0.2511)\tPrec@1 91.406 (91.219)\n",
            "Epoch: [50][330/391]\tLoss 0.3947 (0.2556)\tPrec@1 85.156 (91.069)\n",
            "Epoch: [50][385/391]\tLoss 0.1833 (0.2563)\tPrec@1 96.094 (91.034)\n",
            "Test\t  Prec@1: 86.250 (Err: 13.750 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [51][0/391]\tLoss 0.2185 (0.2185)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [51][55/391]\tLoss 0.2048 (0.2402)\tPrec@1 91.406 (91.755)\n",
            "Epoch: [51][110/391]\tLoss 0.2100 (0.2429)\tPrec@1 92.188 (91.547)\n",
            "Epoch: [51][165/391]\tLoss 0.3585 (0.2436)\tPrec@1 87.500 (91.500)\n",
            "Epoch: [51][220/391]\tLoss 0.1873 (0.2477)\tPrec@1 96.875 (91.378)\n",
            "Epoch: [51][275/391]\tLoss 0.1618 (0.2465)\tPrec@1 94.531 (91.432)\n",
            "Epoch: [51][330/391]\tLoss 0.3033 (0.2485)\tPrec@1 90.625 (91.317)\n",
            "Epoch: [51][385/391]\tLoss 0.3068 (0.2517)\tPrec@1 91.406 (91.184)\n",
            "Test\t  Prec@1: 82.170 (Err: 17.830 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [52][0/391]\tLoss 0.3200 (0.3200)\tPrec@1 87.500 (87.500)\n",
            "Epoch: [52][55/391]\tLoss 0.2814 (0.2545)\tPrec@1 88.281 (90.834)\n",
            "Epoch: [52][110/391]\tLoss 0.2516 (0.2537)\tPrec@1 89.844 (91.012)\n",
            "Epoch: [52][165/391]\tLoss 0.2634 (0.2519)\tPrec@1 88.281 (91.067)\n",
            "Epoch: [52][220/391]\tLoss 0.3466 (0.2547)\tPrec@1 86.719 (91.042)\n",
            "Epoch: [52][275/391]\tLoss 0.3079 (0.2587)\tPrec@1 87.500 (91.018)\n",
            "Epoch: [52][330/391]\tLoss 0.1577 (0.2588)\tPrec@1 96.094 (91.019)\n",
            "Epoch: [52][385/391]\tLoss 0.3476 (0.2589)\tPrec@1 88.281 (90.991)\n",
            "Test\t  Prec@1: 79.950 (Err: 20.050 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [53][0/391]\tLoss 0.1631 (0.1631)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [53][55/391]\tLoss 0.2380 (0.2344)\tPrec@1 93.750 (91.699)\n",
            "Epoch: [53][110/391]\tLoss 0.2088 (0.2423)\tPrec@1 89.844 (91.617)\n",
            "Epoch: [53][165/391]\tLoss 0.2255 (0.2443)\tPrec@1 93.750 (91.538)\n",
            "Epoch: [53][220/391]\tLoss 0.2830 (0.2534)\tPrec@1 89.844 (91.081)\n",
            "Epoch: [53][275/391]\tLoss 0.4341 (0.2564)\tPrec@1 86.719 (91.013)\n",
            "Epoch: [53][330/391]\tLoss 0.2850 (0.2550)\tPrec@1 91.406 (91.106)\n",
            "Epoch: [53][385/391]\tLoss 0.2108 (0.2531)\tPrec@1 92.188 (91.151)\n",
            "Test\t  Prec@1: 84.610 (Err: 15.390 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [54][0/391]\tLoss 0.2028 (0.2028)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [54][55/391]\tLoss 0.2603 (0.2422)\tPrec@1 89.062 (91.406)\n",
            "Epoch: [54][110/391]\tLoss 0.3245 (0.2453)\tPrec@1 89.062 (91.441)\n",
            "Epoch: [54][165/391]\tLoss 0.2235 (0.2506)\tPrec@1 91.406 (91.204)\n",
            "Epoch: [54][220/391]\tLoss 0.2814 (0.2549)\tPrec@1 90.625 (91.053)\n",
            "Epoch: [54][275/391]\tLoss 0.2928 (0.2545)\tPrec@1 89.062 (90.982)\n",
            "Epoch: [54][330/391]\tLoss 0.2724 (0.2557)\tPrec@1 93.750 (90.993)\n",
            "Epoch: [54][385/391]\tLoss 0.2390 (0.2567)\tPrec@1 91.406 (90.981)\n",
            "Test\t  Prec@1: 85.460 (Err: 14.540 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [55][0/391]\tLoss 0.2096 (0.2096)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [55][55/391]\tLoss 0.2436 (0.2229)\tPrec@1 91.406 (91.992)\n",
            "Epoch: [55][110/391]\tLoss 0.3049 (0.2333)\tPrec@1 89.062 (91.772)\n",
            "Epoch: [55][165/391]\tLoss 0.2703 (0.2418)\tPrec@1 92.188 (91.590)\n",
            "Epoch: [55][220/391]\tLoss 0.2090 (0.2448)\tPrec@1 92.969 (91.509)\n",
            "Epoch: [55][275/391]\tLoss 0.2738 (0.2463)\tPrec@1 89.062 (91.389)\n",
            "Epoch: [55][330/391]\tLoss 0.2971 (0.2462)\tPrec@1 88.281 (91.394)\n",
            "Epoch: [55][385/391]\tLoss 0.3060 (0.2505)\tPrec@1 86.719 (91.240)\n",
            "Test\t  Prec@1: 85.160 (Err: 14.840 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [56][0/391]\tLoss 0.2328 (0.2328)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [56][55/391]\tLoss 0.1580 (0.2345)\tPrec@1 93.750 (91.406)\n",
            "Epoch: [56][110/391]\tLoss 0.2158 (0.2467)\tPrec@1 91.406 (91.357)\n",
            "Epoch: [56][165/391]\tLoss 0.2747 (0.2444)\tPrec@1 90.625 (91.491)\n",
            "Epoch: [56][220/391]\tLoss 0.3067 (0.2471)\tPrec@1 89.062 (91.374)\n",
            "Epoch: [56][275/391]\tLoss 0.1945 (0.2467)\tPrec@1 95.312 (91.386)\n",
            "Epoch: [56][330/391]\tLoss 0.3672 (0.2504)\tPrec@1 86.719 (91.236)\n",
            "Epoch: [56][385/391]\tLoss 0.3416 (0.2551)\tPrec@1 87.500 (91.014)\n",
            "Test\t  Prec@1: 87.090 (Err: 12.910 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [57][0/391]\tLoss 0.3087 (0.3087)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [57][55/391]\tLoss 0.2377 (0.2411)\tPrec@1 89.844 (91.378)\n",
            "Epoch: [57][110/391]\tLoss 0.3203 (0.2486)\tPrec@1 85.156 (91.322)\n",
            "Epoch: [57][165/391]\tLoss 0.2675 (0.2479)\tPrec@1 90.625 (91.303)\n",
            "Epoch: [57][220/391]\tLoss 0.3079 (0.2487)\tPrec@1 88.281 (91.321)\n",
            "Epoch: [57][275/391]\tLoss 0.2527 (0.2496)\tPrec@1 89.844 (91.282)\n",
            "Epoch: [57][330/391]\tLoss 0.2238 (0.2486)\tPrec@1 90.625 (91.317)\n",
            "Epoch: [57][385/391]\tLoss 0.3065 (0.2483)\tPrec@1 90.625 (91.325)\n",
            "Test\t  Prec@1: 85.110 (Err: 14.890 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [58][0/391]\tLoss 0.3090 (0.3090)\tPrec@1 87.500 (87.500)\n",
            "Epoch: [58][55/391]\tLoss 0.3097 (0.2496)\tPrec@1 92.969 (91.532)\n",
            "Epoch: [58][110/391]\tLoss 0.2970 (0.2438)\tPrec@1 85.156 (91.413)\n",
            "Epoch: [58][165/391]\tLoss 0.3057 (0.2449)\tPrec@1 89.844 (91.430)\n",
            "Epoch: [58][220/391]\tLoss 0.2346 (0.2458)\tPrec@1 92.188 (91.364)\n",
            "Epoch: [58][275/391]\tLoss 0.3020 (0.2450)\tPrec@1 89.062 (91.415)\n",
            "Epoch: [58][330/391]\tLoss 0.3805 (0.2482)\tPrec@1 90.625 (91.298)\n",
            "Epoch: [58][385/391]\tLoss 0.3255 (0.2502)\tPrec@1 87.500 (91.210)\n",
            "Test\t  Prec@1: 86.930 (Err: 13.070 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [59][0/391]\tLoss 0.1552 (0.1552)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [59][55/391]\tLoss 0.3503 (0.2215)\tPrec@1 89.062 (92.271)\n",
            "Epoch: [59][110/391]\tLoss 0.2242 (0.2319)\tPrec@1 92.188 (91.864)\n",
            "Epoch: [59][165/391]\tLoss 0.1609 (0.2369)\tPrec@1 95.312 (91.679)\n",
            "Epoch: [59][220/391]\tLoss 0.1652 (0.2420)\tPrec@1 95.312 (91.523)\n",
            "Epoch: [59][275/391]\tLoss 0.2229 (0.2449)\tPrec@1 92.188 (91.338)\n",
            "Epoch: [59][330/391]\tLoss 0.2887 (0.2448)\tPrec@1 89.062 (91.333)\n",
            "Epoch: [59][385/391]\tLoss 0.1702 (0.2491)\tPrec@1 95.312 (91.171)\n",
            "Test\t  Prec@1: 85.840 (Err: 14.160 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [60][0/391]\tLoss 0.2040 (0.2040)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [60][55/391]\tLoss 0.3898 (0.2313)\tPrec@1 85.938 (91.908)\n",
            "Epoch: [60][110/391]\tLoss 0.1129 (0.2301)\tPrec@1 96.094 (91.927)\n",
            "Epoch: [60][165/391]\tLoss 0.2385 (0.2388)\tPrec@1 90.625 (91.618)\n",
            "Epoch: [60][220/391]\tLoss 0.2228 (0.2439)\tPrec@1 89.844 (91.385)\n",
            "Epoch: [60][275/391]\tLoss 0.2982 (0.2447)\tPrec@1 90.625 (91.423)\n",
            "Epoch: [60][330/391]\tLoss 0.2816 (0.2444)\tPrec@1 86.719 (91.456)\n",
            "Epoch: [60][385/391]\tLoss 0.2262 (0.2472)\tPrec@1 90.625 (91.370)\n",
            "Test\t  Prec@1: 84.120 (Err: 15.880 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [61][0/391]\tLoss 0.1315 (0.1315)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [61][55/391]\tLoss 0.1430 (0.2376)\tPrec@1 95.312 (91.713)\n",
            "Epoch: [61][110/391]\tLoss 0.2330 (0.2353)\tPrec@1 91.406 (91.744)\n",
            "Epoch: [61][165/391]\tLoss 0.2657 (0.2412)\tPrec@1 92.969 (91.566)\n",
            "Epoch: [61][220/391]\tLoss 0.2295 (0.2414)\tPrec@1 92.188 (91.526)\n",
            "Epoch: [61][275/391]\tLoss 0.2278 (0.2438)\tPrec@1 94.531 (91.469)\n",
            "Epoch: [61][330/391]\tLoss 0.2650 (0.2465)\tPrec@1 89.844 (91.397)\n",
            "Epoch: [61][385/391]\tLoss 0.1969 (0.2466)\tPrec@1 92.969 (91.402)\n",
            "Test\t  Prec@1: 86.160 (Err: 13.840 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [62][0/391]\tLoss 0.3711 (0.3711)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [62][55/391]\tLoss 0.2769 (0.2372)\tPrec@1 90.625 (91.602)\n",
            "Epoch: [62][110/391]\tLoss 0.1674 (0.2304)\tPrec@1 94.531 (91.857)\n",
            "Epoch: [62][165/391]\tLoss 0.3309 (0.2319)\tPrec@1 87.500 (91.844)\n",
            "Epoch: [62][220/391]\tLoss 0.3473 (0.2356)\tPrec@1 89.844 (91.671)\n",
            "Epoch: [62][275/391]\tLoss 0.2533 (0.2366)\tPrec@1 91.406 (91.619)\n",
            "Epoch: [62][330/391]\tLoss 0.2516 (0.2396)\tPrec@1 92.188 (91.520)\n",
            "Epoch: [62][385/391]\tLoss 0.2031 (0.2438)\tPrec@1 92.969 (91.327)\n",
            "Test\t  Prec@1: 84.650 (Err: 15.350 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [63][0/391]\tLoss 0.2542 (0.2542)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [63][55/391]\tLoss 0.2046 (0.2330)\tPrec@1 94.531 (91.825)\n",
            "Epoch: [63][110/391]\tLoss 0.2479 (0.2246)\tPrec@1 91.406 (92.117)\n",
            "Epoch: [63][165/391]\tLoss 0.2500 (0.2264)\tPrec@1 92.188 (92.140)\n",
            "Epoch: [63][220/391]\tLoss 0.2574 (0.2326)\tPrec@1 89.844 (91.887)\n",
            "Epoch: [63][275/391]\tLoss 0.3545 (0.2366)\tPrec@1 89.844 (91.822)\n",
            "Epoch: [63][330/391]\tLoss 0.2506 (0.2387)\tPrec@1 89.062 (91.791)\n",
            "Epoch: [63][385/391]\tLoss 0.2275 (0.2401)\tPrec@1 90.625 (91.667)\n",
            "Test\t  Prec@1: 86.120 (Err: 13.880 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [64][0/391]\tLoss 0.1759 (0.1759)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [64][55/391]\tLoss 0.2496 (0.2273)\tPrec@1 90.625 (91.657)\n",
            "Epoch: [64][110/391]\tLoss 0.1939 (0.2301)\tPrec@1 95.312 (91.702)\n",
            "Epoch: [64][165/391]\tLoss 0.2233 (0.2360)\tPrec@1 90.625 (91.660)\n",
            "Epoch: [64][220/391]\tLoss 0.3001 (0.2391)\tPrec@1 89.844 (91.534)\n",
            "Epoch: [64][275/391]\tLoss 0.2507 (0.2395)\tPrec@1 92.969 (91.582)\n",
            "Epoch: [64][330/391]\tLoss 0.2935 (0.2413)\tPrec@1 90.625 (91.527)\n",
            "Epoch: [64][385/391]\tLoss 0.3060 (0.2427)\tPrec@1 91.406 (91.497)\n",
            "Test\t  Prec@1: 85.540 (Err: 14.460 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [65][0/391]\tLoss 0.1695 (0.1695)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [65][55/391]\tLoss 0.3143 (0.2184)\tPrec@1 90.625 (92.215)\n",
            "Epoch: [65][110/391]\tLoss 0.1317 (0.2186)\tPrec@1 96.094 (92.293)\n",
            "Epoch: [65][165/391]\tLoss 0.2030 (0.2251)\tPrec@1 94.531 (91.957)\n",
            "Epoch: [65][220/391]\tLoss 0.2435 (0.2277)\tPrec@1 89.844 (91.926)\n",
            "Epoch: [65][275/391]\tLoss 0.2238 (0.2337)\tPrec@1 89.844 (91.678)\n",
            "Epoch: [65][330/391]\tLoss 0.2113 (0.2363)\tPrec@1 92.969 (91.652)\n",
            "Epoch: [65][385/391]\tLoss 0.3240 (0.2391)\tPrec@1 88.281 (91.611)\n",
            "Test\t  Prec@1: 85.490 (Err: 14.510 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [66][0/391]\tLoss 0.2272 (0.2272)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [66][55/391]\tLoss 0.2264 (0.2247)\tPrec@1 91.406 (92.048)\n",
            "Epoch: [66][110/391]\tLoss 0.2469 (0.2312)\tPrec@1 91.406 (91.899)\n",
            "Epoch: [66][165/391]\tLoss 0.3427 (0.2374)\tPrec@1 86.719 (91.698)\n",
            "Epoch: [66][220/391]\tLoss 0.1614 (0.2400)\tPrec@1 96.094 (91.703)\n",
            "Epoch: [66][275/391]\tLoss 0.2091 (0.2415)\tPrec@1 92.188 (91.658)\n",
            "Epoch: [66][330/391]\tLoss 0.3170 (0.2443)\tPrec@1 91.406 (91.567)\n",
            "Epoch: [66][385/391]\tLoss 0.2037 (0.2446)\tPrec@1 90.625 (91.514)\n",
            "Test\t  Prec@1: 86.230 (Err: 13.770 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [67][0/391]\tLoss 0.1981 (0.1981)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [67][55/391]\tLoss 0.1548 (0.2091)\tPrec@1 96.094 (92.564)\n",
            "Epoch: [67][110/391]\tLoss 0.2605 (0.2282)\tPrec@1 90.625 (91.723)\n",
            "Epoch: [67][165/391]\tLoss 0.1876 (0.2295)\tPrec@1 93.750 (91.722)\n",
            "Epoch: [67][220/391]\tLoss 0.2134 (0.2326)\tPrec@1 92.969 (91.735)\n",
            "Epoch: [67][275/391]\tLoss 0.2260 (0.2358)\tPrec@1 92.188 (91.613)\n",
            "Epoch: [67][330/391]\tLoss 0.3475 (0.2392)\tPrec@1 87.500 (91.472)\n",
            "Epoch: [67][385/391]\tLoss 0.2691 (0.2423)\tPrec@1 89.844 (91.384)\n",
            "Test\t  Prec@1: 83.890 (Err: 16.110 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [68][0/391]\tLoss 0.3621 (0.3621)\tPrec@1 85.938 (85.938)\n",
            "Epoch: [68][55/391]\tLoss 0.2709 (0.2321)\tPrec@1 89.844 (91.755)\n",
            "Epoch: [68][110/391]\tLoss 0.2930 (0.2327)\tPrec@1 90.625 (91.892)\n",
            "Epoch: [68][165/391]\tLoss 0.2235 (0.2352)\tPrec@1 88.281 (91.689)\n",
            "Epoch: [68][220/391]\tLoss 0.2613 (0.2332)\tPrec@1 92.188 (91.728)\n",
            "Epoch: [68][275/391]\tLoss 0.3489 (0.2360)\tPrec@1 89.844 (91.681)\n",
            "Epoch: [68][330/391]\tLoss 0.2039 (0.2355)\tPrec@1 92.969 (91.682)\n",
            "Epoch: [68][385/391]\tLoss 0.1690 (0.2368)\tPrec@1 93.750 (91.649)\n",
            "Test\t  Prec@1: 85.980 (Err: 14.020 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [69][0/391]\tLoss 0.1894 (0.1894)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [69][55/391]\tLoss 0.1910 (0.2242)\tPrec@1 93.750 (92.104)\n",
            "Epoch: [69][110/391]\tLoss 0.1783 (0.2279)\tPrec@1 93.750 (92.096)\n",
            "Epoch: [69][165/391]\tLoss 0.1691 (0.2301)\tPrec@1 96.094 (91.985)\n",
            "Epoch: [69][220/391]\tLoss 0.2276 (0.2320)\tPrec@1 90.625 (91.862)\n",
            "Epoch: [69][275/391]\tLoss 0.3005 (0.2381)\tPrec@1 89.844 (91.715)\n",
            "Epoch: [69][330/391]\tLoss 0.2704 (0.2401)\tPrec@1 90.625 (91.605)\n",
            "Epoch: [69][385/391]\tLoss 0.2222 (0.2418)\tPrec@1 92.188 (91.479)\n",
            "Test\t  Prec@1: 86.150 (Err: 13.850 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [70][0/391]\tLoss 0.1590 (0.1590)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [70][55/391]\tLoss 0.2750 (0.2257)\tPrec@1 86.719 (91.671)\n",
            "Epoch: [70][110/391]\tLoss 0.1990 (0.2307)\tPrec@1 90.625 (91.843)\n",
            "Epoch: [70][165/391]\tLoss 0.1884 (0.2324)\tPrec@1 92.969 (91.797)\n",
            "Epoch: [70][220/391]\tLoss 0.2265 (0.2363)\tPrec@1 89.844 (91.671)\n",
            "Epoch: [70][275/391]\tLoss 0.2198 (0.2357)\tPrec@1 88.281 (91.706)\n",
            "Epoch: [70][330/391]\tLoss 0.2556 (0.2389)\tPrec@1 92.969 (91.605)\n",
            "Epoch: [70][385/391]\tLoss 0.2014 (0.2380)\tPrec@1 93.750 (91.651)\n",
            "Test\t  Prec@1: 86.000 (Err: 14.000 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [71][0/391]\tLoss 0.2124 (0.2124)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [71][55/391]\tLoss 0.2195 (0.2244)\tPrec@1 89.062 (92.146)\n",
            "Epoch: [71][110/391]\tLoss 0.3059 (0.2304)\tPrec@1 88.281 (91.955)\n",
            "Epoch: [71][165/391]\tLoss 0.3208 (0.2315)\tPrec@1 91.406 (91.943)\n",
            "Epoch: [71][220/391]\tLoss 0.2321 (0.2310)\tPrec@1 89.844 (91.901)\n",
            "Epoch: [71][275/391]\tLoss 0.3934 (0.2343)\tPrec@1 89.062 (91.808)\n",
            "Epoch: [71][330/391]\tLoss 0.1938 (0.2359)\tPrec@1 91.406 (91.737)\n",
            "Epoch: [71][385/391]\tLoss 0.2105 (0.2352)\tPrec@1 95.312 (91.775)\n",
            "Test\t  Prec@1: 85.810 (Err: 14.190 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [72][0/391]\tLoss 0.2428 (0.2428)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [72][55/391]\tLoss 0.2973 (0.2424)\tPrec@1 90.625 (91.462)\n",
            "Epoch: [72][110/391]\tLoss 0.1722 (0.2267)\tPrec@1 92.969 (91.906)\n",
            "Epoch: [72][165/391]\tLoss 0.2689 (0.2190)\tPrec@1 91.406 (92.188)\n",
            "Epoch: [72][220/391]\tLoss 0.2242 (0.2249)\tPrec@1 94.531 (91.965)\n",
            "Epoch: [72][275/391]\tLoss 0.1802 (0.2290)\tPrec@1 94.531 (91.856)\n",
            "Epoch: [72][330/391]\tLoss 0.2077 (0.2331)\tPrec@1 92.188 (91.704)\n",
            "Epoch: [72][385/391]\tLoss 0.3546 (0.2371)\tPrec@1 87.500 (91.627)\n",
            "Test\t  Prec@1: 86.690 (Err: 13.310 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [73][0/391]\tLoss 0.1943 (0.1943)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [73][55/391]\tLoss 0.2157 (0.2097)\tPrec@1 93.750 (92.787)\n",
            "Epoch: [73][110/391]\tLoss 0.2886 (0.2202)\tPrec@1 92.188 (92.286)\n",
            "Epoch: [73][165/391]\tLoss 0.1905 (0.2252)\tPrec@1 90.625 (92.070)\n",
            "Epoch: [73][220/391]\tLoss 0.1578 (0.2278)\tPrec@1 96.094 (91.947)\n",
            "Epoch: [73][275/391]\tLoss 0.2055 (0.2307)\tPrec@1 94.531 (91.913)\n",
            "Epoch: [73][330/391]\tLoss 0.2025 (0.2346)\tPrec@1 92.969 (91.815)\n",
            "Epoch: [73][385/391]\tLoss 0.1645 (0.2364)\tPrec@1 92.969 (91.769)\n",
            "Test\t  Prec@1: 85.290 (Err: 14.710 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [74][0/391]\tLoss 0.2331 (0.2331)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [74][55/391]\tLoss 0.2126 (0.2331)\tPrec@1 92.969 (91.881)\n",
            "Epoch: [74][110/391]\tLoss 0.2716 (0.2335)\tPrec@1 91.406 (91.885)\n",
            "Epoch: [74][165/391]\tLoss 0.2961 (0.2311)\tPrec@1 89.062 (91.933)\n",
            "Epoch: [74][220/391]\tLoss 0.2703 (0.2309)\tPrec@1 92.188 (91.866)\n",
            "Epoch: [74][275/391]\tLoss 0.2997 (0.2322)\tPrec@1 89.062 (91.831)\n",
            "Epoch: [74][330/391]\tLoss 0.2597 (0.2344)\tPrec@1 89.062 (91.708)\n",
            "Epoch: [74][385/391]\tLoss 0.2878 (0.2353)\tPrec@1 85.938 (91.661)\n",
            "Test\t  Prec@1: 87.200 (Err: 12.800 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [75][0/391]\tLoss 0.2236 (0.2236)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [75][55/391]\tLoss 0.2843 (0.2169)\tPrec@1 89.844 (92.104)\n",
            "Epoch: [75][110/391]\tLoss 0.2867 (0.2203)\tPrec@1 88.281 (92.033)\n",
            "Epoch: [75][165/391]\tLoss 0.1864 (0.2246)\tPrec@1 95.312 (91.952)\n",
            "Epoch: [75][220/391]\tLoss 0.1673 (0.2275)\tPrec@1 94.531 (91.781)\n",
            "Epoch: [75][275/391]\tLoss 0.2733 (0.2298)\tPrec@1 88.281 (91.794)\n",
            "Epoch: [75][330/391]\tLoss 0.2386 (0.2337)\tPrec@1 94.531 (91.704)\n",
            "Epoch: [75][385/391]\tLoss 0.3160 (0.2333)\tPrec@1 91.406 (91.726)\n",
            "Test\t  Prec@1: 84.740 (Err: 15.260 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [76][0/391]\tLoss 0.2093 (0.2093)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [76][55/391]\tLoss 0.1936 (0.2127)\tPrec@1 92.188 (92.718)\n",
            "Epoch: [76][110/391]\tLoss 0.2242 (0.2224)\tPrec@1 90.625 (92.335)\n",
            "Epoch: [76][165/391]\tLoss 0.1936 (0.2260)\tPrec@1 92.969 (92.112)\n",
            "Epoch: [76][220/391]\tLoss 0.3033 (0.2312)\tPrec@1 88.281 (91.876)\n",
            "Epoch: [76][275/391]\tLoss 0.1417 (0.2331)\tPrec@1 93.750 (91.797)\n",
            "Epoch: [76][330/391]\tLoss 0.2484 (0.2333)\tPrec@1 89.062 (91.746)\n",
            "Epoch: [76][385/391]\tLoss 0.2391 (0.2334)\tPrec@1 91.406 (91.760)\n",
            "Test\t  Prec@1: 83.470 (Err: 16.530 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [77][0/391]\tLoss 0.2440 (0.2440)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [77][55/391]\tLoss 0.1738 (0.2001)\tPrec@1 92.188 (92.480)\n",
            "Epoch: [77][110/391]\tLoss 0.1951 (0.2100)\tPrec@1 95.312 (92.272)\n",
            "Epoch: [77][165/391]\tLoss 0.1963 (0.2214)\tPrec@1 92.969 (91.995)\n",
            "Epoch: [77][220/391]\tLoss 0.3296 (0.2229)\tPrec@1 89.062 (92.000)\n",
            "Epoch: [77][275/391]\tLoss 0.1936 (0.2253)\tPrec@1 94.531 (91.998)\n",
            "Epoch: [77][330/391]\tLoss 0.2007 (0.2278)\tPrec@1 93.750 (91.935)\n",
            "Epoch: [77][385/391]\tLoss 0.1162 (0.2311)\tPrec@1 97.656 (91.827)\n",
            "Test\t  Prec@1: 85.800 (Err: 14.200 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [78][0/391]\tLoss 0.1869 (0.1869)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [78][55/391]\tLoss 0.2625 (0.2156)\tPrec@1 89.062 (92.369)\n",
            "Epoch: [78][110/391]\tLoss 0.2052 (0.2147)\tPrec@1 92.188 (92.413)\n",
            "Epoch: [78][165/391]\tLoss 0.1669 (0.2227)\tPrec@1 95.312 (92.188)\n",
            "Epoch: [78][220/391]\tLoss 0.2920 (0.2272)\tPrec@1 93.750 (92.035)\n",
            "Epoch: [78][275/391]\tLoss 0.1380 (0.2281)\tPrec@1 93.750 (92.026)\n",
            "Epoch: [78][330/391]\tLoss 0.1153 (0.2275)\tPrec@1 95.312 (91.961)\n",
            "Epoch: [78][385/391]\tLoss 0.2921 (0.2265)\tPrec@1 89.844 (91.979)\n",
            "Test\t  Prec@1: 86.950 (Err: 13.050 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [79][0/391]\tLoss 0.1522 (0.1522)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [79][55/391]\tLoss 0.1863 (0.2073)\tPrec@1 92.969 (92.732)\n",
            "Epoch: [79][110/391]\tLoss 0.3547 (0.2208)\tPrec@1 89.062 (92.427)\n",
            "Epoch: [79][165/391]\tLoss 0.1892 (0.2247)\tPrec@1 93.750 (92.145)\n",
            "Epoch: [79][220/391]\tLoss 0.1976 (0.2279)\tPrec@1 92.188 (91.951)\n",
            "Epoch: [79][275/391]\tLoss 0.2299 (0.2284)\tPrec@1 90.625 (91.882)\n",
            "Epoch: [79][330/391]\tLoss 0.2213 (0.2300)\tPrec@1 93.750 (91.890)\n",
            "Epoch: [79][385/391]\tLoss 0.1754 (0.2304)\tPrec@1 93.750 (91.872)\n",
            "Test\t  Prec@1: 85.560 (Err: 14.440 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [80][0/391]\tLoss 0.2340 (0.2340)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [80][55/391]\tLoss 0.2182 (0.2200)\tPrec@1 92.969 (92.425)\n",
            "Epoch: [80][110/391]\tLoss 0.1980 (0.2160)\tPrec@1 93.750 (92.575)\n",
            "Epoch: [80][165/391]\tLoss 0.1857 (0.2202)\tPrec@1 93.750 (92.324)\n",
            "Epoch: [80][220/391]\tLoss 0.2099 (0.2222)\tPrec@1 92.188 (92.286)\n",
            "Epoch: [80][275/391]\tLoss 0.1867 (0.2254)\tPrec@1 93.750 (92.196)\n",
            "Epoch: [80][330/391]\tLoss 0.3178 (0.2278)\tPrec@1 94.531 (92.117)\n",
            "Epoch: [80][385/391]\tLoss 0.1481 (0.2307)\tPrec@1 94.531 (92.074)\n",
            "Test\t  Prec@1: 85.340 (Err: 14.660 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [81][0/391]\tLoss 0.3305 (0.3305)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [81][55/391]\tLoss 0.1527 (0.2075)\tPrec@1 92.969 (92.899)\n",
            "Epoch: [81][110/391]\tLoss 0.2197 (0.2127)\tPrec@1 89.844 (92.652)\n",
            "Epoch: [81][165/391]\tLoss 0.1952 (0.2153)\tPrec@1 93.750 (92.423)\n",
            "Epoch: [81][220/391]\tLoss 0.2596 (0.2202)\tPrec@1 92.188 (92.290)\n",
            "Epoch: [81][275/391]\tLoss 0.2782 (0.2213)\tPrec@1 91.406 (92.185)\n",
            "Epoch: [81][330/391]\tLoss 0.3009 (0.2254)\tPrec@1 91.406 (92.110)\n",
            "Epoch: [81][385/391]\tLoss 0.2083 (0.2270)\tPrec@1 92.969 (92.068)\n",
            "Test\t  Prec@1: 85.190 (Err: 14.810 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [82][0/391]\tLoss 0.2744 (0.2744)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [82][55/391]\tLoss 0.1997 (0.2132)\tPrec@1 92.969 (92.494)\n",
            "Epoch: [82][110/391]\tLoss 0.1835 (0.2156)\tPrec@1 92.969 (92.314)\n",
            "Epoch: [82][165/391]\tLoss 0.2771 (0.2220)\tPrec@1 89.062 (92.258)\n",
            "Epoch: [82][220/391]\tLoss 0.2554 (0.2238)\tPrec@1 89.844 (92.212)\n",
            "Epoch: [82][275/391]\tLoss 0.1992 (0.2256)\tPrec@1 91.406 (92.154)\n",
            "Epoch: [82][330/391]\tLoss 0.2697 (0.2268)\tPrec@1 89.844 (92.114)\n",
            "Epoch: [82][385/391]\tLoss 0.2213 (0.2282)\tPrec@1 94.531 (92.003)\n",
            "Test\t  Prec@1: 83.540 (Err: 16.460 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [83][0/391]\tLoss 0.1925 (0.1925)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [83][55/391]\tLoss 0.2195 (0.2003)\tPrec@1 92.188 (93.039)\n",
            "Epoch: [83][110/391]\tLoss 0.2043 (0.2020)\tPrec@1 92.188 (92.694)\n",
            "Epoch: [83][165/391]\tLoss 0.2152 (0.2152)\tPrec@1 92.969 (92.249)\n",
            "Epoch: [83][220/391]\tLoss 0.3186 (0.2207)\tPrec@1 90.625 (92.103)\n",
            "Epoch: [83][275/391]\tLoss 0.3511 (0.2235)\tPrec@1 88.281 (92.032)\n",
            "Epoch: [83][330/391]\tLoss 0.2005 (0.2271)\tPrec@1 92.188 (91.959)\n",
            "Epoch: [83][385/391]\tLoss 0.3368 (0.2280)\tPrec@1 87.500 (91.943)\n",
            "Test\t  Prec@1: 86.500 (Err: 13.500 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [84][0/391]\tLoss 0.2138 (0.2138)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [84][55/391]\tLoss 0.2545 (0.2093)\tPrec@1 89.062 (92.634)\n",
            "Epoch: [84][110/391]\tLoss 0.2324 (0.2141)\tPrec@1 92.969 (92.546)\n",
            "Epoch: [84][165/391]\tLoss 0.2319 (0.2253)\tPrec@1 92.188 (92.070)\n",
            "Epoch: [84][220/391]\tLoss 0.1526 (0.2283)\tPrec@1 95.312 (91.887)\n",
            "Epoch: [84][275/391]\tLoss 0.2139 (0.2272)\tPrec@1 92.188 (91.958)\n",
            "Epoch: [84][330/391]\tLoss 0.2114 (0.2256)\tPrec@1 92.969 (92.072)\n",
            "Epoch: [84][385/391]\tLoss 0.1484 (0.2286)\tPrec@1 94.531 (91.943)\n",
            "Test\t  Prec@1: 84.790 (Err: 15.210 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [85][0/391]\tLoss 0.2317 (0.2317)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [85][55/391]\tLoss 0.1912 (0.2026)\tPrec@1 94.531 (92.732)\n",
            "Epoch: [85][110/391]\tLoss 0.2889 (0.2074)\tPrec@1 89.844 (92.807)\n",
            "Epoch: [85][165/391]\tLoss 0.2498 (0.2173)\tPrec@1 91.406 (92.484)\n",
            "Epoch: [85][220/391]\tLoss 0.2194 (0.2238)\tPrec@1 91.406 (92.241)\n",
            "Epoch: [85][275/391]\tLoss 0.1081 (0.2286)\tPrec@1 96.875 (92.156)\n",
            "Epoch: [85][330/391]\tLoss 0.2234 (0.2294)\tPrec@1 93.750 (92.121)\n",
            "Epoch: [85][385/391]\tLoss 0.1769 (0.2300)\tPrec@1 93.750 (92.036)\n",
            "Test\t  Prec@1: 86.240 (Err: 13.760 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [86][0/391]\tLoss 0.1594 (0.1594)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [86][55/391]\tLoss 0.1879 (0.1987)\tPrec@1 89.844 (92.955)\n",
            "Epoch: [86][110/391]\tLoss 0.2431 (0.2045)\tPrec@1 92.188 (92.772)\n",
            "Epoch: [86][165/391]\tLoss 0.3258 (0.2095)\tPrec@1 89.844 (92.705)\n",
            "Epoch: [86][220/391]\tLoss 0.1744 (0.2137)\tPrec@1 93.750 (92.520)\n",
            "Epoch: [86][275/391]\tLoss 0.1982 (0.2166)\tPrec@1 92.188 (92.434)\n",
            "Epoch: [86][330/391]\tLoss 0.3745 (0.2199)\tPrec@1 87.500 (92.301)\n",
            "Epoch: [86][385/391]\tLoss 0.1889 (0.2226)\tPrec@1 93.750 (92.212)\n",
            "Test\t  Prec@1: 86.860 (Err: 13.140 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [87][0/391]\tLoss 0.1794 (0.1794)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [87][55/391]\tLoss 0.1595 (0.2109)\tPrec@1 94.531 (92.634)\n",
            "Epoch: [87][110/391]\tLoss 0.2586 (0.2120)\tPrec@1 89.844 (92.392)\n",
            "Epoch: [87][165/391]\tLoss 0.2196 (0.2169)\tPrec@1 90.625 (92.390)\n",
            "Epoch: [87][220/391]\tLoss 0.1801 (0.2208)\tPrec@1 92.188 (92.233)\n",
            "Epoch: [87][275/391]\tLoss 0.2837 (0.2225)\tPrec@1 88.281 (92.227)\n",
            "Epoch: [87][330/391]\tLoss 0.1847 (0.2213)\tPrec@1 92.969 (92.244)\n",
            "Epoch: [87][385/391]\tLoss 0.2498 (0.2236)\tPrec@1 92.969 (92.240)\n",
            "Test\t  Prec@1: 86.130 (Err: 13.870 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [88][0/391]\tLoss 0.2562 (0.2562)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [88][55/391]\tLoss 0.2373 (0.2026)\tPrec@1 93.750 (93.052)\n",
            "Epoch: [88][110/391]\tLoss 0.2013 (0.2156)\tPrec@1 93.750 (92.469)\n",
            "Epoch: [88][165/391]\tLoss 0.2757 (0.2226)\tPrec@1 91.406 (92.178)\n",
            "Epoch: [88][220/391]\tLoss 0.2668 (0.2230)\tPrec@1 91.406 (92.149)\n",
            "Epoch: [88][275/391]\tLoss 0.2553 (0.2266)\tPrec@1 90.625 (92.074)\n",
            "Epoch: [88][330/391]\tLoss 0.1279 (0.2266)\tPrec@1 95.312 (92.128)\n",
            "Epoch: [88][385/391]\tLoss 0.1859 (0.2264)\tPrec@1 92.188 (92.111)\n",
            "Test\t  Prec@1: 86.350 (Err: 13.650 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [89][0/391]\tLoss 0.2089 (0.2089)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [89][55/391]\tLoss 0.1960 (0.2016)\tPrec@1 92.188 (93.150)\n",
            "Epoch: [89][110/391]\tLoss 0.2461 (0.2034)\tPrec@1 92.188 (93.025)\n",
            "Epoch: [89][165/391]\tLoss 0.2772 (0.2087)\tPrec@1 87.500 (92.729)\n",
            "Epoch: [89][220/391]\tLoss 0.0965 (0.2151)\tPrec@1 96.094 (92.492)\n",
            "Epoch: [89][275/391]\tLoss 0.2992 (0.2143)\tPrec@1 86.719 (92.451)\n",
            "Epoch: [89][330/391]\tLoss 0.1951 (0.2162)\tPrec@1 91.406 (92.421)\n",
            "Epoch: [89][385/391]\tLoss 0.1728 (0.2200)\tPrec@1 95.312 (92.321)\n",
            "Test\t  Prec@1: 86.310 (Err: 13.690 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [90][0/391]\tLoss 0.2929 (0.2929)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [90][55/391]\tLoss 0.2676 (0.2153)\tPrec@1 89.844 (92.578)\n",
            "Epoch: [90][110/391]\tLoss 0.1670 (0.2213)\tPrec@1 93.750 (92.300)\n",
            "Epoch: [90][165/391]\tLoss 0.1626 (0.2149)\tPrec@1 93.750 (92.512)\n",
            "Epoch: [90][220/391]\tLoss 0.1104 (0.2153)\tPrec@1 97.656 (92.460)\n",
            "Epoch: [90][275/391]\tLoss 0.2690 (0.2194)\tPrec@1 89.844 (92.357)\n",
            "Epoch: [90][330/391]\tLoss 0.2748 (0.2226)\tPrec@1 89.844 (92.239)\n",
            "Epoch: [90][385/391]\tLoss 0.1688 (0.2233)\tPrec@1 92.188 (92.179)\n",
            "Test\t  Prec@1: 84.490 (Err: 15.510 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [91][0/391]\tLoss 0.2787 (0.2787)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [91][55/391]\tLoss 0.2471 (0.2221)\tPrec@1 90.625 (92.229)\n",
            "Epoch: [91][110/391]\tLoss 0.2029 (0.2189)\tPrec@1 92.188 (92.349)\n",
            "Epoch: [91][165/391]\tLoss 0.3124 (0.2203)\tPrec@1 89.062 (92.300)\n",
            "Epoch: [91][220/391]\tLoss 0.2665 (0.2221)\tPrec@1 92.969 (92.237)\n",
            "Epoch: [91][275/391]\tLoss 0.4262 (0.2206)\tPrec@1 85.938 (92.281)\n",
            "Epoch: [91][330/391]\tLoss 0.2575 (0.2243)\tPrec@1 89.844 (92.173)\n",
            "Epoch: [91][385/391]\tLoss 0.1893 (0.2231)\tPrec@1 91.406 (92.218)\n",
            "Test\t  Prec@1: 87.170 (Err: 12.830 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [92][0/391]\tLoss 0.2462 (0.2462)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [92][55/391]\tLoss 0.1556 (0.2034)\tPrec@1 95.312 (92.843)\n",
            "Epoch: [92][110/391]\tLoss 0.2379 (0.2012)\tPrec@1 90.625 (92.927)\n",
            "Epoch: [92][165/391]\tLoss 0.1477 (0.2054)\tPrec@1 95.312 (92.602)\n",
            "Epoch: [92][220/391]\tLoss 0.2480 (0.2098)\tPrec@1 91.406 (92.463)\n",
            "Epoch: [92][275/391]\tLoss 0.1865 (0.2161)\tPrec@1 92.188 (92.250)\n",
            "Epoch: [92][330/391]\tLoss 0.2216 (0.2167)\tPrec@1 89.844 (92.256)\n",
            "Epoch: [92][385/391]\tLoss 0.2575 (0.2179)\tPrec@1 92.188 (92.252)\n",
            "Test\t  Prec@1: 86.110 (Err: 13.890 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [93][0/391]\tLoss 0.3550 (0.3550)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [93][55/391]\tLoss 0.1976 (0.2245)\tPrec@1 92.969 (91.978)\n",
            "Epoch: [93][110/391]\tLoss 0.2386 (0.2183)\tPrec@1 91.406 (91.990)\n",
            "Epoch: [93][165/391]\tLoss 0.2074 (0.2218)\tPrec@1 92.188 (91.900)\n",
            "Epoch: [93][220/391]\tLoss 0.2319 (0.2215)\tPrec@1 93.750 (92.089)\n",
            "Epoch: [93][275/391]\tLoss 0.1435 (0.2202)\tPrec@1 94.531 (92.156)\n",
            "Epoch: [93][330/391]\tLoss 0.1738 (0.2237)\tPrec@1 92.188 (92.027)\n",
            "Epoch: [93][385/391]\tLoss 0.2159 (0.2260)\tPrec@1 91.406 (91.951)\n",
            "Test\t  Prec@1: 85.720 (Err: 14.280 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [94][0/391]\tLoss 0.1888 (0.1888)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [94][55/391]\tLoss 0.2057 (0.2137)\tPrec@1 94.531 (92.467)\n",
            "Epoch: [94][110/391]\tLoss 0.2376 (0.2146)\tPrec@1 92.188 (92.399)\n",
            "Epoch: [94][165/391]\tLoss 0.2773 (0.2170)\tPrec@1 92.188 (92.319)\n",
            "Epoch: [94][220/391]\tLoss 0.1506 (0.2174)\tPrec@1 93.750 (92.385)\n",
            "Epoch: [94][275/391]\tLoss 0.2091 (0.2164)\tPrec@1 92.188 (92.448)\n",
            "Epoch: [94][330/391]\tLoss 0.1976 (0.2197)\tPrec@1 94.531 (92.353)\n",
            "Epoch: [94][385/391]\tLoss 0.1750 (0.2204)\tPrec@1 92.969 (92.358)\n",
            "Test\t  Prec@1: 85.940 (Err: 14.060 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [95][0/391]\tLoss 0.1831 (0.1831)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [95][55/391]\tLoss 0.2699 (0.2103)\tPrec@1 89.844 (92.313)\n",
            "Epoch: [95][110/391]\tLoss 0.1205 (0.2073)\tPrec@1 95.312 (92.610)\n",
            "Epoch: [95][165/391]\tLoss 0.2016 (0.2146)\tPrec@1 92.188 (92.230)\n",
            "Epoch: [95][220/391]\tLoss 0.2854 (0.2173)\tPrec@1 90.625 (92.163)\n",
            "Epoch: [95][275/391]\tLoss 0.1585 (0.2210)\tPrec@1 94.531 (92.094)\n",
            "Epoch: [95][330/391]\tLoss 0.1825 (0.2225)\tPrec@1 90.625 (92.086)\n",
            "Epoch: [95][385/391]\tLoss 0.1981 (0.2223)\tPrec@1 93.750 (92.098)\n",
            "Test\t  Prec@1: 86.480 (Err: 13.520 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [96][0/391]\tLoss 0.2598 (0.2598)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [96][55/391]\tLoss 0.1514 (0.2052)\tPrec@1 94.531 (92.899)\n",
            "Epoch: [96][110/391]\tLoss 0.2131 (0.2055)\tPrec@1 90.625 (92.793)\n",
            "Epoch: [96][165/391]\tLoss 0.2510 (0.2091)\tPrec@1 91.406 (92.686)\n",
            "Epoch: [96][220/391]\tLoss 0.2325 (0.2088)\tPrec@1 90.625 (92.700)\n",
            "Epoch: [96][275/391]\tLoss 0.2627 (0.2108)\tPrec@1 91.406 (92.572)\n",
            "Epoch: [96][330/391]\tLoss 0.2009 (0.2149)\tPrec@1 92.969 (92.350)\n",
            "Epoch: [96][385/391]\tLoss 0.2798 (0.2188)\tPrec@1 91.406 (92.204)\n",
            "Test\t  Prec@1: 86.620 (Err: 13.380 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [97][0/391]\tLoss 0.2977 (0.2977)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [97][55/391]\tLoss 0.1611 (0.2038)\tPrec@1 95.312 (92.871)\n",
            "Epoch: [97][110/391]\tLoss 0.1689 (0.2049)\tPrec@1 94.531 (92.849)\n",
            "Epoch: [97][165/391]\tLoss 0.2311 (0.2061)\tPrec@1 90.625 (92.757)\n",
            "Epoch: [97][220/391]\tLoss 0.1883 (0.2130)\tPrec@1 92.969 (92.513)\n",
            "Epoch: [97][275/391]\tLoss 0.2348 (0.2172)\tPrec@1 92.188 (92.349)\n",
            "Epoch: [97][330/391]\tLoss 0.1965 (0.2165)\tPrec@1 93.750 (92.402)\n",
            "Epoch: [97][385/391]\tLoss 0.1841 (0.2193)\tPrec@1 91.406 (92.307)\n",
            "Test\t  Prec@1: 86.820 (Err: 13.180 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [98][0/391]\tLoss 0.1187 (0.1187)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [98][55/391]\tLoss 0.1961 (0.2097)\tPrec@1 92.188 (92.536)\n",
            "Epoch: [98][110/391]\tLoss 0.1996 (0.2077)\tPrec@1 91.406 (92.751)\n",
            "Epoch: [98][165/391]\tLoss 0.1577 (0.2032)\tPrec@1 93.750 (92.945)\n",
            "Epoch: [98][220/391]\tLoss 0.1429 (0.2038)\tPrec@1 94.531 (92.834)\n",
            "Epoch: [98][275/391]\tLoss 0.1557 (0.2050)\tPrec@1 95.312 (92.773)\n",
            "Epoch: [98][330/391]\tLoss 0.1866 (0.2118)\tPrec@1 92.969 (92.551)\n",
            "Epoch: [98][385/391]\tLoss 0.2371 (0.2132)\tPrec@1 91.406 (92.499)\n",
            "Test\t  Prec@1: 88.360 (Err: 11.640 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [99][0/391]\tLoss 0.1842 (0.1842)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [99][55/391]\tLoss 0.2035 (0.1943)\tPrec@1 93.750 (93.122)\n",
            "Epoch: [99][110/391]\tLoss 0.1347 (0.1971)\tPrec@1 96.094 (93.117)\n",
            "Epoch: [99][165/391]\tLoss 0.1926 (0.2035)\tPrec@1 92.188 (92.748)\n",
            "Epoch: [99][220/391]\tLoss 0.1439 (0.2099)\tPrec@1 94.531 (92.502)\n",
            "Epoch: [99][275/391]\tLoss 0.2671 (0.2176)\tPrec@1 92.188 (92.233)\n",
            "Epoch: [99][330/391]\tLoss 0.3102 (0.2214)\tPrec@1 86.719 (92.126)\n",
            "Epoch: [99][385/391]\tLoss 0.2385 (0.2206)\tPrec@1 92.188 (92.145)\n",
            "Test\t  Prec@1: 87.110 (Err: 12.890 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [100][0/391]\tLoss 0.2615 (0.2615)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [100][55/391]\tLoss 0.1143 (0.1744)\tPrec@1 96.875 (93.792)\n",
            "Epoch: [100][110/391]\tLoss 0.1489 (0.1608)\tPrec@1 92.188 (94.510)\n",
            "Epoch: [100][165/391]\tLoss 0.1349 (0.1500)\tPrec@1 92.969 (94.880)\n",
            "Epoch: [100][220/391]\tLoss 0.1149 (0.1454)\tPrec@1 95.312 (95.037)\n",
            "Epoch: [100][275/391]\tLoss 0.1210 (0.1423)\tPrec@1 94.531 (95.151)\n",
            "Epoch: [100][330/391]\tLoss 0.1259 (0.1400)\tPrec@1 96.094 (95.249)\n",
            "Epoch: [100][385/391]\tLoss 0.1191 (0.1371)\tPrec@1 96.094 (95.377)\n",
            "Test\t  Prec@1: 91.220 (Err: 8.780 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [101][0/391]\tLoss 0.1154 (0.1154)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [101][55/391]\tLoss 0.1200 (0.1084)\tPrec@1 95.312 (96.624)\n",
            "Epoch: [101][110/391]\tLoss 0.1361 (0.1097)\tPrec@1 93.750 (96.418)\n",
            "Epoch: [101][165/391]\tLoss 0.0880 (0.1082)\tPrec@1 96.875 (96.414)\n",
            "Epoch: [101][220/391]\tLoss 0.0826 (0.1078)\tPrec@1 99.219 (96.458)\n",
            "Epoch: [101][275/391]\tLoss 0.1184 (0.1075)\tPrec@1 96.875 (96.479)\n",
            "Epoch: [101][330/391]\tLoss 0.0422 (0.1082)\tPrec@1 98.438 (96.408)\n",
            "Epoch: [101][385/391]\tLoss 0.0856 (0.1077)\tPrec@1 97.656 (96.393)\n",
            "Test\t  Prec@1: 91.500 (Err: 8.500 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [102][0/391]\tLoss 0.0688 (0.0688)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [102][55/391]\tLoss 0.0918 (0.0999)\tPrec@1 96.875 (96.624)\n",
            "Epoch: [102][110/391]\tLoss 0.0864 (0.0992)\tPrec@1 98.438 (96.671)\n",
            "Epoch: [102][165/391]\tLoss 0.0695 (0.0977)\tPrec@1 97.656 (96.724)\n",
            "Epoch: [102][220/391]\tLoss 0.1027 (0.0968)\tPrec@1 96.094 (96.794)\n",
            "Epoch: [102][275/391]\tLoss 0.1232 (0.0973)\tPrec@1 96.875 (96.782)\n",
            "Epoch: [102][330/391]\tLoss 0.0914 (0.0968)\tPrec@1 97.656 (96.809)\n",
            "Epoch: [102][385/391]\tLoss 0.0822 (0.0959)\tPrec@1 98.438 (96.839)\n",
            "Test\t  Prec@1: 91.420 (Err: 8.580 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [103][0/391]\tLoss 0.0521 (0.0521)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [103][55/391]\tLoss 0.0618 (0.0966)\tPrec@1 97.656 (96.763)\n",
            "Epoch: [103][110/391]\tLoss 0.0917 (0.0947)\tPrec@1 96.875 (96.755)\n",
            "Epoch: [103][165/391]\tLoss 0.0924 (0.0923)\tPrec@1 96.094 (96.880)\n",
            "Epoch: [103][220/391]\tLoss 0.0636 (0.0935)\tPrec@1 96.875 (96.818)\n",
            "Epoch: [103][275/391]\tLoss 0.1034 (0.0932)\tPrec@1 97.656 (96.847)\n",
            "Epoch: [103][330/391]\tLoss 0.0753 (0.0924)\tPrec@1 96.875 (96.861)\n",
            "Epoch: [103][385/391]\tLoss 0.1098 (0.0913)\tPrec@1 97.656 (96.930)\n",
            "Test\t  Prec@1: 91.490 (Err: 8.510 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [104][0/391]\tLoss 0.0905 (0.0905)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [104][55/391]\tLoss 0.1621 (0.0875)\tPrec@1 92.188 (96.889)\n",
            "Epoch: [104][110/391]\tLoss 0.0623 (0.0852)\tPrec@1 98.438 (96.917)\n",
            "Epoch: [104][165/391]\tLoss 0.1030 (0.0864)\tPrec@1 96.094 (97.007)\n",
            "Epoch: [104][220/391]\tLoss 0.1156 (0.0849)\tPrec@1 95.312 (97.055)\n",
            "Epoch: [104][275/391]\tLoss 0.0573 (0.0864)\tPrec@1 97.656 (96.983)\n",
            "Epoch: [104][330/391]\tLoss 0.0578 (0.0860)\tPrec@1 98.438 (97.012)\n",
            "Epoch: [104][385/391]\tLoss 0.0856 (0.0857)\tPrec@1 96.875 (97.035)\n",
            "Test\t  Prec@1: 91.740 (Err: 8.260 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [105][0/391]\tLoss 0.2007 (0.2007)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [105][55/391]\tLoss 0.0641 (0.0873)\tPrec@1 96.875 (96.973)\n",
            "Epoch: [105][110/391]\tLoss 0.0985 (0.0830)\tPrec@1 97.656 (97.234)\n",
            "Epoch: [105][165/391]\tLoss 0.1022 (0.0817)\tPrec@1 96.094 (97.308)\n",
            "Epoch: [105][220/391]\tLoss 0.1144 (0.0802)\tPrec@1 97.656 (97.349)\n",
            "Epoch: [105][275/391]\tLoss 0.0406 (0.0812)\tPrec@1 99.219 (97.339)\n",
            "Epoch: [105][330/391]\tLoss 0.0876 (0.0817)\tPrec@1 96.875 (97.274)\n",
            "Epoch: [105][385/391]\tLoss 0.0575 (0.0824)\tPrec@1 99.219 (97.211)\n",
            "Test\t  Prec@1: 91.640 (Err: 8.360 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [106][0/391]\tLoss 0.0695 (0.0695)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [106][55/391]\tLoss 0.0770 (0.0765)\tPrec@1 97.656 (97.475)\n",
            "Epoch: [106][110/391]\tLoss 0.0879 (0.0741)\tPrec@1 97.656 (97.586)\n",
            "Epoch: [106][165/391]\tLoss 0.0862 (0.0749)\tPrec@1 96.875 (97.548)\n",
            "Epoch: [106][220/391]\tLoss 0.1316 (0.0764)\tPrec@1 94.531 (97.451)\n",
            "Epoch: [106][275/391]\tLoss 0.0898 (0.0776)\tPrec@1 97.656 (97.421)\n",
            "Epoch: [106][330/391]\tLoss 0.0865 (0.0771)\tPrec@1 96.875 (97.441)\n",
            "Epoch: [106][385/391]\tLoss 0.0718 (0.0773)\tPrec@1 96.875 (97.440)\n",
            "Test\t  Prec@1: 91.510 (Err: 8.490 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [107][0/391]\tLoss 0.1041 (0.1041)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [107][55/391]\tLoss 0.0901 (0.0746)\tPrec@1 98.438 (97.419)\n",
            "Epoch: [107][110/391]\tLoss 0.0900 (0.0722)\tPrec@1 97.656 (97.607)\n",
            "Epoch: [107][165/391]\tLoss 0.0563 (0.0736)\tPrec@1 97.656 (97.628)\n",
            "Epoch: [107][220/391]\tLoss 0.0707 (0.0731)\tPrec@1 96.875 (97.593)\n",
            "Epoch: [107][275/391]\tLoss 0.0670 (0.0728)\tPrec@1 97.656 (97.597)\n",
            "Epoch: [107][330/391]\tLoss 0.0921 (0.0730)\tPrec@1 96.094 (97.557)\n",
            "Epoch: [107][385/391]\tLoss 0.0388 (0.0725)\tPrec@1 99.219 (97.559)\n",
            "Test\t  Prec@1: 91.290 (Err: 8.710 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [108][0/391]\tLoss 0.0380 (0.0380)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [108][55/391]\tLoss 0.1320 (0.0807)\tPrec@1 96.875 (97.224)\n",
            "Epoch: [108][110/391]\tLoss 0.0537 (0.0736)\tPrec@1 98.438 (97.480)\n",
            "Epoch: [108][165/391]\tLoss 0.0983 (0.0716)\tPrec@1 96.094 (97.604)\n",
            "Epoch: [108][220/391]\tLoss 0.0788 (0.0712)\tPrec@1 97.656 (97.649)\n",
            "Epoch: [108][275/391]\tLoss 0.1035 (0.0701)\tPrec@1 96.875 (97.679)\n",
            "Epoch: [108][330/391]\tLoss 0.0726 (0.0696)\tPrec@1 98.438 (97.713)\n",
            "Epoch: [108][385/391]\tLoss 0.0514 (0.0701)\tPrec@1 97.656 (97.693)\n",
            "Test\t  Prec@1: 91.610 (Err: 8.390 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [109][0/391]\tLoss 0.0832 (0.0832)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [109][55/391]\tLoss 0.0276 (0.0726)\tPrec@1 100.000 (97.377)\n",
            "Epoch: [109][110/391]\tLoss 0.0734 (0.0690)\tPrec@1 96.875 (97.614)\n",
            "Epoch: [109][165/391]\tLoss 0.0581 (0.0692)\tPrec@1 96.875 (97.670)\n",
            "Epoch: [109][220/391]\tLoss 0.0926 (0.0686)\tPrec@1 95.312 (97.685)\n",
            "Epoch: [109][275/391]\tLoss 0.0891 (0.0682)\tPrec@1 95.312 (97.704)\n",
            "Epoch: [109][330/391]\tLoss 0.0380 (0.0675)\tPrec@1 98.438 (97.727)\n",
            "Epoch: [109][385/391]\tLoss 0.0646 (0.0678)\tPrec@1 97.656 (97.703)\n",
            "Test\t  Prec@1: 91.560 (Err: 8.440 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [110][0/391]\tLoss 0.0801 (0.0801)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [110][55/391]\tLoss 0.0679 (0.0649)\tPrec@1 97.656 (97.810)\n",
            "Epoch: [110][110/391]\tLoss 0.0313 (0.0651)\tPrec@1 100.000 (97.783)\n",
            "Epoch: [110][165/391]\tLoss 0.0330 (0.0651)\tPrec@1 98.438 (97.797)\n",
            "Epoch: [110][220/391]\tLoss 0.0469 (0.0658)\tPrec@1 98.438 (97.801)\n",
            "Epoch: [110][275/391]\tLoss 0.0397 (0.0670)\tPrec@1 98.438 (97.699)\n",
            "Epoch: [110][330/391]\tLoss 0.0825 (0.0660)\tPrec@1 97.656 (97.762)\n",
            "Epoch: [110][385/391]\tLoss 0.0988 (0.0657)\tPrec@1 96.875 (97.753)\n",
            "Test\t  Prec@1: 91.550 (Err: 8.450 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [111][0/391]\tLoss 0.1365 (0.1365)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [111][55/391]\tLoss 0.0232 (0.0568)\tPrec@1 100.000 (98.242)\n",
            "Epoch: [111][110/391]\tLoss 0.0424 (0.0599)\tPrec@1 98.438 (98.100)\n",
            "Epoch: [111][165/391]\tLoss 0.0495 (0.0615)\tPrec@1 98.438 (98.075)\n",
            "Epoch: [111][220/391]\tLoss 0.0578 (0.0627)\tPrec@1 97.656 (97.957)\n",
            "Epoch: [111][275/391]\tLoss 0.0855 (0.0631)\tPrec@1 96.094 (97.894)\n",
            "Epoch: [111][330/391]\tLoss 0.1350 (0.0630)\tPrec@1 94.531 (97.869)\n",
            "Epoch: [111][385/391]\tLoss 0.0654 (0.0636)\tPrec@1 97.656 (97.855)\n",
            "Test\t  Prec@1: 91.640 (Err: 8.360 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [112][0/391]\tLoss 0.0708 (0.0708)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [112][55/391]\tLoss 0.0956 (0.0602)\tPrec@1 97.656 (98.214)\n",
            "Epoch: [112][110/391]\tLoss 0.0337 (0.0597)\tPrec@1 99.219 (98.142)\n",
            "Epoch: [112][165/391]\tLoss 0.0348 (0.0599)\tPrec@1 99.219 (98.099)\n",
            "Epoch: [112][220/391]\tLoss 0.0317 (0.0593)\tPrec@1 99.219 (98.088)\n",
            "Epoch: [112][275/391]\tLoss 0.0987 (0.0597)\tPrec@1 96.094 (98.053)\n",
            "Epoch: [112][330/391]\tLoss 0.1166 (0.0616)\tPrec@1 96.094 (97.947)\n",
            "Epoch: [112][385/391]\tLoss 0.0880 (0.0624)\tPrec@1 98.438 (97.921)\n",
            "Test\t  Prec@1: 91.590 (Err: 8.410 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [113][0/391]\tLoss 0.0395 (0.0395)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [113][55/391]\tLoss 0.0313 (0.0604)\tPrec@1 99.219 (97.921)\n",
            "Epoch: [113][110/391]\tLoss 0.0506 (0.0590)\tPrec@1 99.219 (98.057)\n",
            "Epoch: [113][165/391]\tLoss 0.0308 (0.0592)\tPrec@1 99.219 (98.056)\n",
            "Epoch: [113][220/391]\tLoss 0.0651 (0.0596)\tPrec@1 97.656 (98.077)\n",
            "Epoch: [113][275/391]\tLoss 0.0539 (0.0595)\tPrec@1 98.438 (98.061)\n",
            "Epoch: [113][330/391]\tLoss 0.0935 (0.0601)\tPrec@1 96.875 (98.022)\n",
            "Epoch: [113][385/391]\tLoss 0.0712 (0.0604)\tPrec@1 97.656 (97.988)\n",
            "Test\t  Prec@1: 91.700 (Err: 8.300 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [114][0/391]\tLoss 0.0531 (0.0531)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [114][55/391]\tLoss 0.0453 (0.0584)\tPrec@1 98.438 (97.991)\n",
            "Epoch: [114][110/391]\tLoss 0.0291 (0.0557)\tPrec@1 99.219 (98.057)\n",
            "Epoch: [114][165/391]\tLoss 0.1142 (0.0559)\tPrec@1 95.312 (98.141)\n",
            "Epoch: [114][220/391]\tLoss 0.0678 (0.0569)\tPrec@1 97.656 (98.077)\n",
            "Epoch: [114][275/391]\tLoss 0.0716 (0.0580)\tPrec@1 95.312 (98.007)\n",
            "Epoch: [114][330/391]\tLoss 0.0439 (0.0580)\tPrec@1 98.438 (97.998)\n",
            "Epoch: [114][385/391]\tLoss 0.0512 (0.0572)\tPrec@1 99.219 (98.037)\n",
            "Test\t  Prec@1: 91.640 (Err: 8.360 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [115][0/391]\tLoss 0.1038 (0.1038)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [115][55/391]\tLoss 0.0504 (0.0556)\tPrec@1 98.438 (98.214)\n",
            "Epoch: [115][110/391]\tLoss 0.0682 (0.0549)\tPrec@1 96.875 (98.156)\n",
            "Epoch: [115][165/391]\tLoss 0.0561 (0.0561)\tPrec@1 99.219 (98.132)\n",
            "Epoch: [115][220/391]\tLoss 0.0473 (0.0565)\tPrec@1 99.219 (98.123)\n",
            "Epoch: [115][275/391]\tLoss 0.0465 (0.0569)\tPrec@1 97.656 (98.075)\n",
            "Epoch: [115][330/391]\tLoss 0.0232 (0.0566)\tPrec@1 99.219 (98.086)\n",
            "Epoch: [115][385/391]\tLoss 0.0421 (0.0574)\tPrec@1 99.219 (98.063)\n",
            "Test\t  Prec@1: 91.390 (Err: 8.610 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [116][0/391]\tLoss 0.0460 (0.0460)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [116][55/391]\tLoss 0.0342 (0.0499)\tPrec@1 100.000 (98.270)\n",
            "Epoch: [116][110/391]\tLoss 0.0714 (0.0497)\tPrec@1 96.875 (98.402)\n",
            "Epoch: [116][165/391]\tLoss 0.0815 (0.0530)\tPrec@1 97.656 (98.296)\n",
            "Epoch: [116][220/391]\tLoss 0.0375 (0.0524)\tPrec@1 99.219 (98.310)\n",
            "Epoch: [116][275/391]\tLoss 0.0687 (0.0531)\tPrec@1 96.875 (98.242)\n",
            "Epoch: [116][330/391]\tLoss 0.0218 (0.0536)\tPrec@1 100.000 (98.249)\n",
            "Epoch: [116][385/391]\tLoss 0.0583 (0.0533)\tPrec@1 97.656 (98.257)\n",
            "Test\t  Prec@1: 91.710 (Err: 8.290 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [117][0/391]\tLoss 0.0626 (0.0626)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [117][55/391]\tLoss 0.0560 (0.0531)\tPrec@1 99.219 (98.340)\n",
            "Epoch: [117][110/391]\tLoss 0.0480 (0.0538)\tPrec@1 97.656 (98.283)\n",
            "Epoch: [117][165/391]\tLoss 0.0699 (0.0564)\tPrec@1 97.656 (98.174)\n",
            "Epoch: [117][220/391]\tLoss 0.0590 (0.0560)\tPrec@1 96.875 (98.155)\n",
            "Epoch: [117][275/391]\tLoss 0.0488 (0.0546)\tPrec@1 97.656 (98.217)\n",
            "Epoch: [117][330/391]\tLoss 0.0164 (0.0535)\tPrec@1 99.219 (98.275)\n",
            "Epoch: [117][385/391]\tLoss 0.0417 (0.0528)\tPrec@1 99.219 (98.302)\n",
            "Test\t  Prec@1: 91.840 (Err: 8.160 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [118][0/391]\tLoss 0.0625 (0.0625)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [118][55/391]\tLoss 0.0696 (0.0573)\tPrec@1 97.656 (98.131)\n",
            "Epoch: [118][110/391]\tLoss 0.0380 (0.0537)\tPrec@1 98.438 (98.184)\n",
            "Epoch: [118][165/391]\tLoss 0.0565 (0.0556)\tPrec@1 97.656 (98.108)\n",
            "Epoch: [118][220/391]\tLoss 0.0564 (0.0531)\tPrec@1 97.656 (98.254)\n",
            "Epoch: [118][275/391]\tLoss 0.0534 (0.0524)\tPrec@1 97.656 (98.270)\n",
            "Epoch: [118][330/391]\tLoss 0.0535 (0.0523)\tPrec@1 98.438 (98.268)\n",
            "Epoch: [118][385/391]\tLoss 0.0780 (0.0529)\tPrec@1 97.656 (98.233)\n",
            "Test\t  Prec@1: 91.760 (Err: 8.240 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [119][0/391]\tLoss 0.0553 (0.0553)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [119][55/391]\tLoss 0.0413 (0.0517)\tPrec@1 99.219 (98.145)\n",
            "Epoch: [119][110/391]\tLoss 0.0288 (0.0486)\tPrec@1 100.000 (98.311)\n",
            "Epoch: [119][165/391]\tLoss 0.0290 (0.0477)\tPrec@1 99.219 (98.325)\n",
            "Epoch: [119][220/391]\tLoss 0.0866 (0.0486)\tPrec@1 96.875 (98.296)\n",
            "Epoch: [119][275/391]\tLoss 0.0375 (0.0491)\tPrec@1 98.438 (98.287)\n",
            "Epoch: [119][330/391]\tLoss 0.0384 (0.0494)\tPrec@1 98.438 (98.294)\n",
            "Epoch: [119][385/391]\tLoss 0.0634 (0.0494)\tPrec@1 97.656 (98.310)\n",
            "Test\t  Prec@1: 91.530 (Err: 8.470 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [120][0/391]\tLoss 0.0570 (0.0570)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [120][55/391]\tLoss 0.0614 (0.0457)\tPrec@1 97.656 (98.382)\n",
            "Epoch: [120][110/391]\tLoss 0.0522 (0.0479)\tPrec@1 98.438 (98.452)\n",
            "Epoch: [120][165/391]\tLoss 0.0158 (0.0482)\tPrec@1 100.000 (98.395)\n",
            "Epoch: [120][220/391]\tLoss 0.0226 (0.0487)\tPrec@1 100.000 (98.388)\n",
            "Epoch: [120][275/391]\tLoss 0.0381 (0.0489)\tPrec@1 98.438 (98.384)\n",
            "Epoch: [120][330/391]\tLoss 0.0737 (0.0495)\tPrec@1 97.656 (98.360)\n",
            "Epoch: [120][385/391]\tLoss 0.0947 (0.0497)\tPrec@1 96.875 (98.352)\n",
            "Test\t  Prec@1: 91.430 (Err: 8.570 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [121][0/391]\tLoss 0.0147 (0.0147)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [121][55/391]\tLoss 0.0590 (0.0476)\tPrec@1 98.438 (98.451)\n",
            "Epoch: [121][110/391]\tLoss 0.0703 (0.0481)\tPrec@1 98.438 (98.402)\n",
            "Epoch: [121][165/391]\tLoss 0.0512 (0.0493)\tPrec@1 98.438 (98.357)\n",
            "Epoch: [121][220/391]\tLoss 0.0656 (0.0497)\tPrec@1 96.094 (98.335)\n",
            "Epoch: [121][275/391]\tLoss 0.0179 (0.0489)\tPrec@1 99.219 (98.358)\n",
            "Epoch: [121][330/391]\tLoss 0.0196 (0.0490)\tPrec@1 100.000 (98.378)\n",
            "Epoch: [121][385/391]\tLoss 0.0313 (0.0487)\tPrec@1 98.438 (98.379)\n",
            "Test\t  Prec@1: 91.540 (Err: 8.460 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [122][0/391]\tLoss 0.0312 (0.0312)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [122][55/391]\tLoss 0.0442 (0.0470)\tPrec@1 98.438 (98.563)\n",
            "Epoch: [122][110/391]\tLoss 0.0221 (0.0465)\tPrec@1 99.219 (98.494)\n",
            "Epoch: [122][165/391]\tLoss 0.0663 (0.0449)\tPrec@1 96.875 (98.522)\n",
            "Epoch: [122][220/391]\tLoss 0.0548 (0.0444)\tPrec@1 98.438 (98.533)\n",
            "Epoch: [122][275/391]\tLoss 0.0789 (0.0453)\tPrec@1 97.656 (98.528)\n",
            "Epoch: [122][330/391]\tLoss 0.0170 (0.0457)\tPrec@1 99.219 (98.508)\n",
            "Epoch: [122][385/391]\tLoss 0.0492 (0.0462)\tPrec@1 98.438 (98.474)\n",
            "Test\t  Prec@1: 91.410 (Err: 8.590 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [123][0/391]\tLoss 0.0356 (0.0356)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [123][55/391]\tLoss 0.0534 (0.0458)\tPrec@1 98.438 (98.382)\n",
            "Epoch: [123][110/391]\tLoss 0.0520 (0.0471)\tPrec@1 98.438 (98.332)\n",
            "Epoch: [123][165/391]\tLoss 0.0496 (0.0469)\tPrec@1 98.438 (98.414)\n",
            "Epoch: [123][220/391]\tLoss 0.0580 (0.0457)\tPrec@1 97.656 (98.501)\n",
            "Epoch: [123][275/391]\tLoss 0.0174 (0.0447)\tPrec@1 99.219 (98.508)\n",
            "Epoch: [123][330/391]\tLoss 0.0734 (0.0451)\tPrec@1 96.875 (98.468)\n",
            "Epoch: [123][385/391]\tLoss 0.0297 (0.0457)\tPrec@1 99.219 (98.470)\n",
            "Test\t  Prec@1: 91.530 (Err: 8.470 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [124][0/391]\tLoss 0.0625 (0.0625)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [124][55/391]\tLoss 0.0161 (0.0399)\tPrec@1 100.000 (98.842)\n",
            "Epoch: [124][110/391]\tLoss 0.0606 (0.0419)\tPrec@1 98.438 (98.726)\n",
            "Epoch: [124][165/391]\tLoss 0.0382 (0.0461)\tPrec@1 98.438 (98.560)\n",
            "Epoch: [124][220/391]\tLoss 0.0459 (0.0461)\tPrec@1 98.438 (98.536)\n",
            "Epoch: [124][275/391]\tLoss 0.0270 (0.0458)\tPrec@1 99.219 (98.548)\n",
            "Epoch: [124][330/391]\tLoss 0.0408 (0.0462)\tPrec@1 99.219 (98.518)\n",
            "Epoch: [124][385/391]\tLoss 0.0655 (0.0462)\tPrec@1 96.875 (98.500)\n",
            "Test\t  Prec@1: 91.540 (Err: 8.460 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [125][0/391]\tLoss 0.0441 (0.0441)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [125][55/391]\tLoss 0.0153 (0.0414)\tPrec@1 99.219 (98.619)\n",
            "Epoch: [125][110/391]\tLoss 0.0294 (0.0414)\tPrec@1 98.438 (98.606)\n",
            "Epoch: [125][165/391]\tLoss 0.0390 (0.0432)\tPrec@1 98.438 (98.522)\n",
            "Epoch: [125][220/391]\tLoss 0.0478 (0.0439)\tPrec@1 97.656 (98.505)\n",
            "Epoch: [125][275/391]\tLoss 0.0377 (0.0437)\tPrec@1 98.438 (98.511)\n",
            "Epoch: [125][330/391]\tLoss 0.0405 (0.0439)\tPrec@1 97.656 (98.506)\n",
            "Epoch: [125][385/391]\tLoss 0.0368 (0.0444)\tPrec@1 99.219 (98.500)\n",
            "Test\t  Prec@1: 91.640 (Err: 8.360 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [126][0/391]\tLoss 0.0278 (0.0278)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [126][55/391]\tLoss 0.0324 (0.0422)\tPrec@1 98.438 (98.577)\n",
            "Epoch: [126][110/391]\tLoss 0.0450 (0.0393)\tPrec@1 98.438 (98.712)\n",
            "Epoch: [126][165/391]\tLoss 0.0606 (0.0399)\tPrec@1 96.875 (98.687)\n",
            "Epoch: [126][220/391]\tLoss 0.0540 (0.0422)\tPrec@1 97.656 (98.621)\n",
            "Epoch: [126][275/391]\tLoss 0.0144 (0.0428)\tPrec@1 100.000 (98.590)\n",
            "Epoch: [126][330/391]\tLoss 0.0172 (0.0428)\tPrec@1 100.000 (98.598)\n",
            "Epoch: [126][385/391]\tLoss 0.0380 (0.0425)\tPrec@1 98.438 (98.595)\n",
            "Test\t  Prec@1: 91.560 (Err: 8.440 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [127][0/391]\tLoss 0.0282 (0.0282)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [127][55/391]\tLoss 0.0386 (0.0439)\tPrec@1 99.219 (98.465)\n",
            "Epoch: [127][110/391]\tLoss 0.0637 (0.0437)\tPrec@1 97.656 (98.508)\n",
            "Epoch: [127][165/391]\tLoss 0.0438 (0.0441)\tPrec@1 97.656 (98.518)\n",
            "Epoch: [127][220/391]\tLoss 0.0383 (0.0441)\tPrec@1 98.438 (98.487)\n",
            "Epoch: [127][275/391]\tLoss 0.0735 (0.0435)\tPrec@1 96.094 (98.522)\n",
            "Epoch: [127][330/391]\tLoss 0.0676 (0.0441)\tPrec@1 98.438 (98.480)\n",
            "Epoch: [127][385/391]\tLoss 0.0476 (0.0447)\tPrec@1 97.656 (98.466)\n",
            "Test\t  Prec@1: 91.330 (Err: 8.670 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [128][0/391]\tLoss 0.0808 (0.0808)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [128][55/391]\tLoss 0.0479 (0.0378)\tPrec@1 97.656 (98.884)\n",
            "Epoch: [128][110/391]\tLoss 0.0287 (0.0384)\tPrec@1 99.219 (98.761)\n",
            "Epoch: [128][165/391]\tLoss 0.0232 (0.0407)\tPrec@1 99.219 (98.734)\n",
            "Epoch: [128][220/391]\tLoss 0.0167 (0.0408)\tPrec@1 100.000 (98.720)\n",
            "Epoch: [128][275/391]\tLoss 0.0096 (0.0421)\tPrec@1 100.000 (98.621)\n",
            "Epoch: [128][330/391]\tLoss 0.0553 (0.0423)\tPrec@1 97.656 (98.591)\n",
            "Epoch: [128][385/391]\tLoss 0.0237 (0.0431)\tPrec@1 99.219 (98.583)\n",
            "Test\t  Prec@1: 91.440 (Err: 8.560 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [129][0/391]\tLoss 0.0321 (0.0321)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [129][55/391]\tLoss 0.0387 (0.0364)\tPrec@1 99.219 (98.954)\n",
            "Epoch: [129][110/391]\tLoss 0.0394 (0.0370)\tPrec@1 98.438 (98.881)\n",
            "Epoch: [129][165/391]\tLoss 0.0235 (0.0394)\tPrec@1 99.219 (98.786)\n",
            "Epoch: [129][220/391]\tLoss 0.0327 (0.0405)\tPrec@1 100.000 (98.717)\n",
            "Epoch: [129][275/391]\tLoss 0.0580 (0.0406)\tPrec@1 97.656 (98.684)\n",
            "Epoch: [129][330/391]\tLoss 0.0404 (0.0409)\tPrec@1 97.656 (98.659)\n",
            "Epoch: [129][385/391]\tLoss 0.0322 (0.0415)\tPrec@1 98.438 (98.644)\n",
            "Test\t  Prec@1: 91.530 (Err: 8.470 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [130][0/391]\tLoss 0.0345 (0.0345)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [130][55/391]\tLoss 0.0299 (0.0379)\tPrec@1 99.219 (98.800)\n",
            "Epoch: [130][110/391]\tLoss 0.0433 (0.0353)\tPrec@1 97.656 (98.867)\n",
            "Epoch: [130][165/391]\tLoss 0.0755 (0.0372)\tPrec@1 97.656 (98.781)\n",
            "Epoch: [130][220/391]\tLoss 0.0344 (0.0380)\tPrec@1 98.438 (98.773)\n",
            "Epoch: [130][275/391]\tLoss 0.0553 (0.0383)\tPrec@1 97.656 (98.757)\n",
            "Epoch: [130][330/391]\tLoss 0.0285 (0.0388)\tPrec@1 99.219 (98.728)\n",
            "Epoch: [130][385/391]\tLoss 0.0450 (0.0390)\tPrec@1 98.438 (98.713)\n",
            "Test\t  Prec@1: 91.400 (Err: 8.600 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [131][0/391]\tLoss 0.0200 (0.0200)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [131][55/391]\tLoss 0.0282 (0.0390)\tPrec@1 99.219 (98.758)\n",
            "Epoch: [131][110/391]\tLoss 0.0275 (0.0412)\tPrec@1 100.000 (98.656)\n",
            "Epoch: [131][165/391]\tLoss 0.0407 (0.0390)\tPrec@1 99.219 (98.739)\n",
            "Epoch: [131][220/391]\tLoss 0.0162 (0.0385)\tPrec@1 99.219 (98.770)\n",
            "Epoch: [131][275/391]\tLoss 0.0484 (0.0386)\tPrec@1 97.656 (98.772)\n",
            "Epoch: [131][330/391]\tLoss 0.0314 (0.0389)\tPrec@1 98.438 (98.749)\n",
            "Epoch: [131][385/391]\tLoss 0.0674 (0.0390)\tPrec@1 96.875 (98.747)\n",
            "Test\t  Prec@1: 91.430 (Err: 8.570 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [132][0/391]\tLoss 0.0728 (0.0728)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [132][55/391]\tLoss 0.0754 (0.0359)\tPrec@1 96.875 (98.898)\n",
            "Epoch: [132][110/391]\tLoss 0.0462 (0.0354)\tPrec@1 98.438 (98.846)\n",
            "Epoch: [132][165/391]\tLoss 0.0134 (0.0355)\tPrec@1 100.000 (98.847)\n",
            "Epoch: [132][220/391]\tLoss 0.0524 (0.0361)\tPrec@1 99.219 (98.798)\n",
            "Epoch: [132][275/391]\tLoss 0.0440 (0.0361)\tPrec@1 98.438 (98.814)\n",
            "Epoch: [132][330/391]\tLoss 0.0141 (0.0365)\tPrec@1 100.000 (98.780)\n",
            "Epoch: [132][385/391]\tLoss 0.0497 (0.0368)\tPrec@1 98.438 (98.771)\n",
            "Test\t  Prec@1: 91.470 (Err: 8.530 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [133][0/391]\tLoss 0.0309 (0.0309)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [133][55/391]\tLoss 0.0544 (0.0360)\tPrec@1 97.656 (98.842)\n",
            "Epoch: [133][110/391]\tLoss 0.0136 (0.0349)\tPrec@1 100.000 (98.881)\n",
            "Epoch: [133][165/391]\tLoss 0.0231 (0.0345)\tPrec@1 100.000 (98.903)\n",
            "Epoch: [133][220/391]\tLoss 0.0776 (0.0344)\tPrec@1 98.438 (98.894)\n",
            "Epoch: [133][275/391]\tLoss 0.0365 (0.0355)\tPrec@1 98.438 (98.871)\n",
            "Epoch: [133][330/391]\tLoss 0.0649 (0.0357)\tPrec@1 97.656 (98.843)\n",
            "Epoch: [133][385/391]\tLoss 0.0296 (0.0361)\tPrec@1 98.438 (98.808)\n",
            "Test\t  Prec@1: 91.470 (Err: 8.530 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [134][0/391]\tLoss 0.0244 (0.0244)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [134][55/391]\tLoss 0.0364 (0.0390)\tPrec@1 98.438 (98.717)\n",
            "Epoch: [134][110/391]\tLoss 0.0269 (0.0390)\tPrec@1 100.000 (98.719)\n",
            "Epoch: [134][165/391]\tLoss 0.0562 (0.0384)\tPrec@1 98.438 (98.687)\n",
            "Epoch: [134][220/391]\tLoss 0.0276 (0.0381)\tPrec@1 100.000 (98.717)\n",
            "Epoch: [134][275/391]\tLoss 0.0244 (0.0379)\tPrec@1 98.438 (98.743)\n",
            "Epoch: [134][330/391]\tLoss 0.0413 (0.0381)\tPrec@1 97.656 (98.749)\n",
            "Epoch: [134][385/391]\tLoss 0.0341 (0.0379)\tPrec@1 99.219 (98.763)\n",
            "Test\t  Prec@1: 91.410 (Err: 8.590 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [135][0/391]\tLoss 0.0043 (0.0043)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [135][55/391]\tLoss 0.0319 (0.0400)\tPrec@1 97.656 (98.563)\n",
            "Epoch: [135][110/391]\tLoss 0.0309 (0.0387)\tPrec@1 99.219 (98.642)\n",
            "Epoch: [135][165/391]\tLoss 0.0293 (0.0373)\tPrec@1 99.219 (98.678)\n",
            "Epoch: [135][220/391]\tLoss 0.0306 (0.0374)\tPrec@1 99.219 (98.681)\n",
            "Epoch: [135][275/391]\tLoss 0.0105 (0.0381)\tPrec@1 100.000 (98.664)\n",
            "Epoch: [135][330/391]\tLoss 0.0714 (0.0377)\tPrec@1 96.875 (98.695)\n",
            "Epoch: [135][385/391]\tLoss 0.1395 (0.0384)\tPrec@1 94.531 (98.670)\n",
            "Test\t  Prec@1: 91.420 (Err: 8.580 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [136][0/391]\tLoss 0.0567 (0.0567)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [136][55/391]\tLoss 0.0578 (0.0364)\tPrec@1 98.438 (98.758)\n",
            "Epoch: [136][110/391]\tLoss 0.0408 (0.0342)\tPrec@1 98.438 (98.902)\n",
            "Epoch: [136][165/391]\tLoss 0.0939 (0.0360)\tPrec@1 96.875 (98.828)\n",
            "Epoch: [136][220/391]\tLoss 0.0693 (0.0368)\tPrec@1 97.656 (98.784)\n",
            "Epoch: [136][275/391]\tLoss 0.0601 (0.0366)\tPrec@1 98.438 (98.803)\n",
            "Epoch: [136][330/391]\tLoss 0.0122 (0.0371)\tPrec@1 100.000 (98.796)\n",
            "Epoch: [136][385/391]\tLoss 0.0419 (0.0368)\tPrec@1 98.438 (98.814)\n",
            "Test\t  Prec@1: 91.710 (Err: 8.290 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [137][0/391]\tLoss 0.0196 (0.0196)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [137][55/391]\tLoss 0.0226 (0.0351)\tPrec@1 100.000 (98.884)\n",
            "Epoch: [137][110/391]\tLoss 0.0090 (0.0349)\tPrec@1 100.000 (98.832)\n",
            "Epoch: [137][165/391]\tLoss 0.0452 (0.0344)\tPrec@1 98.438 (98.828)\n",
            "Epoch: [137][220/391]\tLoss 0.0168 (0.0362)\tPrec@1 100.000 (98.717)\n",
            "Epoch: [137][275/391]\tLoss 0.0803 (0.0372)\tPrec@1 95.312 (98.661)\n",
            "Epoch: [137][330/391]\tLoss 0.0343 (0.0377)\tPrec@1 99.219 (98.662)\n",
            "Epoch: [137][385/391]\tLoss 0.0439 (0.0379)\tPrec@1 98.438 (98.652)\n",
            "Test\t  Prec@1: 91.250 (Err: 8.750 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [138][0/391]\tLoss 0.0206 (0.0206)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [138][55/391]\tLoss 0.0086 (0.0314)\tPrec@1 100.000 (99.009)\n",
            "Epoch: [138][110/391]\tLoss 0.0363 (0.0338)\tPrec@1 98.438 (98.916)\n",
            "Epoch: [138][165/391]\tLoss 0.0667 (0.0350)\tPrec@1 96.875 (98.847)\n",
            "Epoch: [138][220/391]\tLoss 0.0421 (0.0347)\tPrec@1 98.438 (98.869)\n",
            "Epoch: [138][275/391]\tLoss 0.0396 (0.0350)\tPrec@1 98.438 (98.837)\n",
            "Epoch: [138][330/391]\tLoss 0.0306 (0.0353)\tPrec@1 99.219 (98.841)\n",
            "Epoch: [138][385/391]\tLoss 0.0280 (0.0353)\tPrec@1 99.219 (98.836)\n",
            "Test\t  Prec@1: 91.450 (Err: 8.550 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [139][0/391]\tLoss 0.0178 (0.0178)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [139][55/391]\tLoss 0.0255 (0.0355)\tPrec@1 99.219 (98.842)\n",
            "Epoch: [139][110/391]\tLoss 0.0296 (0.0365)\tPrec@1 98.438 (98.782)\n",
            "Epoch: [139][165/391]\tLoss 0.0185 (0.0357)\tPrec@1 100.000 (98.856)\n",
            "Epoch: [139][220/391]\tLoss 0.0394 (0.0349)\tPrec@1 98.438 (98.908)\n",
            "Epoch: [139][275/391]\tLoss 0.0299 (0.0354)\tPrec@1 99.219 (98.902)\n",
            "Epoch: [139][330/391]\tLoss 0.0567 (0.0352)\tPrec@1 98.438 (98.902)\n",
            "Epoch: [139][385/391]\tLoss 0.0234 (0.0343)\tPrec@1 99.219 (98.929)\n",
            "Test\t  Prec@1: 91.410 (Err: 8.590 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [140][0/391]\tLoss 0.0297 (0.0297)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [140][55/391]\tLoss 0.0186 (0.0310)\tPrec@1 99.219 (99.023)\n",
            "Epoch: [140][110/391]\tLoss 0.0276 (0.0309)\tPrec@1 99.219 (98.979)\n",
            "Epoch: [140][165/391]\tLoss 0.0417 (0.0321)\tPrec@1 98.438 (98.979)\n",
            "Epoch: [140][220/391]\tLoss 0.0477 (0.0332)\tPrec@1 98.438 (98.943)\n",
            "Epoch: [140][275/391]\tLoss 0.0310 (0.0332)\tPrec@1 98.438 (98.944)\n",
            "Epoch: [140][330/391]\tLoss 0.0463 (0.0337)\tPrec@1 99.219 (98.943)\n",
            "Epoch: [140][385/391]\tLoss 0.0113 (0.0337)\tPrec@1 100.000 (98.952)\n",
            "Test\t  Prec@1: 91.330 (Err: 8.670 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [141][0/391]\tLoss 0.0407 (0.0407)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [141][55/391]\tLoss 0.0318 (0.0324)\tPrec@1 99.219 (98.898)\n",
            "Epoch: [141][110/391]\tLoss 0.0512 (0.0322)\tPrec@1 98.438 (98.958)\n",
            "Epoch: [141][165/391]\tLoss 0.0271 (0.0326)\tPrec@1 100.000 (98.936)\n",
            "Epoch: [141][220/391]\tLoss 0.0289 (0.0328)\tPrec@1 98.438 (98.936)\n",
            "Epoch: [141][275/391]\tLoss 0.0149 (0.0334)\tPrec@1 100.000 (98.924)\n",
            "Epoch: [141][330/391]\tLoss 0.0182 (0.0336)\tPrec@1 99.219 (98.912)\n",
            "Epoch: [141][385/391]\tLoss 0.0584 (0.0343)\tPrec@1 98.438 (98.871)\n",
            "Test\t  Prec@1: 91.200 (Err: 8.800 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [142][0/391]\tLoss 0.0249 (0.0249)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [142][55/391]\tLoss 0.0359 (0.0378)\tPrec@1 99.219 (98.772)\n",
            "Epoch: [142][110/391]\tLoss 0.0188 (0.0350)\tPrec@1 100.000 (98.839)\n",
            "Epoch: [142][165/391]\tLoss 0.0068 (0.0328)\tPrec@1 100.000 (98.908)\n",
            "Epoch: [142][220/391]\tLoss 0.0271 (0.0330)\tPrec@1 99.219 (98.897)\n",
            "Epoch: [142][275/391]\tLoss 0.0564 (0.0325)\tPrec@1 98.438 (98.944)\n",
            "Epoch: [142][330/391]\tLoss 0.0073 (0.0326)\tPrec@1 100.000 (98.952)\n",
            "Epoch: [142][385/391]\tLoss 0.0164 (0.0326)\tPrec@1 100.000 (98.943)\n",
            "Test\t  Prec@1: 91.420 (Err: 8.580 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [143][0/391]\tLoss 0.0443 (0.0443)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [143][55/391]\tLoss 0.0199 (0.0318)\tPrec@1 99.219 (98.954)\n",
            "Epoch: [143][110/391]\tLoss 0.0214 (0.0305)\tPrec@1 100.000 (98.979)\n",
            "Epoch: [143][165/391]\tLoss 0.0136 (0.0306)\tPrec@1 99.219 (98.979)\n",
            "Epoch: [143][220/391]\tLoss 0.0734 (0.0319)\tPrec@1 98.438 (98.957)\n",
            "Epoch: [143][275/391]\tLoss 0.0267 (0.0320)\tPrec@1 98.438 (98.936)\n",
            "Epoch: [143][330/391]\tLoss 0.0173 (0.0317)\tPrec@1 100.000 (98.957)\n",
            "Epoch: [143][385/391]\tLoss 0.0212 (0.0325)\tPrec@1 100.000 (98.919)\n",
            "Test\t  Prec@1: 91.550 (Err: 8.450 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [144][0/391]\tLoss 0.0376 (0.0376)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [144][55/391]\tLoss 0.0098 (0.0259)\tPrec@1 100.000 (99.289)\n",
            "Epoch: [144][110/391]\tLoss 0.0442 (0.0292)\tPrec@1 99.219 (99.184)\n",
            "Epoch: [144][165/391]\tLoss 0.0150 (0.0305)\tPrec@1 100.000 (99.101)\n",
            "Epoch: [144][220/391]\tLoss 0.0290 (0.0309)\tPrec@1 100.000 (99.063)\n",
            "Epoch: [144][275/391]\tLoss 0.0415 (0.0317)\tPrec@1 99.219 (99.040)\n",
            "Epoch: [144][330/391]\tLoss 0.0519 (0.0319)\tPrec@1 98.438 (99.025)\n",
            "Epoch: [144][385/391]\tLoss 0.0493 (0.0323)\tPrec@1 98.438 (99.006)\n",
            "Test\t  Prec@1: 91.220 (Err: 8.780 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [145][0/391]\tLoss 0.0253 (0.0253)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [145][55/391]\tLoss 0.0336 (0.0326)\tPrec@1 98.438 (98.954)\n",
            "Epoch: [145][110/391]\tLoss 0.0061 (0.0323)\tPrec@1 100.000 (98.937)\n",
            "Epoch: [145][165/391]\tLoss 0.0262 (0.0314)\tPrec@1 99.219 (98.969)\n",
            "Epoch: [145][220/391]\tLoss 0.0148 (0.0301)\tPrec@1 100.000 (99.031)\n",
            "Epoch: [145][275/391]\tLoss 0.0218 (0.0303)\tPrec@1 99.219 (99.032)\n",
            "Epoch: [145][330/391]\tLoss 0.0300 (0.0310)\tPrec@1 98.438 (98.990)\n",
            "Epoch: [145][385/391]\tLoss 0.0399 (0.0311)\tPrec@1 99.219 (98.992)\n",
            "Test\t  Prec@1: 91.480 (Err: 8.520 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [146][0/391]\tLoss 0.0144 (0.0144)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [146][55/391]\tLoss 0.0185 (0.0304)\tPrec@1 100.000 (99.191)\n",
            "Epoch: [146][110/391]\tLoss 0.0311 (0.0302)\tPrec@1 98.438 (99.120)\n",
            "Epoch: [146][165/391]\tLoss 0.0178 (0.0306)\tPrec@1 100.000 (99.111)\n",
            "Epoch: [146][220/391]\tLoss 0.0089 (0.0298)\tPrec@1 100.000 (99.102)\n",
            "Epoch: [146][275/391]\tLoss 0.0253 (0.0304)\tPrec@1 98.438 (99.066)\n",
            "Epoch: [146][330/391]\tLoss 0.0202 (0.0313)\tPrec@1 100.000 (99.028)\n",
            "Epoch: [146][385/391]\tLoss 0.0452 (0.0317)\tPrec@1 98.438 (99.026)\n",
            "Test\t  Prec@1: 91.370 (Err: 8.630 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [147][0/391]\tLoss 0.0335 (0.0335)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [147][55/391]\tLoss 0.0236 (0.0322)\tPrec@1 100.000 (98.996)\n",
            "Epoch: [147][110/391]\tLoss 0.0487 (0.0323)\tPrec@1 97.656 (98.958)\n",
            "Epoch: [147][165/391]\tLoss 0.0157 (0.0309)\tPrec@1 100.000 (99.002)\n",
            "Epoch: [147][220/391]\tLoss 0.0149 (0.0303)\tPrec@1 100.000 (98.993)\n",
            "Epoch: [147][275/391]\tLoss 0.0302 (0.0305)\tPrec@1 97.656 (98.978)\n",
            "Epoch: [147][330/391]\tLoss 0.0276 (0.0310)\tPrec@1 99.219 (98.964)\n",
            "Epoch: [147][385/391]\tLoss 0.0084 (0.0307)\tPrec@1 100.000 (98.992)\n",
            "Test\t  Prec@1: 91.130 (Err: 8.870 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [148][0/391]\tLoss 0.0173 (0.0173)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [148][55/391]\tLoss 0.0123 (0.0310)\tPrec@1 100.000 (98.982)\n",
            "Epoch: [148][110/391]\tLoss 0.0162 (0.0309)\tPrec@1 100.000 (98.958)\n",
            "Epoch: [148][165/391]\tLoss 0.0201 (0.0317)\tPrec@1 100.000 (98.932)\n",
            "Epoch: [148][220/391]\tLoss 0.0239 (0.0306)\tPrec@1 99.219 (99.003)\n",
            "Epoch: [148][275/391]\tLoss 0.0291 (0.0306)\tPrec@1 98.438 (99.009)\n",
            "Epoch: [148][330/391]\tLoss 0.0208 (0.0305)\tPrec@1 99.219 (99.004)\n",
            "Epoch: [148][385/391]\tLoss 0.0227 (0.0309)\tPrec@1 99.219 (99.004)\n",
            "Test\t  Prec@1: 91.440 (Err: 8.560 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [149][0/391]\tLoss 0.0479 (0.0479)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [149][55/391]\tLoss 0.0246 (0.0296)\tPrec@1 99.219 (99.065)\n",
            "Epoch: [149][110/391]\tLoss 0.0531 (0.0298)\tPrec@1 97.656 (99.043)\n",
            "Epoch: [149][165/391]\tLoss 0.0568 (0.0308)\tPrec@1 97.656 (99.016)\n",
            "Epoch: [149][220/391]\tLoss 0.0239 (0.0304)\tPrec@1 99.219 (99.014)\n",
            "Epoch: [149][275/391]\tLoss 0.0283 (0.0299)\tPrec@1 99.219 (99.009)\n",
            "Epoch: [149][330/391]\tLoss 0.0149 (0.0306)\tPrec@1 100.000 (98.969)\n",
            "Epoch: [149][385/391]\tLoss 0.0333 (0.0306)\tPrec@1 99.219 (98.974)\n",
            "Test\t  Prec@1: 91.690 (Err: 8.310 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [150][0/391]\tLoss 0.0190 (0.0190)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [150][55/391]\tLoss 0.0134 (0.0290)\tPrec@1 100.000 (99.135)\n",
            "Epoch: [150][110/391]\tLoss 0.0144 (0.0272)\tPrec@1 100.000 (99.233)\n",
            "Epoch: [150][165/391]\tLoss 0.0519 (0.0274)\tPrec@1 98.438 (99.172)\n",
            "Epoch: [150][220/391]\tLoss 0.0282 (0.0267)\tPrec@1 98.438 (99.208)\n",
            "Epoch: [150][275/391]\tLoss 0.0148 (0.0265)\tPrec@1 99.219 (99.205)\n",
            "Epoch: [150][330/391]\tLoss 0.0088 (0.0262)\tPrec@1 100.000 (99.205)\n",
            "Epoch: [150][385/391]\tLoss 0.0217 (0.0267)\tPrec@1 100.000 (99.199)\n",
            "Test\t  Prec@1: 91.720 (Err: 8.280 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [151][0/391]\tLoss 0.0203 (0.0203)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [151][55/391]\tLoss 0.0722 (0.0287)\tPrec@1 97.656 (99.079)\n",
            "Epoch: [151][110/391]\tLoss 0.0397 (0.0259)\tPrec@1 99.219 (99.212)\n",
            "Epoch: [151][165/391]\tLoss 0.0272 (0.0254)\tPrec@1 99.219 (99.247)\n",
            "Epoch: [151][220/391]\tLoss 0.0293 (0.0257)\tPrec@1 99.219 (99.215)\n",
            "Epoch: [151][275/391]\tLoss 0.0201 (0.0256)\tPrec@1 100.000 (99.241)\n",
            "Epoch: [151][330/391]\tLoss 0.0113 (0.0255)\tPrec@1 100.000 (99.231)\n",
            "Epoch: [151][385/391]\tLoss 0.0319 (0.0248)\tPrec@1 99.219 (99.261)\n",
            "Test\t  Prec@1: 91.760 (Err: 8.240 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [152][0/391]\tLoss 0.0267 (0.0267)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [152][55/391]\tLoss 0.0632 (0.0227)\tPrec@1 96.875 (99.358)\n",
            "Epoch: [152][110/391]\tLoss 0.0299 (0.0225)\tPrec@1 99.219 (99.345)\n",
            "Epoch: [152][165/391]\tLoss 0.0462 (0.0239)\tPrec@1 98.438 (99.271)\n",
            "Epoch: [152][220/391]\tLoss 0.0146 (0.0235)\tPrec@1 100.000 (99.289)\n",
            "Epoch: [152][275/391]\tLoss 0.0154 (0.0231)\tPrec@1 100.000 (99.323)\n",
            "Epoch: [152][330/391]\tLoss 0.0364 (0.0234)\tPrec@1 99.219 (99.316)\n",
            "Epoch: [152][385/391]\tLoss 0.0258 (0.0237)\tPrec@1 100.000 (99.298)\n",
            "Test\t  Prec@1: 91.730 (Err: 8.270 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [153][0/391]\tLoss 0.0180 (0.0180)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [153][55/391]\tLoss 0.0325 (0.0219)\tPrec@1 98.438 (99.358)\n",
            "Epoch: [153][110/391]\tLoss 0.0154 (0.0204)\tPrec@1 99.219 (99.437)\n",
            "Epoch: [153][165/391]\tLoss 0.0170 (0.0211)\tPrec@1 100.000 (99.421)\n",
            "Epoch: [153][220/391]\tLoss 0.0125 (0.0214)\tPrec@1 100.000 (99.424)\n",
            "Epoch: [153][275/391]\tLoss 0.0306 (0.0206)\tPrec@1 99.219 (99.457)\n",
            "Epoch: [153][330/391]\tLoss 0.0204 (0.0212)\tPrec@1 99.219 (99.441)\n",
            "Epoch: [153][385/391]\tLoss 0.0175 (0.0219)\tPrec@1 100.000 (99.419)\n",
            "Test\t  Prec@1: 91.690 (Err: 8.310 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [154][0/391]\tLoss 0.0119 (0.0119)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [154][55/391]\tLoss 0.0148 (0.0230)\tPrec@1 99.219 (99.316)\n",
            "Epoch: [154][110/391]\tLoss 0.0110 (0.0217)\tPrec@1 100.000 (99.374)\n",
            "Epoch: [154][165/391]\tLoss 0.0342 (0.0221)\tPrec@1 99.219 (99.369)\n",
            "Epoch: [154][220/391]\tLoss 0.0077 (0.0221)\tPrec@1 100.000 (99.399)\n",
            "Epoch: [154][275/391]\tLoss 0.0269 (0.0215)\tPrec@1 98.438 (99.423)\n",
            "Epoch: [154][330/391]\tLoss 0.0316 (0.0213)\tPrec@1 99.219 (99.441)\n",
            "Epoch: [154][385/391]\tLoss 0.0182 (0.0213)\tPrec@1 100.000 (99.435)\n",
            "Test\t  Prec@1: 91.820 (Err: 8.180 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [155][0/391]\tLoss 0.0195 (0.0195)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [155][55/391]\tLoss 0.0372 (0.0210)\tPrec@1 98.438 (99.498)\n",
            "Epoch: [155][110/391]\tLoss 0.0165 (0.0205)\tPrec@1 100.000 (99.458)\n",
            "Epoch: [155][165/391]\tLoss 0.0439 (0.0204)\tPrec@1 98.438 (99.459)\n",
            "Epoch: [155][220/391]\tLoss 0.0323 (0.0209)\tPrec@1 98.438 (99.441)\n",
            "Epoch: [155][275/391]\tLoss 0.0392 (0.0208)\tPrec@1 98.438 (99.431)\n",
            "Epoch: [155][330/391]\tLoss 0.0088 (0.0203)\tPrec@1 100.000 (99.438)\n",
            "Epoch: [155][385/391]\tLoss 0.0144 (0.0204)\tPrec@1 100.000 (99.441)\n",
            "Test\t  Prec@1: 91.730 (Err: 8.270 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [156][0/391]\tLoss 0.0025 (0.0025)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [156][55/391]\tLoss 0.0246 (0.0213)\tPrec@1 99.219 (99.414)\n",
            "Epoch: [156][110/391]\tLoss 0.0330 (0.0212)\tPrec@1 98.438 (99.465)\n",
            "Epoch: [156][165/391]\tLoss 0.0487 (0.0223)\tPrec@1 98.438 (99.383)\n",
            "Epoch: [156][220/391]\tLoss 0.0094 (0.0219)\tPrec@1 100.000 (99.406)\n",
            "Epoch: [156][275/391]\tLoss 0.0068 (0.0215)\tPrec@1 100.000 (99.417)\n",
            "Epoch: [156][330/391]\tLoss 0.0155 (0.0212)\tPrec@1 100.000 (99.408)\n",
            "Epoch: [156][385/391]\tLoss 0.0071 (0.0209)\tPrec@1 100.000 (99.431)\n",
            "Test\t  Prec@1: 91.670 (Err: 8.330 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [157][0/391]\tLoss 0.0143 (0.0143)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [157][55/391]\tLoss 0.0275 (0.0202)\tPrec@1 99.219 (99.372)\n",
            "Epoch: [157][110/391]\tLoss 0.0144 (0.0216)\tPrec@1 100.000 (99.352)\n",
            "Epoch: [157][165/391]\tLoss 0.0165 (0.0213)\tPrec@1 100.000 (99.355)\n",
            "Epoch: [157][220/391]\tLoss 0.0243 (0.0216)\tPrec@1 100.000 (99.360)\n",
            "Epoch: [157][275/391]\tLoss 0.0330 (0.0211)\tPrec@1 99.219 (99.389)\n",
            "Epoch: [157][330/391]\tLoss 0.0265 (0.0213)\tPrec@1 98.438 (99.375)\n",
            "Epoch: [157][385/391]\tLoss 0.0401 (0.0212)\tPrec@1 99.219 (99.387)\n",
            "Test\t  Prec@1: 91.810 (Err: 8.190 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [158][0/391]\tLoss 0.0162 (0.0162)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [158][55/391]\tLoss 0.0160 (0.0200)\tPrec@1 99.219 (99.442)\n",
            "Epoch: [158][110/391]\tLoss 0.0200 (0.0215)\tPrec@1 99.219 (99.360)\n",
            "Epoch: [158][165/391]\tLoss 0.0380 (0.0211)\tPrec@1 99.219 (99.369)\n",
            "Epoch: [158][220/391]\tLoss 0.0181 (0.0212)\tPrec@1 100.000 (99.385)\n",
            "Epoch: [158][275/391]\tLoss 0.0266 (0.0208)\tPrec@1 99.219 (99.411)\n",
            "Epoch: [158][330/391]\tLoss 0.0408 (0.0215)\tPrec@1 99.219 (99.391)\n",
            "Epoch: [158][385/391]\tLoss 0.0101 (0.0217)\tPrec@1 100.000 (99.373)\n",
            "Test\t  Prec@1: 91.850 (Err: 8.150 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [159][0/391]\tLoss 0.0562 (0.0562)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [159][55/391]\tLoss 0.0145 (0.0195)\tPrec@1 100.000 (99.442)\n",
            "Epoch: [159][110/391]\tLoss 0.0089 (0.0205)\tPrec@1 100.000 (99.388)\n",
            "Epoch: [159][165/391]\tLoss 0.0150 (0.0210)\tPrec@1 100.000 (99.402)\n",
            "Epoch: [159][220/391]\tLoss 0.0135 (0.0198)\tPrec@1 100.000 (99.452)\n",
            "Epoch: [159][275/391]\tLoss 0.0320 (0.0200)\tPrec@1 98.438 (99.434)\n",
            "Epoch: [159][330/391]\tLoss 0.0045 (0.0206)\tPrec@1 100.000 (99.424)\n",
            "Epoch: [159][385/391]\tLoss 0.0299 (0.0209)\tPrec@1 99.219 (99.415)\n",
            "Test\t  Prec@1: 91.820 (Err: 8.180 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [160][0/391]\tLoss 0.0351 (0.0351)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [160][55/391]\tLoss 0.0093 (0.0202)\tPrec@1 100.000 (99.512)\n",
            "Epoch: [160][110/391]\tLoss 0.0146 (0.0204)\tPrec@1 99.219 (99.444)\n",
            "Epoch: [160][165/391]\tLoss 0.0073 (0.0213)\tPrec@1 100.000 (99.431)\n",
            "Epoch: [160][220/391]\tLoss 0.0141 (0.0209)\tPrec@1 100.000 (99.445)\n",
            "Epoch: [160][275/391]\tLoss 0.0232 (0.0203)\tPrec@1 99.219 (99.465)\n",
            "Epoch: [160][330/391]\tLoss 0.0281 (0.0202)\tPrec@1 98.438 (99.476)\n",
            "Epoch: [160][385/391]\tLoss 0.0085 (0.0203)\tPrec@1 100.000 (99.478)\n",
            "Test\t  Prec@1: 91.750 (Err: 8.250 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [161][0/391]\tLoss 0.0116 (0.0116)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [161][55/391]\tLoss 0.0063 (0.0191)\tPrec@1 100.000 (99.512)\n",
            "Epoch: [161][110/391]\tLoss 0.0228 (0.0193)\tPrec@1 98.438 (99.507)\n",
            "Epoch: [161][165/391]\tLoss 0.0130 (0.0194)\tPrec@1 100.000 (99.520)\n",
            "Epoch: [161][220/391]\tLoss 0.0158 (0.0200)\tPrec@1 99.219 (99.473)\n",
            "Epoch: [161][275/391]\tLoss 0.0256 (0.0200)\tPrec@1 99.219 (99.465)\n",
            "Epoch: [161][330/391]\tLoss 0.0330 (0.0203)\tPrec@1 99.219 (99.438)\n",
            "Epoch: [161][385/391]\tLoss 0.0156 (0.0203)\tPrec@1 99.219 (99.425)\n",
            "Test\t  Prec@1: 91.710 (Err: 8.290 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [162][0/391]\tLoss 0.0131 (0.0131)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [162][55/391]\tLoss 0.0106 (0.0193)\tPrec@1 100.000 (99.526)\n",
            "Epoch: [162][110/391]\tLoss 0.0213 (0.0195)\tPrec@1 99.219 (99.543)\n",
            "Epoch: [162][165/391]\tLoss 0.0201 (0.0195)\tPrec@1 100.000 (99.529)\n",
            "Epoch: [162][220/391]\tLoss 0.0128 (0.0187)\tPrec@1 99.219 (99.544)\n",
            "Epoch: [162][275/391]\tLoss 0.0074 (0.0192)\tPrec@1 100.000 (99.510)\n",
            "Epoch: [162][330/391]\tLoss 0.0547 (0.0198)\tPrec@1 99.219 (99.469)\n",
            "Epoch: [162][385/391]\tLoss 0.0286 (0.0199)\tPrec@1 98.438 (99.456)\n",
            "Test\t  Prec@1: 91.770 (Err: 8.230 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [163][0/391]\tLoss 0.0141 (0.0141)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [163][55/391]\tLoss 0.0098 (0.0209)\tPrec@1 100.000 (99.484)\n",
            "Epoch: [163][110/391]\tLoss 0.0409 (0.0209)\tPrec@1 99.219 (99.423)\n",
            "Epoch: [163][165/391]\tLoss 0.0136 (0.0209)\tPrec@1 100.000 (99.431)\n",
            "Epoch: [163][220/391]\tLoss 0.0328 (0.0209)\tPrec@1 99.219 (99.420)\n",
            "Epoch: [163][275/391]\tLoss 0.0425 (0.0203)\tPrec@1 98.438 (99.442)\n",
            "Epoch: [163][330/391]\tLoss 0.0272 (0.0206)\tPrec@1 98.438 (99.429)\n",
            "Epoch: [163][385/391]\tLoss 0.0057 (0.0202)\tPrec@1 100.000 (99.458)\n",
            "Test\t  Prec@1: 91.740 (Err: 8.260 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [164][0/391]\tLoss 0.0160 (0.0160)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [164][55/391]\tLoss 0.0121 (0.0188)\tPrec@1 100.000 (99.512)\n",
            "Epoch: [164][110/391]\tLoss 0.0126 (0.0189)\tPrec@1 100.000 (99.493)\n",
            "Epoch: [164][165/391]\tLoss 0.0074 (0.0188)\tPrec@1 100.000 (99.529)\n",
            "Epoch: [164][220/391]\tLoss 0.0507 (0.0195)\tPrec@1 97.656 (99.505)\n",
            "Epoch: [164][275/391]\tLoss 0.0056 (0.0196)\tPrec@1 100.000 (99.499)\n",
            "Epoch: [164][330/391]\tLoss 0.0324 (0.0199)\tPrec@1 99.219 (99.478)\n",
            "Epoch: [164][385/391]\tLoss 0.0162 (0.0198)\tPrec@1 100.000 (99.488)\n",
            "Test\t  Prec@1: 91.800 (Err: 8.200 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [165][0/391]\tLoss 0.0261 (0.0261)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [165][55/391]\tLoss 0.0351 (0.0203)\tPrec@1 99.219 (99.414)\n",
            "Epoch: [165][110/391]\tLoss 0.0126 (0.0197)\tPrec@1 100.000 (99.430)\n",
            "Epoch: [165][165/391]\tLoss 0.0129 (0.0198)\tPrec@1 100.000 (99.440)\n",
            "Epoch: [165][220/391]\tLoss 0.0116 (0.0197)\tPrec@1 100.000 (99.434)\n",
            "Epoch: [165][275/391]\tLoss 0.0148 (0.0197)\tPrec@1 99.219 (99.442)\n",
            "Epoch: [165][330/391]\tLoss 0.0193 (0.0198)\tPrec@1 99.219 (99.448)\n",
            "Epoch: [165][385/391]\tLoss 0.0541 (0.0200)\tPrec@1 96.875 (99.423)\n",
            "Test\t  Prec@1: 91.690 (Err: 8.310 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [166][0/391]\tLoss 0.0303 (0.0303)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [166][55/391]\tLoss 0.0155 (0.0195)\tPrec@1 100.000 (99.526)\n",
            "Epoch: [166][110/391]\tLoss 0.0178 (0.0197)\tPrec@1 99.219 (99.486)\n",
            "Epoch: [166][165/391]\tLoss 0.0138 (0.0198)\tPrec@1 100.000 (99.473)\n",
            "Epoch: [166][220/391]\tLoss 0.0263 (0.0195)\tPrec@1 98.438 (99.502)\n",
            "Epoch: [166][275/391]\tLoss 0.0203 (0.0191)\tPrec@1 99.219 (99.513)\n",
            "Epoch: [166][330/391]\tLoss 0.0143 (0.0195)\tPrec@1 100.000 (99.488)\n",
            "Epoch: [166][385/391]\tLoss 0.0081 (0.0193)\tPrec@1 100.000 (99.500)\n",
            "Test\t  Prec@1: 91.740 (Err: 8.260 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [167][0/391]\tLoss 0.0158 (0.0158)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [167][55/391]\tLoss 0.0216 (0.0202)\tPrec@1 99.219 (99.414)\n",
            "Epoch: [167][110/391]\tLoss 0.0075 (0.0196)\tPrec@1 100.000 (99.430)\n",
            "Epoch: [167][165/391]\tLoss 0.0195 (0.0190)\tPrec@1 100.000 (99.454)\n",
            "Epoch: [167][220/391]\tLoss 0.0120 (0.0191)\tPrec@1 100.000 (99.470)\n",
            "Epoch: [167][275/391]\tLoss 0.0394 (0.0195)\tPrec@1 99.219 (99.468)\n",
            "Epoch: [167][330/391]\tLoss 0.0256 (0.0193)\tPrec@1 99.219 (99.481)\n",
            "Epoch: [167][385/391]\tLoss 0.0311 (0.0194)\tPrec@1 98.438 (99.472)\n",
            "Test\t  Prec@1: 91.860 (Err: 8.140 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [168][0/391]\tLoss 0.0129 (0.0129)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [168][55/391]\tLoss 0.0133 (0.0194)\tPrec@1 99.219 (99.470)\n",
            "Epoch: [168][110/391]\tLoss 0.0056 (0.0187)\tPrec@1 100.000 (99.465)\n",
            "Epoch: [168][165/391]\tLoss 0.0113 (0.0184)\tPrec@1 100.000 (99.525)\n",
            "Epoch: [168][220/391]\tLoss 0.0119 (0.0185)\tPrec@1 100.000 (99.523)\n",
            "Epoch: [168][275/391]\tLoss 0.0042 (0.0185)\tPrec@1 100.000 (99.507)\n",
            "Epoch: [168][330/391]\tLoss 0.0417 (0.0187)\tPrec@1 99.219 (99.509)\n",
            "Epoch: [168][385/391]\tLoss 0.0301 (0.0188)\tPrec@1 97.656 (99.494)\n",
            "Test\t  Prec@1: 91.750 (Err: 8.250 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [169][0/391]\tLoss 0.0085 (0.0085)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [169][55/391]\tLoss 0.0075 (0.0183)\tPrec@1 100.000 (99.400)\n",
            "Epoch: [169][110/391]\tLoss 0.0225 (0.0188)\tPrec@1 99.219 (99.423)\n",
            "Epoch: [169][165/391]\tLoss 0.0288 (0.0191)\tPrec@1 99.219 (99.435)\n",
            "Epoch: [169][220/391]\tLoss 0.0384 (0.0189)\tPrec@1 99.219 (99.456)\n",
            "Epoch: [169][275/391]\tLoss 0.0112 (0.0189)\tPrec@1 100.000 (99.471)\n",
            "Epoch: [169][330/391]\tLoss 0.0269 (0.0189)\tPrec@1 98.438 (99.471)\n",
            "Epoch: [169][385/391]\tLoss 0.0188 (0.0192)\tPrec@1 99.219 (99.464)\n",
            "Test\t  Prec@1: 91.800 (Err: 8.200 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [170][0/391]\tLoss 0.0456 (0.0456)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [170][55/391]\tLoss 0.0057 (0.0189)\tPrec@1 100.000 (99.470)\n",
            "Epoch: [170][110/391]\tLoss 0.0082 (0.0189)\tPrec@1 100.000 (99.458)\n",
            "Epoch: [170][165/391]\tLoss 0.0105 (0.0178)\tPrec@1 100.000 (99.520)\n",
            "Epoch: [170][220/391]\tLoss 0.0114 (0.0179)\tPrec@1 100.000 (99.512)\n",
            "Epoch: [170][275/391]\tLoss 0.0281 (0.0182)\tPrec@1 99.219 (99.507)\n",
            "Epoch: [170][330/391]\tLoss 0.0405 (0.0183)\tPrec@1 98.438 (99.511)\n",
            "Epoch: [170][385/391]\tLoss 0.0252 (0.0187)\tPrec@1 99.219 (99.500)\n",
            "Test\t  Prec@1: 91.810 (Err: 8.190 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [171][0/391]\tLoss 0.0067 (0.0067)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [171][55/391]\tLoss 0.0280 (0.0177)\tPrec@1 99.219 (99.540)\n",
            "Epoch: [171][110/391]\tLoss 0.0366 (0.0174)\tPrec@1 99.219 (99.578)\n",
            "Epoch: [171][165/391]\tLoss 0.0083 (0.0180)\tPrec@1 100.000 (99.562)\n",
            "Epoch: [171][220/391]\tLoss 0.0162 (0.0180)\tPrec@1 100.000 (99.576)\n",
            "Epoch: [171][275/391]\tLoss 0.0061 (0.0184)\tPrec@1 100.000 (99.539)\n",
            "Epoch: [171][330/391]\tLoss 0.0266 (0.0187)\tPrec@1 98.438 (99.509)\n",
            "Epoch: [171][385/391]\tLoss 0.0042 (0.0187)\tPrec@1 100.000 (99.506)\n",
            "Test\t  Prec@1: 91.810 (Err: 8.190 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [172][0/391]\tLoss 0.0227 (0.0227)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [172][55/391]\tLoss 0.0155 (0.0172)\tPrec@1 100.000 (99.581)\n",
            "Epoch: [172][110/391]\tLoss 0.0232 (0.0181)\tPrec@1 98.438 (99.535)\n",
            "Epoch: [172][165/391]\tLoss 0.0098 (0.0184)\tPrec@1 100.000 (99.534)\n",
            "Epoch: [172][220/391]\tLoss 0.0080 (0.0186)\tPrec@1 100.000 (99.523)\n",
            "Epoch: [172][275/391]\tLoss 0.0160 (0.0185)\tPrec@1 99.219 (99.536)\n",
            "Epoch: [172][330/391]\tLoss 0.0083 (0.0186)\tPrec@1 100.000 (99.533)\n",
            "Epoch: [172][385/391]\tLoss 0.0069 (0.0185)\tPrec@1 100.000 (99.526)\n",
            "Test\t  Prec@1: 91.760 (Err: 8.240 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [173][0/391]\tLoss 0.0119 (0.0119)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [173][55/391]\tLoss 0.0259 (0.0192)\tPrec@1 99.219 (99.554)\n",
            "Epoch: [173][110/391]\tLoss 0.0202 (0.0184)\tPrec@1 100.000 (99.592)\n",
            "Epoch: [173][165/391]\tLoss 0.0455 (0.0182)\tPrec@1 99.219 (99.591)\n",
            "Epoch: [173][220/391]\tLoss 0.0346 (0.0183)\tPrec@1 99.219 (99.576)\n",
            "Epoch: [173][275/391]\tLoss 0.0228 (0.0189)\tPrec@1 99.219 (99.547)\n",
            "Epoch: [173][330/391]\tLoss 0.0186 (0.0181)\tPrec@1 99.219 (99.568)\n",
            "Epoch: [173][385/391]\tLoss 0.0148 (0.0187)\tPrec@1 99.219 (99.528)\n",
            "Test\t  Prec@1: 91.810 (Err: 8.190 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [174][0/391]\tLoss 0.0279 (0.0279)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [174][55/391]\tLoss 0.0058 (0.0171)\tPrec@1 100.000 (99.609)\n",
            "Epoch: [174][110/391]\tLoss 0.0227 (0.0184)\tPrec@1 99.219 (99.500)\n",
            "Epoch: [174][165/391]\tLoss 0.0397 (0.0185)\tPrec@1 99.219 (99.506)\n",
            "Epoch: [174][220/391]\tLoss 0.0115 (0.0179)\tPrec@1 100.000 (99.533)\n",
            "Epoch: [174][275/391]\tLoss 0.0179 (0.0181)\tPrec@1 99.219 (99.519)\n",
            "Epoch: [174][330/391]\tLoss 0.0251 (0.0180)\tPrec@1 100.000 (99.528)\n",
            "Epoch: [174][385/391]\tLoss 0.0133 (0.0177)\tPrec@1 100.000 (99.539)\n",
            "Test\t  Prec@1: 91.740 (Err: 8.260 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [175][0/391]\tLoss 0.0054 (0.0054)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [175][55/391]\tLoss 0.0251 (0.0200)\tPrec@1 99.219 (99.414)\n",
            "Epoch: [175][110/391]\tLoss 0.0375 (0.0192)\tPrec@1 98.438 (99.472)\n",
            "Epoch: [175][165/391]\tLoss 0.0045 (0.0192)\tPrec@1 100.000 (99.482)\n",
            "Epoch: [175][220/391]\tLoss 0.0122 (0.0187)\tPrec@1 100.000 (99.519)\n",
            "Epoch: [175][275/391]\tLoss 0.0223 (0.0189)\tPrec@1 99.219 (99.513)\n",
            "Epoch: [175][330/391]\tLoss 0.0085 (0.0184)\tPrec@1 100.000 (99.535)\n",
            "Epoch: [175][385/391]\tLoss 0.0333 (0.0184)\tPrec@1 98.438 (99.528)\n",
            "Test\t  Prec@1: 91.760 (Err: 8.240 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [176][0/391]\tLoss 0.0448 (0.0448)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [176][55/391]\tLoss 0.0343 (0.0219)\tPrec@1 99.219 (99.316)\n",
            "Epoch: [176][110/391]\tLoss 0.0073 (0.0191)\tPrec@1 100.000 (99.479)\n",
            "Epoch: [176][165/391]\tLoss 0.0699 (0.0193)\tPrec@1 97.656 (99.435)\n",
            "Epoch: [176][220/391]\tLoss 0.0515 (0.0193)\tPrec@1 98.438 (99.431)\n",
            "Epoch: [176][275/391]\tLoss 0.0050 (0.0190)\tPrec@1 100.000 (99.445)\n",
            "Epoch: [176][330/391]\tLoss 0.0133 (0.0192)\tPrec@1 99.219 (99.441)\n",
            "Epoch: [176][385/391]\tLoss 0.0196 (0.0189)\tPrec@1 99.219 (99.464)\n",
            "Test\t  Prec@1: 91.870 (Err: 8.130 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [177][0/391]\tLoss 0.0183 (0.0183)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [177][55/391]\tLoss 0.0065 (0.0147)\tPrec@1 100.000 (99.707)\n",
            "Epoch: [177][110/391]\tLoss 0.0135 (0.0167)\tPrec@1 100.000 (99.585)\n",
            "Epoch: [177][165/391]\tLoss 0.0238 (0.0180)\tPrec@1 99.219 (99.496)\n",
            "Epoch: [177][220/391]\tLoss 0.0373 (0.0180)\tPrec@1 98.438 (99.505)\n",
            "Epoch: [177][275/391]\tLoss 0.0313 (0.0179)\tPrec@1 98.438 (99.519)\n",
            "Epoch: [177][330/391]\tLoss 0.0716 (0.0183)\tPrec@1 98.438 (99.502)\n",
            "Epoch: [177][385/391]\tLoss 0.0134 (0.0183)\tPrec@1 99.219 (99.510)\n",
            "Test\t  Prec@1: 91.750 (Err: 8.250 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [178][0/391]\tLoss 0.0123 (0.0123)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [178][55/391]\tLoss 0.0775 (0.0190)\tPrec@1 98.438 (99.526)\n",
            "Epoch: [178][110/391]\tLoss 0.0284 (0.0181)\tPrec@1 99.219 (99.550)\n",
            "Epoch: [178][165/391]\tLoss 0.0159 (0.0172)\tPrec@1 99.219 (99.581)\n",
            "Epoch: [178][220/391]\tLoss 0.0188 (0.0174)\tPrec@1 100.000 (99.579)\n",
            "Epoch: [178][275/391]\tLoss 0.0094 (0.0181)\tPrec@1 100.000 (99.550)\n",
            "Epoch: [178][330/391]\tLoss 0.0192 (0.0181)\tPrec@1 100.000 (99.552)\n",
            "Epoch: [178][385/391]\tLoss 0.0251 (0.0181)\tPrec@1 99.219 (99.549)\n",
            "Test\t  Prec@1: 91.770 (Err: 8.230 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [179][0/391]\tLoss 0.0078 (0.0078)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [179][55/391]\tLoss 0.0180 (0.0186)\tPrec@1 100.000 (99.498)\n",
            "Epoch: [179][110/391]\tLoss 0.0107 (0.0186)\tPrec@1 100.000 (99.543)\n",
            "Epoch: [179][165/391]\tLoss 0.0179 (0.0187)\tPrec@1 99.219 (99.525)\n",
            "Epoch: [179][220/391]\tLoss 0.0273 (0.0177)\tPrec@1 98.438 (99.576)\n",
            "Epoch: [179][275/391]\tLoss 0.0335 (0.0183)\tPrec@1 98.438 (99.533)\n",
            "Epoch: [179][330/391]\tLoss 0.0143 (0.0184)\tPrec@1 100.000 (99.535)\n",
            "Epoch: [179][385/391]\tLoss 0.0300 (0.0185)\tPrec@1 99.219 (99.508)\n",
            "Test\t  Prec@1: 91.810 (Err: 8.190 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [180][0/391]\tLoss 0.0138 (0.0138)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [180][55/391]\tLoss 0.0082 (0.0160)\tPrec@1 100.000 (99.568)\n",
            "Epoch: [180][110/391]\tLoss 0.0191 (0.0173)\tPrec@1 99.219 (99.550)\n",
            "Epoch: [180][165/391]\tLoss 0.0175 (0.0176)\tPrec@1 99.219 (99.534)\n",
            "Epoch: [180][220/391]\tLoss 0.0118 (0.0177)\tPrec@1 100.000 (99.516)\n",
            "Epoch: [180][275/391]\tLoss 0.0045 (0.0183)\tPrec@1 100.000 (99.499)\n",
            "Epoch: [180][330/391]\tLoss 0.0095 (0.0181)\tPrec@1 100.000 (99.509)\n",
            "Epoch: [180][385/391]\tLoss 0.0112 (0.0182)\tPrec@1 100.000 (99.516)\n",
            "Test\t  Prec@1: 91.890 (Err: 8.110 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [181][0/391]\tLoss 0.0033 (0.0033)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [181][55/391]\tLoss 0.0085 (0.0168)\tPrec@1 100.000 (99.623)\n",
            "Epoch: [181][110/391]\tLoss 0.0417 (0.0175)\tPrec@1 98.438 (99.543)\n",
            "Epoch: [181][165/391]\tLoss 0.0031 (0.0166)\tPrec@1 100.000 (99.586)\n",
            "Epoch: [181][220/391]\tLoss 0.0121 (0.0168)\tPrec@1 100.000 (99.579)\n",
            "Epoch: [181][275/391]\tLoss 0.0158 (0.0170)\tPrec@1 99.219 (99.570)\n",
            "Epoch: [181][330/391]\tLoss 0.0073 (0.0175)\tPrec@1 100.000 (99.549)\n",
            "Epoch: [181][385/391]\tLoss 0.0447 (0.0178)\tPrec@1 98.438 (99.543)\n",
            "Test\t  Prec@1: 91.830 (Err: 8.170 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [182][0/391]\tLoss 0.0110 (0.0110)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [182][55/391]\tLoss 0.0100 (0.0179)\tPrec@1 99.219 (99.540)\n",
            "Epoch: [182][110/391]\tLoss 0.0205 (0.0179)\tPrec@1 99.219 (99.507)\n",
            "Epoch: [182][165/391]\tLoss 0.0301 (0.0180)\tPrec@1 99.219 (99.539)\n",
            "Epoch: [182][220/391]\tLoss 0.0198 (0.0173)\tPrec@1 99.219 (99.569)\n",
            "Epoch: [182][275/391]\tLoss 0.0149 (0.0171)\tPrec@1 100.000 (99.575)\n",
            "Epoch: [182][330/391]\tLoss 0.0185 (0.0175)\tPrec@1 99.219 (99.547)\n",
            "Epoch: [182][385/391]\tLoss 0.0083 (0.0173)\tPrec@1 100.000 (99.551)\n",
            "Test\t  Prec@1: 91.860 (Err: 8.140 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [183][0/391]\tLoss 0.0132 (0.0132)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [183][55/391]\tLoss 0.0150 (0.0179)\tPrec@1 100.000 (99.442)\n",
            "Epoch: [183][110/391]\tLoss 0.0097 (0.0186)\tPrec@1 100.000 (99.479)\n",
            "Epoch: [183][165/391]\tLoss 0.0058 (0.0189)\tPrec@1 100.000 (99.468)\n",
            "Epoch: [183][220/391]\tLoss 0.0209 (0.0188)\tPrec@1 99.219 (99.505)\n",
            "Epoch: [183][275/391]\tLoss 0.0082 (0.0187)\tPrec@1 100.000 (99.488)\n",
            "Epoch: [183][330/391]\tLoss 0.0122 (0.0188)\tPrec@1 100.000 (99.481)\n",
            "Epoch: [183][385/391]\tLoss 0.0046 (0.0187)\tPrec@1 100.000 (99.480)\n",
            "Test\t  Prec@1: 91.810 (Err: 8.190 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [184][0/391]\tLoss 0.0108 (0.0108)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [184][55/391]\tLoss 0.0136 (0.0177)\tPrec@1 100.000 (99.512)\n",
            "Epoch: [184][110/391]\tLoss 0.0160 (0.0184)\tPrec@1 100.000 (99.507)\n",
            "Epoch: [184][165/391]\tLoss 0.0407 (0.0181)\tPrec@1 98.438 (99.562)\n",
            "Epoch: [184][220/391]\tLoss 0.0249 (0.0184)\tPrec@1 99.219 (99.526)\n",
            "Epoch: [184][275/391]\tLoss 0.0086 (0.0181)\tPrec@1 100.000 (99.527)\n",
            "Epoch: [184][330/391]\tLoss 0.0073 (0.0182)\tPrec@1 100.000 (99.528)\n",
            "Epoch: [184][385/391]\tLoss 0.0125 (0.0179)\tPrec@1 100.000 (99.549)\n",
            "Test\t  Prec@1: 91.770 (Err: 8.230 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [185][0/391]\tLoss 0.0185 (0.0185)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [185][55/391]\tLoss 0.0339 (0.0152)\tPrec@1 98.438 (99.595)\n",
            "Epoch: [185][110/391]\tLoss 0.0501 (0.0165)\tPrec@1 97.656 (99.550)\n",
            "Epoch: [185][165/391]\tLoss 0.0269 (0.0166)\tPrec@1 99.219 (99.591)\n",
            "Epoch: [185][220/391]\tLoss 0.0226 (0.0172)\tPrec@1 99.219 (99.583)\n",
            "Epoch: [185][275/391]\tLoss 0.0236 (0.0171)\tPrec@1 99.219 (99.581)\n",
            "Epoch: [185][330/391]\tLoss 0.0209 (0.0173)\tPrec@1 99.219 (99.561)\n",
            "Epoch: [185][385/391]\tLoss 0.0165 (0.0176)\tPrec@1 100.000 (99.555)\n",
            "Test\t  Prec@1: 91.740 (Err: 8.260 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [186][0/391]\tLoss 0.0269 (0.0269)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [186][55/391]\tLoss 0.0194 (0.0167)\tPrec@1 100.000 (99.651)\n",
            "Epoch: [186][110/391]\tLoss 0.0200 (0.0176)\tPrec@1 99.219 (99.578)\n",
            "Epoch: [186][165/391]\tLoss 0.0229 (0.0175)\tPrec@1 100.000 (99.586)\n",
            "Epoch: [186][220/391]\tLoss 0.0087 (0.0177)\tPrec@1 100.000 (99.569)\n",
            "Epoch: [186][275/391]\tLoss 0.0127 (0.0179)\tPrec@1 100.000 (99.564)\n",
            "Epoch: [186][330/391]\tLoss 0.0138 (0.0180)\tPrec@1 99.219 (99.566)\n",
            "Epoch: [186][385/391]\tLoss 0.0205 (0.0175)\tPrec@1 99.219 (99.577)\n",
            "Test\t  Prec@1: 91.780 (Err: 8.220 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [187][0/391]\tLoss 0.0198 (0.0198)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [187][55/391]\tLoss 0.0061 (0.0167)\tPrec@1 100.000 (99.568)\n",
            "Epoch: [187][110/391]\tLoss 0.0356 (0.0171)\tPrec@1 98.438 (99.535)\n",
            "Epoch: [187][165/391]\tLoss 0.0074 (0.0175)\tPrec@1 100.000 (99.525)\n",
            "Epoch: [187][220/391]\tLoss 0.0148 (0.0176)\tPrec@1 99.219 (99.548)\n",
            "Epoch: [187][275/391]\tLoss 0.0093 (0.0175)\tPrec@1 99.219 (99.539)\n",
            "Epoch: [187][330/391]\tLoss 0.0210 (0.0175)\tPrec@1 99.219 (99.547)\n",
            "Epoch: [187][385/391]\tLoss 0.0296 (0.0179)\tPrec@1 98.438 (99.532)\n",
            "Test\t  Prec@1: 91.830 (Err: 8.170 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [188][0/391]\tLoss 0.0206 (0.0206)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [188][55/391]\tLoss 0.0051 (0.0170)\tPrec@1 100.000 (99.609)\n",
            "Epoch: [188][110/391]\tLoss 0.0124 (0.0170)\tPrec@1 100.000 (99.578)\n",
            "Epoch: [188][165/391]\tLoss 0.0199 (0.0164)\tPrec@1 99.219 (99.595)\n",
            "Epoch: [188][220/391]\tLoss 0.0168 (0.0171)\tPrec@1 99.219 (99.586)\n",
            "Epoch: [188][275/391]\tLoss 0.0329 (0.0173)\tPrec@1 98.438 (99.590)\n",
            "Epoch: [188][330/391]\tLoss 0.0054 (0.0179)\tPrec@1 100.000 (99.568)\n",
            "Epoch: [188][385/391]\tLoss 0.0197 (0.0180)\tPrec@1 100.000 (99.567)\n",
            "Test\t  Prec@1: 91.820 (Err: 8.180 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [189][0/391]\tLoss 0.0097 (0.0097)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [189][55/391]\tLoss 0.0371 (0.0161)\tPrec@1 99.219 (99.651)\n",
            "Epoch: [189][110/391]\tLoss 0.0079 (0.0162)\tPrec@1 100.000 (99.634)\n",
            "Epoch: [189][165/391]\tLoss 0.0215 (0.0175)\tPrec@1 99.219 (99.520)\n",
            "Epoch: [189][220/391]\tLoss 0.0081 (0.0173)\tPrec@1 100.000 (99.537)\n",
            "Epoch: [189][275/391]\tLoss 0.0135 (0.0172)\tPrec@1 99.219 (99.530)\n",
            "Epoch: [189][330/391]\tLoss 0.0072 (0.0176)\tPrec@1 100.000 (99.507)\n",
            "Epoch: [189][385/391]\tLoss 0.0044 (0.0175)\tPrec@1 100.000 (99.518)\n",
            "Test\t  Prec@1: 91.760 (Err: 8.240 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [190][0/391]\tLoss 0.0135 (0.0135)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [190][55/391]\tLoss 0.0201 (0.0166)\tPrec@1 99.219 (99.623)\n",
            "Epoch: [190][110/391]\tLoss 0.0157 (0.0171)\tPrec@1 100.000 (99.606)\n",
            "Epoch: [190][165/391]\tLoss 0.0184 (0.0167)\tPrec@1 100.000 (99.605)\n",
            "Epoch: [190][220/391]\tLoss 0.0111 (0.0170)\tPrec@1 100.000 (99.579)\n",
            "Epoch: [190][275/391]\tLoss 0.0100 (0.0172)\tPrec@1 100.000 (99.558)\n",
            "Epoch: [190][330/391]\tLoss 0.0092 (0.0177)\tPrec@1 100.000 (99.528)\n",
            "Epoch: [190][385/391]\tLoss 0.0234 (0.0175)\tPrec@1 98.438 (99.522)\n",
            "Test\t  Prec@1: 91.790 (Err: 8.210 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [191][0/391]\tLoss 0.0254 (0.0254)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [191][55/391]\tLoss 0.0111 (0.0171)\tPrec@1 99.219 (99.512)\n",
            "Epoch: [191][110/391]\tLoss 0.0221 (0.0171)\tPrec@1 99.219 (99.543)\n",
            "Epoch: [191][165/391]\tLoss 0.0056 (0.0174)\tPrec@1 100.000 (99.553)\n",
            "Epoch: [191][220/391]\tLoss 0.0399 (0.0181)\tPrec@1 99.219 (99.519)\n",
            "Epoch: [191][275/391]\tLoss 0.0207 (0.0182)\tPrec@1 99.219 (99.510)\n",
            "Epoch: [191][330/391]\tLoss 0.0175 (0.0181)\tPrec@1 99.219 (99.526)\n",
            "Epoch: [191][385/391]\tLoss 0.0314 (0.0181)\tPrec@1 99.219 (99.537)\n",
            "Test\t  Prec@1: 91.860 (Err: 8.140 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [192][0/391]\tLoss 0.0153 (0.0153)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [192][55/391]\tLoss 0.0246 (0.0174)\tPrec@1 99.219 (99.512)\n",
            "Epoch: [192][110/391]\tLoss 0.0098 (0.0159)\tPrec@1 100.000 (99.606)\n",
            "Epoch: [192][165/391]\tLoss 0.0258 (0.0158)\tPrec@1 98.438 (99.642)\n",
            "Epoch: [192][220/391]\tLoss 0.0174 (0.0155)\tPrec@1 99.219 (99.632)\n",
            "Epoch: [192][275/391]\tLoss 0.0329 (0.0159)\tPrec@1 98.438 (99.621)\n",
            "Epoch: [192][330/391]\tLoss 0.0288 (0.0161)\tPrec@1 99.219 (99.599)\n",
            "Epoch: [192][385/391]\tLoss 0.0084 (0.0162)\tPrec@1 100.000 (99.599)\n",
            "Test\t  Prec@1: 91.860 (Err: 8.140 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [193][0/391]\tLoss 0.0059 (0.0059)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [193][55/391]\tLoss 0.0051 (0.0176)\tPrec@1 100.000 (99.470)\n",
            "Epoch: [193][110/391]\tLoss 0.0070 (0.0170)\tPrec@1 100.000 (99.521)\n",
            "Epoch: [193][165/391]\tLoss 0.0081 (0.0166)\tPrec@1 100.000 (99.572)\n",
            "Epoch: [193][220/391]\tLoss 0.0135 (0.0168)\tPrec@1 100.000 (99.558)\n",
            "Epoch: [193][275/391]\tLoss 0.0191 (0.0169)\tPrec@1 99.219 (99.547)\n",
            "Epoch: [193][330/391]\tLoss 0.0141 (0.0171)\tPrec@1 99.219 (99.530)\n",
            "Epoch: [193][385/391]\tLoss 0.0160 (0.0175)\tPrec@1 99.219 (99.520)\n",
            "Test\t  Prec@1: 91.990 (Err: 8.010 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [194][0/391]\tLoss 0.0084 (0.0084)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [194][55/391]\tLoss 0.0180 (0.0166)\tPrec@1 99.219 (99.637)\n",
            "Epoch: [194][110/391]\tLoss 0.0353 (0.0167)\tPrec@1 98.438 (99.620)\n",
            "Epoch: [194][165/391]\tLoss 0.0152 (0.0166)\tPrec@1 99.219 (99.576)\n",
            "Epoch: [194][220/391]\tLoss 0.0383 (0.0168)\tPrec@1 97.656 (99.533)\n",
            "Epoch: [194][275/391]\tLoss 0.0161 (0.0170)\tPrec@1 99.219 (99.550)\n",
            "Epoch: [194][330/391]\tLoss 0.0317 (0.0169)\tPrec@1 99.219 (99.540)\n",
            "Epoch: [194][385/391]\tLoss 0.0225 (0.0169)\tPrec@1 99.219 (99.537)\n",
            "Test\t  Prec@1: 91.720 (Err: 8.280 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [195][0/391]\tLoss 0.0073 (0.0073)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [195][55/391]\tLoss 0.0073 (0.0197)\tPrec@1 100.000 (99.386)\n",
            "Epoch: [195][110/391]\tLoss 0.0158 (0.0177)\tPrec@1 100.000 (99.514)\n",
            "Epoch: [195][165/391]\tLoss 0.0157 (0.0173)\tPrec@1 100.000 (99.548)\n",
            "Epoch: [195][220/391]\tLoss 0.0102 (0.0173)\tPrec@1 100.000 (99.555)\n",
            "Epoch: [195][275/391]\tLoss 0.0065 (0.0175)\tPrec@1 100.000 (99.547)\n",
            "Epoch: [195][330/391]\tLoss 0.0359 (0.0171)\tPrec@1 98.438 (99.570)\n",
            "Epoch: [195][385/391]\tLoss 0.0063 (0.0172)\tPrec@1 100.000 (99.567)\n",
            "Test\t  Prec@1: 91.870 (Err: 8.130 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [196][0/391]\tLoss 0.0166 (0.0166)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [196][55/391]\tLoss 0.0069 (0.0176)\tPrec@1 100.000 (99.554)\n",
            "Epoch: [196][110/391]\tLoss 0.0074 (0.0164)\tPrec@1 100.000 (99.627)\n",
            "Epoch: [196][165/391]\tLoss 0.0128 (0.0166)\tPrec@1 100.000 (99.595)\n",
            "Epoch: [196][220/391]\tLoss 0.0068 (0.0164)\tPrec@1 100.000 (99.593)\n",
            "Epoch: [196][275/391]\tLoss 0.0204 (0.0164)\tPrec@1 99.219 (99.590)\n",
            "Epoch: [196][330/391]\tLoss 0.0071 (0.0171)\tPrec@1 100.000 (99.542)\n",
            "Epoch: [196][385/391]\tLoss 0.0109 (0.0172)\tPrec@1 100.000 (99.534)\n",
            "Test\t  Prec@1: 91.820 (Err: 8.180 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [197][0/391]\tLoss 0.0051 (0.0051)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [197][55/391]\tLoss 0.0161 (0.0183)\tPrec@1 100.000 (99.442)\n",
            "Epoch: [197][110/391]\tLoss 0.0077 (0.0172)\tPrec@1 100.000 (99.543)\n",
            "Epoch: [197][165/391]\tLoss 0.0159 (0.0166)\tPrec@1 99.219 (99.567)\n",
            "Epoch: [197][220/391]\tLoss 0.0525 (0.0175)\tPrec@1 98.438 (99.516)\n",
            "Epoch: [197][275/391]\tLoss 0.0156 (0.0175)\tPrec@1 99.219 (99.530)\n",
            "Epoch: [197][330/391]\tLoss 0.0146 (0.0178)\tPrec@1 99.219 (99.528)\n",
            "Epoch: [197][385/391]\tLoss 0.0114 (0.0176)\tPrec@1 100.000 (99.543)\n",
            "Test\t  Prec@1: 91.870 (Err: 8.130 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [198][0/391]\tLoss 0.0065 (0.0065)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [198][55/391]\tLoss 0.0185 (0.0150)\tPrec@1 100.000 (99.637)\n",
            "Epoch: [198][110/391]\tLoss 0.0108 (0.0166)\tPrec@1 100.000 (99.599)\n",
            "Epoch: [198][165/391]\tLoss 0.0095 (0.0165)\tPrec@1 100.000 (99.591)\n",
            "Epoch: [198][220/391]\tLoss 0.0116 (0.0158)\tPrec@1 100.000 (99.615)\n",
            "Epoch: [198][275/391]\tLoss 0.0129 (0.0159)\tPrec@1 100.000 (99.598)\n",
            "Epoch: [198][330/391]\tLoss 0.0092 (0.0160)\tPrec@1 100.000 (99.608)\n",
            "Epoch: [198][385/391]\tLoss 0.0173 (0.0161)\tPrec@1 99.219 (99.595)\n",
            "Test\t  Prec@1: 91.830 (Err: 8.170 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [199][0/391]\tLoss 0.0142 (0.0142)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [199][55/391]\tLoss 0.0140 (0.0191)\tPrec@1 99.219 (99.568)\n",
            "Epoch: [199][110/391]\tLoss 0.0370 (0.0177)\tPrec@1 98.438 (99.599)\n",
            "Epoch: [199][165/391]\tLoss 0.0289 (0.0173)\tPrec@1 99.219 (99.595)\n",
            "Epoch: [199][220/391]\tLoss 0.0461 (0.0177)\tPrec@1 98.438 (99.565)\n",
            "Epoch: [199][275/391]\tLoss 0.0150 (0.0176)\tPrec@1 99.219 (99.547)\n",
            "Epoch: [199][330/391]\tLoss 0.0197 (0.0176)\tPrec@1 100.000 (99.556)\n",
            "Epoch: [199][385/391]\tLoss 0.0287 (0.0179)\tPrec@1 98.438 (99.530)\n",
            "Test\t  Prec@1: 91.800 (Err: 8.200 )\n",
            "\n",
            "The lowest error from resnet20 model after 200 epochs is 8.010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNU4-9Tvzg4Z"
      },
      "source": [
        "We conclude here that the results from the original ResNet paper are reproducible for the CIFAR-10 dataset. \n",
        "\n",
        "\n"
      ]
    }
  ]
}